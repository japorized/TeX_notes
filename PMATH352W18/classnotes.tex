% Document Head
\documentclass[11pt, oneside]{book}

\input{latex-classnotes-preamble.tex}

% Main Body
\title{PMATH352W18 Complex Analysis - Class Notes}
\author{Johnson Ng}

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\tableofcontents

\chapter*{List of Definitions}
\theoremlisttype{all}
\listtheorems{defn}

\chapter*{List of Theorems}
\theoremlisttype{allname}
\listtheorems{axiom,lemma,thm,crly,propo}

\chapter{Lecture 1 Jan 3rd 2018}
	\label{chapter:lecture_1_jan_3rd_2018}

\section{Complex Numbers and Their Properties} % (fold)
\label{sec:complex_numbers_and_their_properties}

\begin{defn}[Complex Number, Complex Plane]\label{defn:Complex Number, Complex Plane}
	A \hldefn{complex number} is a vector in $\mathbb{R}^2$. The \hldefn{complex plane}, denoted by $\mathbb{C}$, is a set of complex numbers,
	\begin{equation*}
		\mathbb{C} = \mathbb{R}^2 = \left\{ \begin{pmatrix} x \\ y \end{pmatrix} : x , y \in \mathbb{R} \right\}
	\end{equation*}
	In $\mathbb{C}$, we usually write \\
	\begin{center}
		\begin{tabular}{c c}
			$0 = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$ & $1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ \\
			$i = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$ & $x = \begin{pmatrix} x \\ 0 \end{pmatrix}$ \\
			$iy = \begin{pmatrix} 0 \\ y \end{pmatrix}$
		\end{tabular}
	\end{center}
	where $x, y \in \mathbb{R}$. Consequently, we have that
	\begin{equation*}
		x + iy = x + yi = \begin{pmatrix} x \\ y \end{pmatrix}
	\end{equation*}
	If for $x, y \in \mathbb{R}, \; z = x + iy$, then $x$ is called the \hldefn{real part} of $z$ and $y$ is called the \hldefn{imaginary part} of $z$, and we write
	\begin{equation*}
		\re(z) = x \quad \im(z) = y.
	\end{equation*}
\end{defn}

\begin{note}
	\begin{itemize}
		\item It is easy to see how $\mathbb{R}$ is a subset of $\mathbb{C}$.
		\item Complex Numbers of the form $\left(\begin{smallmatrix} 0 \\ y \end{smallmatrix}\right)$ where $y \in \mathbb{R}$ are called \hlnotea{purely imaginary numbers}.
		\item Certain authors may prefer to denote $i = \left(\begin{smallmatrix} 0 \\ 1 \end{smallmatrix}\right)$.
	\end{itemize}
\end{note}

\begin{defn}[Sum and Product]\label{defn:Sum and Product}
	We define the sum of two complex numbers to be the usual vector sum, i.e.
	\begin{align*}
		(a + ib) + (c + id) &= \begin{pmatrix} a \\ b \end{pmatrix} + \begin{pmatrix} c \\ d \end{pmatrix} \\
											&= \begin{pmatrix} a + c \\ b + d \end{pmatrix} \\
											&= (a + c) + i (b + d)
	\end{align*}
	where $a, b, c, d \in \mathbb{R}$.

	We define the product of two complex numbers by setting $i^2 = -1$, and by requiring the product to be \hlimpo{commutative, associative, and distributive} over the sum. In this setup, we have that
	\begin{align}
		(a + ib)(c + id) &= ac + iad + ibc + i^2 bd \nonumber \\
						 &= (ac - bd) + i (ad + bc) \label{eq:complex multiplication}
	\end{align}
\end{defn}

\begin{note}
 It is interesting to note that \hlwarn{any complex number times zero is zero}, just like what we have with real numbers.
 \begin{gather*}
 	\forall z = x + iy \in \mathbb{C} \; x, y \in \mathbb{R} \enspace 0 \in \mathbb{C} \\
 	z \cdot 0 = (x + iy)(0 + i0) = 0 + i0 = 0
 \end{gather*}
\end{note}

\begin{eg}\label{eg:1}
	Let $z = 2 + i, w = 1 + 3i$. Find $z + w$ and $zw$.
	\begin{align*}
		z + w &= (2 + i) + (1 + 3i) \\
			  	&= 3 + 4i \\
			  	\\
		zw 		&= (2 + i)(1 + 3i) \\
					&= (2 - 3) + i (6 + 1) \quad \text{By } \cref{eq:complex multiplication}\\
					&= -1 + 7i
	\end{align*}
\end{eg}

\begin{eg}\label{eg:multiplicative inverse of a complex number}
	Show that every non-zero complex number has a \hldefn{multiplicative inverse}, $z^{-1}$, and find a formula for this inverse.

	Let $z = a + ib$ where $a, b \in \mathbb{R}$ with $a^2 + b^2 \neq 0$. Then
	\begin{align*}
				 & z(x + iy) = 1 \\
		\iff & (ax - by) + i(ay + bx) = 1 \\
		\iff & \begin{pmatrix} ax - by \\ ay + bx	\end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
		\iff & \begin{pmatrix} a & -b \\ b & a \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
		\iff & \begin{pmatrix} x \\ y \end{pmatrix} = \frac{1}{a^2 + b^2} \begin{pmatrix} a & b \\ -b & a \end{pmatrix}	\begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
		\iff & \begin{pmatrix} x \\ y \end{pmatrix} = \frac{1}{a^2 + b^2}\begin{pmatrix} a \\ -b \end{pmatrix} \\
		\iff & x + iy = \frac{a}{a^2 + b^2} - i \frac{b}{a^2 + b^2} 
	\end{align*}
	Therefore, we have that the formula for the inverse is
	\begin{equation}\label{eq:complex inverse}
		(a + ib)^{-1} = \frac{a}{a^2 +b^2} - i \frac{b}{a^2 + b^2} 
	\end{equation}
\end{eg}

\begin{notation}
	For $z, w \in \mathbb{C}$, we write
	\begin{center}
		\begin{tabular}{c c}
			$-z = -1z$ & $w - z = w + (-z)$ \\
			$\frac{1}{z} = z^{-1}$ & $\frac{w}{z} = wz^{-1}$
		\end{tabular}
	\end{center}
\end{notation}

\begin{eg}\label{eg:3}
	Find $\frac{(4 - i) - (1 - 2i)}{1 + 2i}$.
	\begin{align*}
		\frac{(4 - i) - (1 - 2i)}{1 + 2i} &= \frac{3 + i}{1 + 2i} \\
					&= (3 + i)(\frac{1}{5} - i \frac{2}{5} ) \\
					&= 1 - i
	\end{align*}
\end{eg}

\begin{note}
	The set of complex numbers is a \hlnotea{field} under the operations of addition and multiplication. This means that $\forall u, v, w \in \mathbb{C}$,
	\begin{center}
		\begin{tabular}{r@{\;{=}\;}l r@{\;{=}\;}l}
			u + v 			& v + u 			& uv 			& vu \\
			(u + v) + w & u + (v + w) & (uv)w 	& u(vw) \\
			0 + u 			& u 					& 1u			& u \\
			u + (-u)		& 0						& $uu^{-1}$	& 1, $\; u \neq 0$ \\
			u(v + w)		& uv + uw
		\end{tabular}
	\end{center}

	Since the distributive law holds for complex numbers, note that the \hlwarn{binomial expansion works} for $(w + z)^n$ where $w, z \in \mathbb{C}$ and $n \in \mathbb{N}$. (I did not verify if this is still true for when $n \in \mathbb{R}$.)
\end{note}

\begin{defn}[Conjugate]\label{defn:Conjugate}
	If $z = x + iy$ where $x, y \in \mathbb{R}$, then the \hldefn{conjugate of z} is given by $\bar{z} = x - iy$
\end{defn}

\begin{eg}\label{eg:4}
	Let $z = 3 + 4i$. Then the $\bar{z} = 3 - 4i$. Represented in the complex plane, we have the following:
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[four quad complex, xtick={-4,-3,...,4}, ytick={-5,-4,...,5}, xmin=-4, xmax=4, ymin=-5, ymax=5]
				\node[label={0:{$z$}},circle,fill,inner sep=2pt] at (axis cs:3,4) {};
				\node[label={0:{$\bar{z}$}},circle,fill,inner sep=2pt] at (axis cs:3,-4) {};
			\end{axis}
		\end{tikzpicture}
	\end{center}

	We observe that on the complex plane, the conjugate of a complex number is simply its reflection on the real axis.
\end{eg}

\begin{defn}[Modulus]\label{defn:Modulus}
	We define the \hldefn{modulus} (length, magnitude) of $z = x + iy \in \mathbb{C}, x, y \in \mathbb{R}$, to be
	\begin{equation}
		\abs{z} = \sqrt{x^2 + y^2} \in \mathbb{R}.
	\end{equation}
\end{defn}

\begin{note}
 Note that this definition is consistent with the notion of the absolute value in real numbers when $z$ is a real number, since if $y = 0$, $\abs{z} = \abs{x + i0} = \sqrt{x^2} = \pm x$.
\end{note}

\begin{note}
 For $z, w \in \mathbb{C}$ and $n \in \mathbb{N}$, we have
 \begin{center}
 	\begin{tabular}{r@{\;{=}\;}l r@{\;{=}\;}l r@{\;{=}\;}l}
 		$\bar{\bar{z}}$ 	& z 									& $z + \bar{z}$ 	& $2 \re(z)$ 			& $z - \bar{z}$ 	& $2i \im(z)$ \\
 		$z\bar{z}$				& $\abs{z}^2$					& $\abs{z}$				& $\abs{\bar{z}}$	& $\bar{z \pm w}$	& $\bar{z} \pm \bar{w}$ \\
 		\color{base16-eighties-lightblue}$\bar{zw}$		& \color{base16-eighties-lightblue}$\bar{z}\bar{w}$
 		& $\abs{zw}$			&	$\abs{z}\abs{w}$
 		& $\bar{z}^n$ & $\bar{z^n}$
 	\end{tabular}
 \end{center}
 but note that $\abs{z + w} \neq \abs{z} + \abs{w}$.

 Also, note that the last equation is a generalization of the \hlnoteb{highlighted equation}.
\end{note}

\begin{note}
 While inequalities such as $z_1 < z_2$, where $z_1, z_2 \in \mathbb{C}$, are meaningless unless if both of them are real, $\abs{z_1} < \abs{z_2}$ means that the point $z_1$ in the complex plane is closer to the origin than the point $z_2$.
\end{note}

\begin{propo}[Basic Inequalities]\label{propo:Basic Inequalities}
	\begin{enumerate}
		\item $\abs{\re(z)} \leq \abs{z}$ \\
		\item $\abs{\im(z)} \leq \abs{z}$ \\
		\item $\abs{z + w} \leq \abs{z} + \abs{w} \quad$ Triangle Inequality \label{eq:triangle inequality}\\
		\item $\abs{z + w} \geq \abs{\;\abs{z} - \abs{w}\;} \quad$ Inverse Triangle Inequality
	\end{enumerate}
\end{propo}

\begin{proof}
	Note that $\abs{z}^2 = \re(z)^2 + \im(z)^2$ and that we can express $\abs{x} = \sqrt{x^2}$ for any $x \in \mathbb{R} $. 1 and 2 immediately follows from that.

	To prove 3, we have that
	\begin{align*}
		\abs{z + w}^2 &= (z + w)(\bar{z} + \bar{w}) \\
				&= \abs{z}^2 + \abs{w}^2 + (w\bar{z} + \bar{w}z) \\
				&= \abs{z}^2 + \abs{w}^2 + 2\re(w\bar{z}) \\
				&\leq \abs{z}^2 + \abs{w}^2 + 2\abs{w\bar{z}} \quad \text{by 1} \\
				&= \abs{z}^2 + \abs{w}^2 + 2\abs{wz} \quad \text{since } \abs{w\bar{z}} = \abs{w}\abs{\bar{z}} \text{ and } \abs{z} = \abs{\bar{z}} \\
				&= (\abs{z} + \abs{w})^2
	\end{align*}
	
	To prove 4, note that
	\begin{alignat}{3}
		&\abs{z} &&= \abs{z + w - w} &&\leq \abs{z + w} + \abs{w} \label{basicinequal1} \\
		&\abs{w} &&= \abs{w + z - z} &&\leq \abs{z + w} + \abs{z} \label{basicinequal2}
	\end{alignat}

	Observe that
	\begin{align*}
		\cref{basicinequal1} \implies \abs{z} - \abs{w} \leq \abs{z + w} \\
		\cref{basicinequal2} \implies \abs{w} - \abs{z} \leq \abs{z + w}
	\end{align*}

	Thus, we have that
	\begin{equation*}
		\abs{z + w} \geq \abs{\; \abs{z} - \abs{w} \;}
	\end{equation*}
	as required.\qed
\end{proof}

\cref{eq:triangle inequality} in \cref{propo:Basic Inequalities} can be generalized by the means of mathematical induction to sums involving any finite number of terms, as:
\begin{equation}\label{eq:generalized triangle inequality}
	\abs{z_1 + z_2 + \hdots + z_n} \leq \abs{z_1} + \abs{z_2} + \hdots + \abs{z_n}
\end{equation}
where $n \in \mathbb{N} \setminus \{0, 1\}$.

To note the induction proof, when $n = 2$, \cref{eq:generalized triangle inequality} is just \cref{eq:triangle inequality}. If \cref{eq:generalized triangle inequality} is true for when $n = m$ where $m \in \mathbb{N} \setminus \{0, 1\}$, $n = m + 1$ is also true since by \cref{eq:triangle inequality},
\begin{align*}
	\abs{(z_1 + z_2 + \hdots + z_m) + z_{m+1}} &\leq \abs{z_1 + z_2 + \hdots + z_m} + \abs{z_{m + 1}} \\
			&\leq (\abs{z_1} + \abs{z_2} + \hdots + \abs{z_m}) + \abs{z_{m+1}}.
\end{align*}

The distance between two points $z_1 = x_1 + iy_1, z_2 = x_2 + iy_2 \in \mathbb{C}, x_1, x_2, y_1, y_2 \in \mathbb{R}$ is $\abs{z_1 - z_2}$, since $\abs{z_1 - z_2} = \sqrt{(x_1 - x_2)^2 (y_1 - y_2)^2}$ is our usual notion of the Euclidean distance of two points on a plane.

Also, note that
\begin{equation*}
	z_1 - z_2 = z_1 + (-z_2)
\end{equation*}
and thus if we apply our knowledge of vector representation, $z_1 - z_2$ is the directed line segment from the point $z_2$ to $z_1$.

With the notion of a ``distance'' set on the complex plane, we can now explore upon points lying on a circle with a center $z_0$ and radius $R$, which satisfies the equation
\begin{equation*}
	\abs{z - z_0} = R.
\end{equation*}
We may simply refer to this set of points as the circle $\abs{z - z_0} = R$.

\begin{eg}
	We may describe a set $\left\{z \in \mathbb{C} : \abs{z -i} = 1 \right\}$ as follows:

	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[four quad complex, xtick={-3,-2,...,3}, ytick={-3,-2,...,3}, xmin=-3, xmax=3, ymin=-1, ymax=3]
				\draw (axis cs:0, 1) circle [radius=1];
			\end{axis}
		\end{tikzpicture}
	\end{center}

	Let $a, b \in \mathbb{C}$ describe the set $\left\{z \in \mathbb{C} : \abs{z - a} < \abs{z - b} \right\}$.

	Suppose the following coordinates for $a$ and $b$ are arbitrary,

	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[four quad complex, xmin=-15, xmax=15, ymin=-15, ymax=15]
				\draw[line width=0pt,dashed,fill=black,fill opacity=0.25](-20,20)--(-20,-20)--(20,-20)--(20,-10)--(-5,15);
				\draw (-1,13) node {$f$};
				\draw[line width=2pt,solid](2,2)--(8,8);
				\draw[color=black] (3.5,5) node {$g$};
				\node[label={0:{$a$}},circle,fill,inner sep=2pt] at (axis cs:2,2) {};
				\node[label={0:{$b$}},circle,fill,inner sep=2pt] at (axis cs:8,8) {};
			\end{axis}
		\end{tikzpicture}
	\end{center}

	In the above, $g$ is the line segment that connects the points $a$ and $b$ on the complex plane, while $f$ is the perpendicular bisector of the line segment $g$. The area described by the set $\left\{z \in \mathbb{C} : \abs{z - a} < \abs{z - b} \right\}$ is the shaded area which is below $f$.
\end{eg}

% section complex_numbers_and_their_properties (end)

% chapter lecture_1_jan_3rd_2018 (end)

\chapter{Lecture 2 Jan 5th 2018}
	\label{chapter:lecture_2_jan_5th_2018}

\section{Complex Numbers and Their Properties (Continued)} % (fold)
\label{sec:complex_numbers_and_their_properties_continued}

\begin{eg}
	Let $a \in \mathbb{C}$. Describe the set $\{z \in \mathbb{C} : 1 < \abs{z-a} < 2\}$.

	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				four quad complex,
				xmin=-2, xmax=7,
				ymin=-2, ymax=7
			]
				\draw [fill=black,fill opacity=0.25,even odd rule] (3,3) circle (1.4142135623730951) (3,3) circle (1);
				\node[label={0:{$a$}},circle,fill,inner sep=1pt] at (axis cs:3,3) {};
			\end{axis}
		\end{tikzpicture}
	\end{center}
\end{eg}

\begin{eg}
	\label{eg:complex number has exactly two roots}
	Show that every non-zero complex number has exactly two complex square roots, and find a formula for the square roots.

	Let $z = x + iy \in \mathbb{C}, x, y \in \mathbb{R}$, and let $w = u + iv, u, v \in \mathbb{R}$. Then

	\begin{alignat}{3}
		&w^2 = z &&\iff && (u + iv)^2 = x + iy \nonumber \\
		&	&&\iff 	&& (u^2 - v^2) + i(2uv) = x + iy \nonumber \\
		&	&&\iff && x = u^2 + v^2 \quad \text{and} \label{tworoots 1} \\
		&	&&	   && y = 2uv \label{tworoots 2}
	\end{alignat}

	Square both sides of \cref{tworoots 2}, and thus we have $y^2 = 4u^2 v^2$.

	Multiply \cref{tworoots 1} by $4u^2$, and we get
	\begin{alignat*}{3}
		& 	  &&4u^2 x &&= 4u^4 - 4u^2 v^2 = 4u^4 - y^2 \\
		&\iff &&0 	&&= 4u^4 - 4u^2 x - y^2 \\
		&\iff &&u^2 &&= \frac{4x \pm \sqrt{16x^2 + 16y^2}}{8} \\
		&	  &&	&&= \frac{x \pm \sqrt{x^2 + y^2}}{2} 
	\end{alignat*}

	Suppose $y \neq 0$. Note that $x < \sqrt{x^2 + y^2}$. Thus $u^2 = \frac{x + \sqrt{x^2 + y^2}}{2} \implies u = \left(\frac{x + \sqrt{x^2 + y^2}}{2} \right)^{\frac{1}{2}}$.

	Similarly, we can get
	\begin{equation*}
		v = \pm \left(\frac{-x + \sqrt{x^2 + y^2}}{2}\right)^{\frac{1}{2}}
	\end{equation*}

	Note that all four choices of signs satisfy \cref{tworoots 1}. If $y > 0$, then $u$ and $v$ are either both positive or both negative by \cref{tworoots 2}.

	Suppose $y = 0$. Then we have
	\begin{equation*}
		w^2 = z = x
	\end{equation*}

	Therefore, we get
	\begin{equation*}
		w = \begin{cases}
			\pm \left[ \left(\frac{x + \sqrt{x^2 + y^2}}{2} \right)^{\frac{1}{2}} + i \left( \frac{-x + \sqrt{x^2 + y^2}}{2} \right)^{\frac{1}{2}} \right] & y > 0 \\
			\pm \left[ \left(\frac{x + \sqrt{x^2 + y^2}}{2} \right)^{\frac{1}{2}} - i \left(\frac{-x + \sqrt{x^2 + y^2}}{2} \right)^{\frac{1}{2}} \right] & y < 0 \\
			\pm \sqrt{x} & y = 0, x > 0 \\
			\pm i \sqrt{x} & y = 0, x < 0
		\end{cases}
	\end{equation*}
\end{eg}

\begin{remark}
	Let $z \in \mathbb{C}$. The notation $\sqrt{z}$ may represent either one of the square roots of $z$ or both of the square roots, i.e. \hlwarn{it is possible that $\sqrt{z}$ represents a set}.
\end{remark}

\begin{ex}\label{ex:Separation of Multiplication in Square Roots}
	Is it always okay for complex numbers such that $\sqrt{zw} = \sqrt{z} \sqrt{w}$, for $z, w \in \mathbb{C}$?

	No. For example, consider $z = w = -1$. Then we have
	\begin{equation*}
		\sqrt{zw} = \sqrt{1} = \pm 1
	\end{equation*}
	while
	\begin{equation*}
		\sqrt{z} \sqrt{w} = i \cdot i = -1
	\end{equation*}
	and thus
	\begin{equation*}
		\sqrt{zw} \neq \sqrt{z} \sqrt{w}.
	\end{equation*}
\end{ex}

\begin{eg}
	Find the values of $\sqrt{3 - 4i}$.

	By \cref{eg:complex number has exactly two roots},

	\begin{align*}
		\sqrt{3 - 4i} &= \pm \left( \sqrt{\frac{3 + \sqrt{9 + 16}}{2}} - i \sqrt{\frac{-3 + \sqrt{9 + 16}}{2}} \right) \\
			&= \pm (2 - i)
	\end{align*}
\end{eg}

\begin{remark}\label{remark:quadratic formula for complex numbers}
	The quadratic formula holds for complex polynomials, i.e.
	\begin{equation*}
		\forall a, b, c \in \mathbb{C} \quad a \neq 0 \quad \forall z \in \mathbb{C} \; az^2 + bz + c = 0,
	\end{equation*}
	the solution for $z$ is given by
	\begin{equation}\label{eq:quadractic formula for complex numbers}
		z_{1, 2} = \frac{-b + \sqrt{b^2 - 4ac}}{b} 
	\end{equation}

	The following is a short proof.

	\begin{proof}
		\begin{align*}
			az^2 + bz + c = 0 &\iff z^2 + \frac{b}{a} z + \frac{c}{a} = 0 \\
				&\iff z^2 + \frac{b}{a} z + \left(\frac{b}{2a} \right)^2 - \left(\frac{b}{2a}\right)^2 + \frac{c}{a} = 0 \\
				&\iff \left(z + \frac{b}{2a} \right)^2 = \frac{b^2}{4a^2} - \frac{c}{a} = \frac{b^2 - 4ac}{4a^2} \\
				&\iff z = \frac{-b + \sqrt{b^2 - 4ac}}{2a}  
		\end{align*}
	\end{proof}

	(Personal Note: where did the $-$ for the supposed $\pm$ go? Or should it really be $\pm$?)
\end{remark}

\begin{eg}
	Solve $iz^2 - (2 + 3i)z + 5(1 + i) = 0$.
	\begin{align*}
		z &= \frac{2 + 3i + \sqrt{(2 + 3i)^2 - 4i[5(1 + i)]}}{2i} \\
			&= \frac{2 + 3i + \sqrt{-5 + 12i -20i + 20}}{2i} \\
			&= \frac{2 + 3i + \sqrt{15 + 8i}}{2i} 
	\end{align*}
	Note that by \cref{eg:complex number has exactly two roots},
	\begin{align*}
		\sqrt{15 - 8i} &= \pm \left[ \sqrt{\frac{15 + \sqrt{225 + 64}}{2} } - i \sqrt{\frac{-15 + \sqrt{225 + 64}}{2} } \; \right] \\
			&= \pm \left[ \sqrt{\frac{15 + 17}{2} } - i \sqrt{\frac{-15 + 17}{2} } \; \right] \\
			&= \pm (4 - i)
	\end{align*}
	Thus we have
	\begin{align*}
		z &= \frac{2 + 3i + \sqrt{15 + 8i}}{2i} \\
			&= \frac{2 + 3i \pm (4 - i)}{2i} \\
			&= (6 + 2i) \left( -\frac{1}{2}i \right) \text{ or } (-2 + 4i) \left( -\frac{1}{2} i \right) \quad \text{by } \cref{eg:multiplicative inverse of a complex number} \\
			&= (1 - 3i) \text{ or } (2 + i)
	\end{align*}
\end{eg}

% section complex_numbers_and_their_properties_continued (end)

% chapter lecture_2_jan_5th_2018 (end)

\chapter{Lecture 3 Jan 8th 2018}
	\label{chapter:lecture_3_jan_8th_2018}

\section{Complex Numbers and Their Properties (Continued 2)} % (fold)
\label{sec:complex_numbers_and_their_properties_continued_2}

\begin{defn}[Argument of a Complex Number]\label{defn:Argument of a Complex Number}
	Let $z \in \mathbb{C} \setminus \{0\}$. The \hldefn{argument} (or the angle) of $z$, denoted by $\arg{z}$, $\Arg{z}$, or simply $\theta = \theta(z)$, is the angle modulo $2 \pi$ (i.e. $0 \leq \theta < 2 \pi$) between the vector defining $z$ and the positive real axis (in the counterclockwise direction).

	\begin{center}
		\begin{tikzpicture}
			\tikzset{>=stealth}
			\draw[->] (-2, 0) -- ++(4, 0) coordinate (X) node[below] {$\re$};
			\draw[->] (0, -1) -- ++(0, 3) node[left] {$\im$};
			\coordinate (O) at (0, 0);

			\draw[->] (O) -- (1,1) coordinate (z) node[above] {$z$};
			\path (X) -- (O) -- (z) pic [draw,->,pic text=$\theta$, angle eccentricity=1.3] {angle=X--O--z};

			\draw[->] (O) -- (-1.5, 1.5) coordinate (z1) node[above] {$z_1$};
			\path (X) -- (O) -- (z1) pic [draw,->,angle radius=1cm, pic text=$\theta_1$, angle eccentricity=1.2] {angle=X--O--z1};
		\end{tikzpicture}
	\end{center}
\end{defn}

\begin{notation}
	Let $e^{i \theta} := \cos \theta + i \sin \theta$. Note that this definition, called \hldefn{Euler's formula}, can be derived by the extending the Taylor expansion of $e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} $ for when $x \in \mathbb{C}$ (the sum of the real parts of the expansion is the Taylor expansion of cosine while the imaginary part for sine).

	Now $e^{i \theta}$ is on the unit circle.
	\begin{center}
		\begin{tikzpicture}
			\tikzset{>=stealth}
			\draw[->] (-2, 0) -- ++(4, 0) coordinate (X) node[below] {$\re$};
			\draw[->] (0, -2) -- ++(0, 4) node[left] {$\im$};
			\coordinate (O) at (0, 0);

			\newcommand \CircleRadius{1.5};
			\draw (0,0) circle (\CircleRadius);
			\coordinate (z) at (45:\CircleRadius);

			\draw[dotted] (O) -- (z) node[above right] {$e^{t \theta}$};
			\draw[dotted] (z) -- (z -| O) node[left] {$\sin \theta$};
			\draw[dotted] (z) -- (z |- O) node[below] {$\cos \theta$};
		\end{tikzpicture}
	\end{center}
\end{notation}

\begin{remark}
	If $z = 0$, the coordinate $\theta$ is undefined, and so it is implied that $z \neq 0$ whenever we use the polar form.
\end{remark}

\begin{eg}
Some examples of $\theta \in [0, 2\pi)$:

	\begin{tabular}{r@{\;{=}\;}l r@{\;{=}\;}l}
		$e^{i \frac{\pi}{4}}$ & $\frac{\sqrt{2}}{2} + i \frac{\sqrt{2}}{2}$ & $e^{i \frac{\pi}{2} }$ & $i$ \\
		$e^{i \frac{3 \pi}{4}}$ & $-\frac{\sqrt{2}}{2} + i \frac{\sqrt{2}}{2}$ & $e^{i \pi} + 1$ & $0$
	\end{tabular}
\end{eg}

\begin{remark}
	\begin{equation*}
		\forall k \in \mathbb{Z} \enspace \forall \theta \in \mathbb{R} \enspace e^{i\theta} = e^{i(\theta + 2\pi k)} 
	\end{equation*}
\end{remark}

\begin{remark}
	The complex number $re^{i \theta}$, where $r > 0, \theta \in [0, 2\pi)$, represents the complex number with modulus $r$ and argument $\theta$.
	\begin{center}
		\begin{tikzpicture}
			\tikzset{>=stealth}
			\draw[->] (-2, 0) -- ++(4, 0) coordinate (X) node[below] {$\re$};
			\draw[->] (0, -2) -- ++(0, 4) node[left] {$\im$};
			\coordinate (O) at (0, 0);

			\newcommand \CircleRadius{1.5};
			\draw[dotted] (0,0) circle (\CircleRadius);
			\coordinate (z) at (135:\CircleRadius);

			\draw[<->] (O) -- (z) node[midway,below] {$r$} node[above left] {$z$};
			\path (X) -- (O) -- (z) pic [draw,->,angle radius=0.5cm,pic text=$\theta$,angle eccentricity=1.3] {angle=X--O--z};
		\end{tikzpicture}
	\end{center}

	Therefore, $\forall z \in \mathbb{C}$, we can express
	\begin{equation}\label{eq:polar representation of a complex number}
		z := \abs{z} e^{i \Arg{z}}.
	\end{equation}
\end{remark}

With that, we now have two representations of a complex number:
\begin{itemize}
	\item \hlnotea{Cartesian representation}: $z = x + iy$ where $x = \re(z)$ and $y = \im(z)$
	\item \hlnotea{Polar representation}: $z = re^{i \theta}$ where $r = \abs{z}$ and $\theta = \Arg{z} \in [0, 2\pi)$
\end{itemize}

To convert between the two representations, we have the following equations:

Polar $\to$ Cartesian:

\begin{equation}\label{eq:polar to cartesian}
	x = r \cos \theta \quad y = r \sin \theta
\end{equation}

Cartesian $\to$ Polar:

\begin{gather}\label{eq:cartesian to polar}
	r = \abs{z} \nonumber \\
	x \neq 0 \implies \tan \theta = \frac{y}{x} \\
	x = 0 \implies \theta = \frac{\pi}{2} \text{ or } \frac{3\pi}{2} \nonumber
\end{gather}

On another note,
\begin{equation*}
	z = re^{i \theta} \implies \bar{z} = re^{-i \theta}
\end{equation*}
and
\begin{equation}\label{eq:inverse of nonzero complex number in polar coords}
	z \neq 0 \implies \frac{1}{z} = \frac{1}{r} e^{-i \theta}
\end{equation}

\begin{remark}
	\begin{gather*}
		\forall r_1, r_2 \in \mathbb{R} \; \forall \theta_1, \theta_2 \in [0, 2\pi) \\
		z_1 := r_1 e^{i \theta_1} \quad z_2 := r_2 e^{i \theta_2}
	\end{gather*}
	Then
	\begin{equation*}
		z_1 z_2 = r_1 r_2 e^{i \theta_1} e^{i \theta_2} = r_1 r_2 e^{i (\theta_1 + \theta_2)}
	\end{equation*}

	Note that $e^{ix} e^{iy} = e^{i(x + y)}$ is true for all $x, y \in \mathbb{R} $ since
	\begin{align*}
		e^{ix} e^{iy}
			&= (\cos x + i \sin x)(\cos y + i \sin y) \\
			&= (\cos x \cos y - \sin x \sin y) + i (\cos x \sin y + \cos y \sin x) \\
			&= \cos (x + y) + i \sin (x + y) \\
			&= e^{i (x + y)}.
	\end{align*}

	Generalizing the above, we get that
	\begin{equation}\label{eq:demoivre's law}
		\forall n \in \mathbb{Z} \enspace z = (re^{in}) = r^n e^{in\theta}
	\end{equation}
	which is commonly known as \hldefn{deMoivre's Law}. Note that by simply generalizing the above, all we have is that $n \in \mathbb{Z^+}$. But by \cref{eq:inverse of nonzero complex number in polar coords}, we can have that for $n \in \mathbb{Z^-}$, let $m = -n$, and thus
	\begin{equation*}
		z^n = \left[ \frac{1}{r} e^{i (-\theta)} \right]^m = \left(\frac{1}{r}\right)^m e^{im (-\theta)} = \left(\frac{1}{r} \right)^{-n} e^{i (-n) (-\theta)} = r^n e^{i\theta}
	\end{equation*}
	This proves that deMoivre's Law also holds for when $n \in \mathbb{Z^-}$.

	Observe that if $r = 1$, \cref{eq:demoivre's law} becomes
	\begin{equation}
		(e^{i\theta})^n = e^{in\theta} \quad \text{for all } n \in \mathbb{Z} \setminus \{0\} 
	\end{equation}
	When written in the form
	\begin{equation}\label{eq:demoivre's formula}
		(\cos \theta + i \sin \theta)^n = \cos n\theta + i \sin n\theta \quad (n \in \mathbb{Z} \setminus \{0\})
	\end{equation}
	this is known as \hldefn{deMoivre's formula}.
\end{remark}

\begin{eg}
	\cref{eq:demoivre's formula} with $n = 2$ tells us that
	\begin{equation*}
		(\cos \theta + i \sin \theta)^n = \cos 2\theta + i \sin 2\theta
	\end{equation*}
	or we can express the equation as
	\begin{equation*}
		\cos^2 \theta - \sin^2 \theta + i2 \sin\theta \cos\theta = \cos 2\theta + i \sin 2\theta
	\end{equation*}
	Equating real and imaginary parts, we have the familiar double angle trigonometric identities
	\begin{equation*}
		\cos 2\theta = \cos^2 \theta - \sin^2 \theta, \quad \sin 2\theta = 2\sin\theta\cos\theta.
	\end{equation*}
\end{eg}

\subsection{Roots of Complex Numbers} % (fold)
\label{sub:roots_of_complex_numbers}

\begin{propo}[nth Roots of a Complex Number]\label{propo:nth Roots of a Complex Number}
	\begin{gather*}
		\forall z = re^{i\theta} \in \mathbb{C} \enspace r = \abs{z} \in \mathbb{R} \enspace \theta \in [0, 2\pi) \\
		\exists w = se^{i\tau} \in \mathbb{C} \enspace s \in \mathbb{R} \enspace \tau \in [0, 2\pi) \\
		\forall n \in \mathbb{Z} \\ 
		w^n = \left( se^{i\tau}\right)^n = z = re^{i\theta}
	\end{gather*}

	The nth roots of $z$ is described by the set
	\begin{equation}\label{eq:nth roots of a complex number}
		\left\{ r^{\frac{1}{n}} e^{i \left(\frac{\theta + 2 \pi k}{n} \right)} : k = 0, 1, ..., n - 1 \right\}
	\end{equation}

	\begin{proof}
		\begin{gather*}
			s^n = r \iff s = r^{\frac{1}{n}} \\
			e^{in\theta} = e^{i \tau} \iff \theta = \frac{\tau + 2 \pi k}{n}
		\end{gather*}

		Therefore, the set that describes the nth roots of $z$ is
		\begin{gather*}
			\left\{ w = r^\frac{1}{n} e^{i \left( \frac{\theta + 2 \pi k}{n} \right)} : k = 0, 1, ..., n - 1 \right\}
		\end{gather*}
	\end{proof}
\end{propo}

\begin{remark}[nth Roots of Unity]\label{remark:nth Roots of Unity}
	The \hlnotea{nth roots of unity} is a direct consequence of \cref{propo:nth Roots of a Complex Number} where we solve for the equation $z^n = 1$ for any $z \in \mathbb{C}, n \in \mathbb{Z}$.

	The set that describes the nth roots of unity is
	\begin{equation}\label{eq:nth roots of unity}
		\left\{ e^{i \theta} : \theta = \frac{2 \pi k}{n}, k = 0, 1, ..., n - 1 \right\}
	\end{equation}
\end{remark}

It is easy to see how the nth roots of unity \hlnoteb{partitions the unit circle into n parts}.

\begin{eg}
	Find the cubic roots of $-2 + 2i$.

	Let $z = -2 + 2i$. Note that $\abs{z} = 2\sqrt{2}$ and $\Arg{z} = \frac{3 \pi }{4}$.

	Therefore, in polar form, $z = 2 \sqrt{2} e^{i \frac{3\pi}{4} }$.

	Let $w = re^{i \theta}$, where $\theta \in [0, 2\pi)$, and $w^3 = z$. Then
	\begin{gather*}
		r = (2 \sqrt{2})^\frac{1}{3} \\
		\theta = \frac{\frac{3\pi}{4} + 2 \pi k}{3}, \enspace k = 0, 1, 2
	\end{gather*}

	The set that describes the cubic root of $-2 + 2i$ is thus
	\begin{equation*}
		\left\{ (2\sqrt{2})^\frac{1}{3} e^{i \theta} : \theta = \frac{\frac{3\pi}{4} + 2 \pi k}{3}, k = 0, 1, 2 \right\}
	\end{equation*}
\end{eg}

\begin{eg}
	Describe the set $\{z \in \mathbb{C} : \abs{\Arg{z} - \frac{\pi}{2}} < \frac{\pi}{2} \}$. (Note: $\Arg{z} \in [0, 2\pi)$)

	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[four quad complex, xmin=-2, xmax=2, ymin=-1, ymax=2]
				\draw[line width=0pt,dashed,fill=base16-eighties-brown,fill opacity=0.25](-3,0)--(3,0)--(3,3)--(-3,3);
			\end{axis}
		\end{tikzpicture}
	\end{center}
\end{eg}

\begin{ex}
	Solve
	\begin{enumerate}
		\item $z^4 = -1$
			\begin{gather*}
				\text{Let } z = re^{i \theta} \\
				r = \abs{-1} = 1 \quad \theta = \frac{\pi + 2 \pi k}{4} = \frac{(2k + 1) \pi}{4}, \enspace k = 0, 1, 2, 3
			\end{gather*}
		\item $z^4 = -1 + \sqrt{3} i$
			\begin{gather*}
				\text{Let } z = re^{i \theta} \\
				r = \abs{-1 + \sqrt{3} i} = \sqrt{(-1)^2 + 3^2} = \sqrt{10} \\
				\theta = \frac{\frac{2\pi}{3} + 2\pi k}{4} = \frac{(2k + \frac{2}{3}) \pi}{4}, \quad k = 0, 1, 2, 3  
			\end{gather*}
	\end{enumerate}
\end{ex}

% subsection roots_of_complex_numbers (end)

% section complex_numbers_and_their_properties_continued_2 (end)

% chapter lecture_3_jan_8th_2018 (end)

\chapter{Lecture 4 Jan 10th 2018}
	\label{chapter:lecture_4_jan_10th_2018}

\section{Examples for nth Roots of Unity} % (fold)
\label{sec:examples_for_nth_roots_of_unity}

Recall that the $n$th roots of unity are given by $e^{i \frac{2\pi k}{n}}, k = 0, 1, ..., n - 1$.

\begin{ex}\label{ex:sum of root of unity other than one is negative one}
	Let $z$ be any $n$th root of unity other than 1. Show that
	\begin{equation}
		z^{n - 1} + z^{n - 2} + \hdots + z + 1 = 0
	\end{equation}

	\begin{proof}
		By the Sum of Finite Geometric Terms,
		\begin{equation*}
			z^{n - 1} + z^{n - 2} + \hdots + z + 1 = \frac{1 - z^n}{1 - z}.
		\end{equation*}
		Since $z^n = 1$, RHS is thus zero, which in turn completes the proof.
	\end{proof}
\end{ex}

As an aside, if we wish to remove the restriction that $z$ can also be 1, we may consider that
\begin{equation*}
	z^n - 1 = (z - 1)(1 + z + \hdots + z^{n - 1})
\end{equation*}
Since $z^n = 1$, LHS is zero. Then either $z = 1$ or $(1 + z + \hdots + z^{n - 1}) = 0$.

\begin{ex}
	Consider the $n - 1$ diagonals of a regular $n$-gon, inscribed in a circle of radius 1, obtained by connecting one vertex on the $n$-gon to all its other vertices.

	For example, if we are given $n = 6$, we obtain the following diagram.

	\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}
				\newcommand \CircleRadius {2};
				\draw(0, 0) circle (\CircleRadius);
				\coordinate (a) at (180:\CircleRadius);
				\coordinate (z1) at (120:\CircleRadius);
				\coordinate (z2) at (60:\CircleRadius);
				\coordinate (z3) at (0:\CircleRadius);
				\coordinate (z4) at (300:\CircleRadius);
				\coordinate (z5) at (240:\CircleRadius);
				\node[label={180:{$a$}},circle,fill,inner sep=1pt] at (a) {};
				\foreach \x in {1, ..., 5} {
					\draw (a)--(z\x) node[midway, above] {$z_\x$};
				};
				\draw[dotted] (0:\CircleRadius) \foreach \x in {0, 60, ..., 300} {
					-- (\x:\CircleRadius)
				} -- cycle;
			\end{tikzpicture}
		\end{center}
		\caption[loftitle]{$n = 6$, where $a$ is an arbitrary vertex on the hexagon}
		\label{figure:regular hexagon with one point connected to all other vertices}
	\end{figure}

	Show that the product of the lengths of these diagonals is equal to n.

	\begin{proof}
		Note that \cref{figure:regular hexagon with one point connected to all other vertices} can be translated into \cref{figure:regular n-gon with roots of unity}.

		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}
					\tikzset{>=stealth}
					\draw[->] (-3, 0) -- ++(6, 0) coordinate (X);
					\draw[->] (0, -3) -- ++(0, 6);
					\newcommand \CircleRadius {2};
					\newcommand \Vertices {10};
					\draw(0, 0) circle (\CircleRadius);
					\foreach \i [evaluate=\i as \x using (\i - 1)*(360 / \Vertices)] in {1, ..., 6} {
						\coordinate (z\i) at (\x:\CircleRadius);
						\ifthenelse{\i<5}{
							\node[label={\x:{\ifthenelse{\i=1}{$a=z_\i$}{$z_\i$}}},circle,fill,inner sep=1pt] at (z\i) {};
						}{};
					};
					\foreach \i [evaluate=\i as \j using \i + 1] in {1, ..., 5} {
						\ifthenelse{\i<4}{
							\draw (z\i)--(z\j);
						}{
							\draw[dotted] (z\i)--(z\j);
						}
					}
					\draw (z1)--(z3);
					\draw (z1)--(z4);
					\draw[dotted] (z1)--(z5);
				\end{tikzpicture}
			\end{center}
			\caption[loftitle]{A regular $n$-gon with the roots of unity on its vertices}
			\label{figure:regular n-gon with roots of unity}
		\end{figure}

		Thus the equation that we wish to prove becomes
		\begin{equation}\label{eq:total length from one vertex to all others is n}
			\abs{1 - z_2}\abs{1 - z_3}\hdots\abs{1 - z_n} = n
		\end{equation}
		Note that $z_2, ..., z_n$ are the $n$th roots of unity other than 1.

		Let $z$ be a variable and consider the polynomial
		\begin{equation}\label{eq:roots of unity polynomial}
			P(z) := 1 + z + z^2 + \hdots + z^{n - 1}
		\end{equation}
		Since the roots of $P(z)$ are the $n$th roots of unity other than 1, we can factorize \cref{eq:roots of unity polynomial} into
		\begin{equation*}
			P(z) = (z - z_2)(z - z_3) \hdots (z - z_n)
		\end{equation*}
		Now let $z = 1$ and take the modulus of $P(z)$, and we get \cref{eq:total length from one vertex to all others is n}.
	\end{proof}
\end{ex}

\begin{ex}
	Let $n \in \mathbb{N}$. Show that $\sum_{j=0}^{n} \binom{3n}{3j} = \frac{2^{3n} + 2(-1)^n}{3}$.

	\begin{proof}
		Let $\alpha = e^{i \frac{2\pi}{3}}$. Then $\alpha$ is a cubic root of unity, i.e. $\alpha^3 = 1$, and from \cref{ex:sum of root of unity other than one is negative one}, $1 + \alpha + \alpha^2 = 0$.

		Consider
		\begin{align}
			\begin{split}\label{eq:combinatorial problem solved in complex terminology 1}
			(1 + 1)^{3n}
				=& \binom{3n}{0} + \binom{3n}{1} + \binom{3n}{2} + \binom{3n}{3} + \binom{3n}{4} \\
				&+ \binom{3n}{5} + \binom{3n}{6} + \hdots + \binom{3n}{3n}
			\end{split} \\
			\begin{split}\label{eq:combinatorial problem solved in complex terminology 2}
			(1 + \alpha)^{3n}
				=& \binom{3n}{0} + \binom{3n}{1}\alpha + \binom{3n}{2}\alpha^2 + \binom{3n}{3} + \binom{3n}{4}\alpha \\
				&+ \binom{3n}{5}\alpha^2 + \binom{3n}{6} + \hdots + \binom{3n}{3n}
			\end{split} \\
			\begin{split}\label{eq:combinatorial problem solved in complex terminology 3}
			(1 + \alpha^2)^{3n}
				=&\binom{3n}{0} + \binom{3n}{1}\alpha^2 + \binom{3n}{2}\alpha + \binom{3n}{3} + \binom{3n}{4}\alpha^2 \\
				&+ \binom{3n}{5}\alpha + \binom{3n}{6} + \hdots + \binom{3n}{3n}
			\end{split}
			\phantom{a}
		\end{align}

		Adding \cref{eq:combinatorial problem solved in complex terminology 1}, \cref{eq:combinatorial problem solved in complex terminology 2} and \cref{eq:combinatorial problem solved in complex terminology 3}, we observe that the terms with coefficients $\binom{3n}{k}$ where $k$ is not a multiple of 3 sums to 0 as given by $1 + \alpha + \alpha^2 = 0$, and therefore we obtain
		\begin{align*}
			2^{3n} + (1 + \alpha)^{3n} + (1 + \alpha^2)^{3n} &= 3 \sum_{j=0}^{n} \binom{3n}{3j} \\
			\frac{1}{3} \left[2^{3n} + (1 + \alpha)^{3n} + (1 + \alpha^2)^{3n}\right] &= \sum_{j=0}^{n} \binom{3n}{3j} \\
			\frac{1}{3} \left[2^{3n} + (-\alpha^2)^{3n} + (-\alpha)^{3n} \right] &= \sum_{j=0}^{n} \binom{3n}{3j} \quad \text{since } 1 + \alpha + \alpha^2 = 0 \\
			\frac{1}{3} \left[2^{3n} + (-1)^n + (-1)^n \right] &= \sum_{j=0}^{n} \binom{3n}{3j} \quad \text{since } \alpha^3 = 1 \\
			\frac{2^{3n} + 2(-1)^n}{3} &= \sum_{j=0}^{n} \binom{3n}{3j}
		\end{align*}
		as required.
	\end{proof}
\end{ex}

\begin{ex}
	Note that we can define $\Arg{z}$ in any interval of length $2 \pi$, i.e. it is not necessary that $\Arg{z} \in [0, 2\pi)$.

	For example, if we restrict $\Arg{z} \in [-\pi, \pi]$, then we can write
	\begin{equation*}
		\Arg{\left(-\frac{1}{\sqrt{2}} - \frac{1}{\sqrt{2}} i \right)} = - \frac{3\pi}{4}
	\end{equation*}

	Let $z$ be on the unit circle and $\Arg{z} \in [-\pi, \pi]$. Suppose that $z \notin \mathbb{R}$, i.e. $z \neq 1, z \neq -1$. Show that
	\begin{equation*}
		\Arg{\left( \frac{z-1}{z+1} \right)} = \begin{cases}
			\frac{\pi}{2} & \im{z} > 0 \\
			-\frac{\pi}{2} & \im{z} < 0
		\end{cases}
	\end{equation*}

	\begin{proof}
		Note that $\forall w_1, w_2 \in \mathbb{C}$, where $\Arg{w_1} = \tau_1, \Arg{w_2} = \tau_2$ for $\tau_1, \tau_2$ in the same $2\pi$-interval,
		\begin{equation}\label{eq:angle division modulo 2 pi}
			\Arg{\frac{w_1}{w_2} = \frac{e^{i\tau_1}}{e^{i\tau_2}} \equiv e^{i (\tau_1 - \tau_2)} = \Arg{w_1} - \Arg{w_2}}
		\end{equation}
		in modulo $2\pi$.

		Suppose $\im{z} > 0$. Let $\theta_1 = \Arg{(z - 1)}$ and $\theta_2 = \Arg{(z + 1)}$. Consider \cref{figure:angle division example}. Note that since both $\theta_1, \theta_2 \in [0, \pi]$, we have that $\theta_1 - \theta_2 \in [-\pi, \pi]$, and thus \cref{eq:angle division modulo 2 pi} holds true without the need of the condition of being in modulo $2 \pi$. We observe that
		\begin{align*}
			\frac{\pi}{2} &= \theta_2 + \pi - \theta_1 \\
			\theta_1 - \theta_2 &= \frac{\pi}{2}
		\end{align*}
		as desired.

		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}
					\tikzset{>=stealth}
					\draw[->] (-2, 0) -- ++(4, 0) coordinate (X);
					\draw[->] (0, -2) -- ++(0, 4);
					\coordinate (O) at (0,0);
					\draw (0,0) circle (1);
					\coordinate (z) at (60:1);
					\node[label={90:{$z$}},circle,fill,inner sep=1pt] at (z) {};
					\coordinate (z-1) at ($(z) + (-1,0)$);
					\node[label={90:{$z-1$}},circle,fill,inner sep=1pt] at (z-1) {};
					\coordinate (z+1) at ($(z) + (1,0)$);
					\node[label={90:{$z+1$}},circle,fill,inner sep=1pt] at (z+1) {};
					\draw[dotted] (z+1)--(z-1);
					\draw[dotted] (O)--(z-1);
					\draw[dotted] (O)--(z+1);
					\path (X) -- (O) -- (z-1) pic [draw,->,pic text=$\theta_1$, angle radius=0.3cm,angle eccentricity=1.6] {angle=X--O--z-1};
					\path (X) -- (O) -- (z+1) pic [draw,->,pic text=$\theta_2$, angle radius=1.2cm,angle eccentricity=1.2] {angle=X--O--z+1};
				\end{tikzpicture}
				\hspace{3cm}
				\begin{tikzpicture}
					\tikzset{>=stealth}
					\draw[->] (-2, 0) -- ++(4, 0) coordinate (X);
					\draw[->] (0, -2) -- ++(0, 4);
					\coordinate (O) at (0,0);
					\draw (0,0) circle (1);
					\coordinate (z) at (60:1);
					\node[label={90:{$z$}},circle,fill,inner sep=1pt] at (z) {};
					\coordinate (z-1) at ($(z) + (-1,0)$);
					\node[label={90:{$z-1$}},circle,fill,inner sep=1pt] at (z-1) {};
					\coordinate (z+1) at ($(z) + (1,0)$);
					\node[label={90:{$z+1$}},circle,fill,inner sep=1pt] at (z+1) {};
					\coordinate (-1) at (180:1);
					\coordinate (1) at (0:1);
					\draw[dotted] (z+1)--(z-1);
					\draw[dotted] (O)--(z-1);
					\draw[dotted] (O)--(z+1);
					\draw (-1)--(z);
					\draw (1)--(z);
					\path (X)--(-1)--(z) pic[draw,->,pic text=$\theta_2$,angle eccentricity=1.5] {angle=X---1--z};
					\path (X)--(1)--(z) pic[draw,->,pic text=$\theta_1$,angle radius=0.3cm,angle eccentricity=1.6] {angle=X--1--z};
					\tkzMarkRightAngle[size=0.2,color=black](-1,z,1);
				\end{tikzpicture}
			\end{center}
			\caption[loftitle]{(Right) Depicted question, (Left) Translated Angles}
			\label{figure:angle division example}
		\end{figure}

		Similarly, we can obtain $\theta_1 - \theta_2 = -\frac{\pi}{2}$ for when $\im{z} < 0$. This completes the proof.
	\end{proof}
\end{ex}

\begin{ex}
	Let $f(z) = e^z$ for $z \in \mathbb{C}$. Let $A = \{z = x + iy \in \mathbb{C} : x \leq 1, y \in [0, \pi] \}$. Describe the image of $f(A)$.

	\begin{solution}
		Firstly, note that
		\begin{align*}
			e^z &= e^{x + iy} \\
			e^x &\in (0, e] \\
			y &\in [0, \pi]
		\end{align*}

		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}
					\tikzset{>=stealth}
					\draw[->] (-2, 0) -- ++(4, 0) coordinate (X) node[below, right] {$\re$};
					\draw[->] (0, -2) -- ++(0, 4) node[above,right] {$\im$};
					\draw [draw=none,fill=base16-eighties-brown,fill opacity=0.25] (-2,0) rectangle (1,1.5);
					\draw (1,0)--(1,1.5)--(-2,1.5);
					\node[label={270:{$1$}}] at (1,0) {};
					\node[label={155:{$\pi$}}] at (0,1.5) {};
				\end{tikzpicture}
				\hspace{3cm}
				\begin{tikzpicture}
					\tikzset{>=stealth}
					\draw[->] (-2, 0) -- ++(4, 0) coordinate (X) node[below,right] {$\re$};
					\draw[->] (0, -2) -- ++(0, 4) node[above,right] {$\im$};
					\begin{scope}
					    \clip (-1,0) rectangle (1,1);
					    \draw[fill=base16-eighties-brown,fill opacity=0.25] (0,0) circle(1);
					    \draw (-1,0) -- (1,0);
					\end{scope}
					\node[label={270:{$e$}}] at (1,0) {};
					\node[label={270:{$-e$}}] at (-1,0) {};
					\node[label={155:{$e$}}] at (0,1) {};
				\end{tikzpicture}
			\end{center}
			\caption[loftitle]{(Right) Domain of $f(A)$, (Left) Image of $f(A)$}
			\label{figure:domain and image of f(A)}
		\end{figure}

		It is clear that the image will be in on the positive side of the imaginary-axis. Also, since $e^x \in (0, e]$, we get the right graph represented in \cref{figure:domain and image of f(A)}. The image of $f(A)$ is described in the left image of \cref{figure:domain and image of f(A)}.
	\end{solution}
\end{ex}

% section examples_for_nth_roots_of_unity (end)

% chapter lecture_4_jan_10th_2018 (end)

\chapter{Lecture 5 Jan 12 2018}
	\label{chapter:lecture_5_jan_12_2018}

\section{Complex Functions} % (fold)
\label{sec:complex_functions}

\subsection{Limits} % (fold)
\label{sub:limits}

\begin{defn}[Convergence]\label{defn:Convergence}
	A sequence of complex numbers $z_1, z_2, z_3, ...$ \hldefn{converges} to $z \in \mathbb{C}$ if
	\begin{equation}
		\lim_{n \to \infty} \abs{z_n - z} = 0
	\end{equation}
	or we may say
	\begin{equation}
		\forall \epsilon > 0 \enspace \exists N \in \mathbb{N} \enspace \forall n > N \enspace \abs{z_n - z} < \epsilon
	\end{equation}
\end{defn}

\begin{note}
 If $\{z_n\}_{n \in \mathbb{N}}$ converges to $z$, we may write $\lim_{n \to \infty} z_n = z$ or $z_n \to z$ (as $n \to \infty$).
\end{note}

\begin{eg}
	For $\abs{z} > 1$, does $\{\frac{1}{z^n}\}^\infty_{n = 1}$ converge? Explain.

	\begin{solution}
		We claim that the limit is 0. Since $\abs{z} > 1$, we have that
		\begin{align*}
			\lim_{n \to \infty} \abs{\frac{1}{z^n} - 0}
				&= \lim_{n \to \infty} \abs{\frac{1}{z}}^n \\
				&= 0
		\end{align*}

		Another way to prove this, since $\abs{z} > 1 \implies 0 < \abs{\frac{1}{z}} < 1$,
		\begin{gather*}
			\forall \epsilon = \abs{\frac{1}{z}} > 0 \\
			\abs{\frac{1}{z^n} - 0} = \abs{\frac{1}{z}}^n < \abs{\frac{1}{z}} = \epsilon
		\end{gather*}
	\end{solution}
\end{eg}

\begin{defn}[Convergence for Complex Functions]\label{defn:Convergence for Complex Functions}
	$\forall \Omega \subseteq \mathbb{C}$, let $f: \Omega \to \mathbb{C}$. We say that
	\begin{equation}
		\lim_{z \to z_0} f(z) = L
	\end{equation}
	for some $L \in \mathbb{C}$ if for every sequence $\{z_n\}_n \subseteq \Omega$ (not including $z_0$ if it is in $\Omega$), we have that
	\begin{equation}
		z_n \to z_0 \implies f(z_n) \to L
	\end{equation}
	Note that $L$ need not be in $\Omega$.
\end{defn}

\begin{eg}\label{eg:limit dne}
	Let $f(z) = \frac{\bar{z}}{z}, z \in \mathbb{C} \setminus \{0\}$. Find $\lim_{z \to 0} f(z)$.

	\begin{solution}
		Suppose $z = x \in \mathbb{R} \setminus \{0\}$. Then $f(z) = f(x) = \frac{x}{x} = 1$.

		Suppose $z = iy, y \in \mathbb{R} \setminus \{0\}$. Then $f(z) = f(iy) = \frac{-iy}{iy} = -1$.

		Therefore, the limit $\lim_{z \to 0} f(z)$ does not exist.
	\end{solution}
\end{eg}

\begin{ex}\label{ex:convergence in complex iff convergence of real and imaginary parts in real}
	Show that $z_n \to z \iff \re(z_n) \to \re(z) \land \im(z_n) \to \im(z)$. \\
	(Hint: $\abs{\re(z)}, \abs{\im(z)} \leq \abs{z} \leq \abs{\re(z)} + \abs{\im(z)}$)

	\begin{solution}
		Suppose $z_n \to z$. Then $\forall \epsilon_0 > 0 \; \exists N \in \mathbb{N} \; \forall n > N \; \abs{z_n - z} < \epsilon$. Note once and for all that
		\begin{gather*}
			\re(z_n - z) = \re(z_n) - \re(z) \\
			\im(z_n - z) = \im(z_n) - \im(z).
		\end{gather*}
		Thus
		\begin{align*}
			\abs{\re(z_n) - \re(z)} &= \abs{\re(z_n - z)} \\
					&\leq \abs{z_n - z} < \epsilon \\
			\abs{\im(z_n) - \im(z)} &= \abs{\im(z_n - z)} \\
					&\leq \abs{z_n - z} < \epsilon
		\end{align*}

		For the other direction,
		\begin{gather*}
			\forall \frac{\epsilon}{2} > 0 \enspace \exists N_0 \in \mathbb{N} \enspace \forall n > N_0 \enspace \abs{\re(z_n) - \re(z)} < \frac{\epsilon}{2} \\
			\forall \frac{\epsilon}{2} > 0 \enspace \exists N_1 \in \mathbb{N} \enspace \forall n > N_1 \enspace \abs{\im(z_n) - \im(z)} < \frac{\epsilon}{2}.
		\end{gather*}
		Therefore,
		\begin{align*}
			\abs{z_n - z} &= \abs{\re(z_n) + \im(z_n) - \re(z) - \im(z)} \\
				&\leq \abs{\re(z_n) - \re(z)} + \abs{\im(z_n) - \im(z)} \\
				&\leq \epsilon
		\end{align*}
		$\qed$
	\end{solution}
\end{ex}

% subsection limits (end)

\subsection{Continuity} % (fold)
\label{sub:continuity}

\begin{defn}[Continuity]\label{defn:Continuity}
	$\forall \Omega \subseteq \mathbb{C}$, let $f: \Omega \to \mathbb{C}$. We say that $f$ is \hldefn{continuous} at $z_0 \in \Omega$ if
	\begin{enumerate}
		\item $\forall \{z_n\}_{n \in \mathbb{N}} \\
				z_n \to z_0 \implies f(z_n) \to f(z_0)$
		\item $\forall \epsilon > 0 \enspace \exists \delta > 0 \\
				\abs{z - z_0} < \delta \implies \abs{f(z) - f(z_0)} < \epsilon$
	\end{enumerate}
\end{defn}

\begin{remark}
	\begin{enumerate}
		\item $f$ is continuous on $\Omega$ if it is continuous on every point in $\Omega$.
		\item We may \hlnotea{split} $f$ into its feal and imaginary parts, i.e.
			\begin{equation}\label{eq:complex function expressed in real-valued functions}
				f(z) = f(x, y) = u(x, y) + iv(x, y)
			\end{equation}
			where $u, v : \mathbb{R}^2 \to \mathbb{R}$.
	\end{enumerate}
\end{remark}

\begin{eg}
	Let $f: \mathbb{C} \to \mathbb{C}$ and for $z \in \mathbb{C}, f(z) = \frac{\bar{z}}{z}$. To split $f$ into real and imaginary parts:
	\begin{align*}
		f(z) &= \frac{\bar{z}}{z} \\
			&= (x + iy)\left(\frac{x}{x^2 + y^2} - i \frac{y}{x^2 + y^2} \right) \\
			&= \frac{x^2 - y^2}{x^2 + y^2} + i \frac{(-2xy)}{x^2 + y^2} 
	\end{align*}
	and we get
	\begin{align*}
		u(x, y) &= \frac{x^2 - y^2}{x^2 + y^2} \\
		v(x, y) &= -\frac{2xy}{x^2 + y^2} 
	\end{align*}
\end{eg}

% subsection continuity (end)

% section complex_functions (end)

% chapter lecture_5_jan_12_2018 (end)

\chapter{Lecture 6 Jan 15th 2018}
	\label{chapter:lecture_6_jan_15th_2018}

\section{Continuity (Continued)} % (fold)
\label{sec:continuity}

\begin{ex}\label{ex:continuity in complex iff continuity in real-valued functions in real}
	Let $f: \Omega \to \mathbb{C}$. Prove that $f(z)$ is continuous at $z_0 = x_0 + iy_0 \in \mathbb{C} \iff$ functions $u, v: \mathbb{R}^2 \to \mathbb{R}$, such that $f(z) = u(x, y) + iv(x,y)$ are both continuous at $(x_0, y_0)$.

	\begin{solution}
		We shall first prove the forward direction. Suppose that $f(z)$ is continuous at $z_0 = x_0 + iy_0 \in \mathbb{C}$. By \cref{defn:Continuity}, $\forall \{z_n\}_{n \in \mathbb{N}} \subseteq \Omega$, $z_n \to z_0 \implies f(z_n) \to f(z_0)$. By \cref{ex:convergence in complex iff convergence of real and imaginary parts in real},
		\begin{align}
			z_n \to z_0 &\iff \re z_n \to \re z_0 \land \im z_n \to \im z_0 \nonumber \\
				&\iff x_n \to x_0 \land y_n \to y_0 \label{eq:continuity in complex iff continuity in real-valued functions in real 1}
		\end{align}
		where $z_n = x_n + iy_n$ for $x_n, y_n \in \mathbb{R}$.

		Similarly so, and by \cref{eq:complex function expressed in real-valued functions},
		\begin{equation}\label{eq:continuity in complex iff continuity in real-valued functions in real 2}
			f(z_n) + f(z_0) \iff u(x_n, y_n) \to u(x_0, y_0) \land v(x_n, y_n) \to v(x_0, y_0)
		\end{equation}

		Putting together \cref{eq:continuity in complex iff continuity in real-valued functions in real 1} and \cref{eq:continuity in complex iff continuity in real-valued functions in real 2}, we get
		\begin{equation*}
			(x_n, y_n) \to (x_0, y_0) \implies u(x_n, y_n) \to u(x_0, y_0) \land v(x_n, y_n) \to v(x_0, y_0)
		\end{equation*}
		as desired.

		The proof of the other direction is simply a reversed process of the above. \qed
	\end{solution}
\end{ex}

% section continuity (end)

\section{Differentiability} % (fold)
\label{sec:differentiability}

\begin{defn}[Neighbourhood]\label{defn:Neighbourhood}
	For $z_0 \in \mathbb{C}, r \in \mathbb{R}$, let
	\begin{equation}\label{eq:neighbourhood_set}
		D(z_0, r) := \{ z \in \mathbb{C} : \abs{z - z_0} < r \}.
	\end{equation}
	On the complex plane, this is seen as a open disk centered around the point $z_0$ with radius $r$, as shown below.
	\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}
				\tikzset{>=stealth}
				\draw[->] (-1, 0) -- ++(5, 0) coordinate (X) node[below,right] {$\re$};
				\draw[->] (0, -1) -- ++(0, 5) node[above,right] {$\im$};
				\draw[dashed,fill=black,fill opacity=0.25] (2, 2) circle (1);
				\coordinate (a) at (1, 2);
				\coordinate (z0) at (2, 2);
				\node[label={0:{$z_0$}},fill,circle,inner sep=1pt] at (z0) {};
				\draw[<->] (z0)--(a) node[midway, above] {$r$};
			\end{tikzpicture}
		\end{center}
		\caption[loftitle]{Open disk centered around $z_0$ with radius $r$}
		\label{figure:neighbourhood_open_disk}
	\end{figure}
	This open disk is called a \hldefn{neighbourhood} of $z_0$.
\end{defn}

\begin{defn}[Differentiable/Holomorphic]\label{defn:Differentiable/Holomorphic}
	Let $f(z)$ be defined in a neighbourhood of $z_0 \in \mathbb{C}$. We say $f$ is \hldefn{differentiable/holomorphic} at $z_0$ if for some $h \in \mathbb{C}$,
	\begin{equation}\label{eq:holomorphic}
		\lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} 
	\end{equation}
	exists. If such a limit exists, we denote the limit by $f'(z_0)$.
\end{defn}

\begin{remark}
	$h \in \mathbb{C}: h$ need not necessarily be real. In this sense, $h$ approaches $0$ from \hlwarn{any direction} around $0 \in \mathbb{C}$.
\end{remark}

\begin{eg}
	For $z \in \mathbb{C} \setminus \{0\},$ let $f(z) = \frac{1}{z}$. Let $z_0 \in \mathbb{C} \setminus \{0\}$. Note that
	\begin{equation*}
		\lim_{h \to 0} \frac{\frac{1}{z_0 + h} - \frac{1}{z_0}}{h} = \lim_{h \to 0} \frac{1}{h} \left[ \frac{-h}{(z_0 + h)z_0} \right] = -\frac{1}{z_0^2} 
	\end{equation*}
	Thus $f$ is holomorphic at any $z \in \mathbb{C} \setminus \{0\}$, and hence $f'(z) = -\frac{1}{z}$.
\end{eg}

\begin{eg}
	For $z \in \mathbb{C},$ let $f(z) = \bar{z}$. Let $z_0 \in \mathbb{C}$. Notice that
	\begin{equation*}
		\lim_{h \to 0} \frac{\bar{z_0 + h} - \bar{z}}{h} = \lim_{h \to 0} \frac{\bar{h}}{h}.
	\end{equation*}
	From \cref{eg:limit dne}, we know that such a limit does not exist. Thus $f$ is not holomorphic on any $z \in \mathbb{C}$.
\end{eg}

\begin{ex}[Holomorphic Functions Properties]\label{ex:holomorphic_functions_properties}
	If $f, g$ are holomorphic at $z \in \mathbb{C}$, prove that
	\begin{enumerate}
		\item $f + g$ is holomorphic and $(f + g)' = f' + g'$. \label{item:holomorphic_linearity}
		\item $fg$ is holomorphic and $(fg)' = f'g + fg'$. \label{item:holomorphic_product_rule}
		\item if $g(z) \neq 0, \frac{f}{g}$ is holomorphic and $(\frac{f}{g})' = \frac{f'g - fg'}{g^2}$. \label{item:holomorphic_quotient_rule}
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item For $f + g$,
			\begin{align*}
					&\lim_{h \to 0} \frac{f(z + h) + g(z + h) - f(z) - g(z)}{h} \\
				= &\lim_{h \to 0} \left[ \frac{f(z + h) - f(z)}{h} + \frac{g(z + h) - g(z)}{g} \right] \\
				= & f'(z) + g'(z) 
			\end{align*}
			Thus $(f + g)' = f' + g'$.

			\item For $fg$,
			\begin{align*}
					&\lim_{h \to 0} \frac{f(z + h)g(z + h) - f(z)g(z)}{h} \\
				= &\lim_{h \to 0} \frac{f(z + h)g(z + h) + f(z)g(z + h) - f(z)g(z + h) - f(z)g(z)}{h} \\
				= &\lim_{h \to 0} \left[ \frac{f(z + h) - f(z)}{h} g(z + h) + f(z) \frac{g(z + h) - g(z)}{h} \right] \\
				= &f'(z)g(z) + f(z)g'(z)
			\end{align*}
			Therefore, $(fg)' = f'g + fg'$.

			\item When $\forall z \in \mathbb{C} \; g(z) \neq 0$, for $\frac{f}{g}$,
			\begin{align*}
					&\lim_{h \to 0} \frac{\frac{f(z + h)}{g(z + h)} - \frac{f(z)}{g(z)}}{h} \\
				= &\lim_{h \to 0} \frac{1}{h} \left[ \frac{f(z + h)g(z) - f(z)g(z + h)}{g(z + h)g(z)} \right] \\
				= &\lim_{h \to 0} \frac{1}{g(z + h)g(z)} \left[ \frac{f(z + h)g(z) + f(z)g(z) - f(z)g(z) - f(z)g(z + h)}{g} \right] \\
				= &\lim_{h \to 0} \frac{1}{g(z + h)g(z)} \left[ \frac{[f(z + h) - f(z)]g(z) - f(z)[g(z + h) - g(z)]}{h} \right] \\
				= &\frac{f'(z)g(z) - f(z)g'(z)}{g^2(z)}  
			\end{align*}
			Hence, $\frac{f}{g} = \frac{f'g - fg'}{g^2} $
		\end{enumerate}
	\end{solution}
\end{ex}

\begin{note}
	If we look at the example above from the perspective of $f$ being treated as a real-valued function, i.e. $f(z) = u(x, y) + iv(x, y)$ where $u, v: \mathbb{R}^2 \to \mathbb{R}$ and $z = x + iy$, observe that $\forall (x, y) \in \mathbb{R}^2, (x, y) \mapsto (x, -y)$, which we see that $u$ and $v$ are partially differentiable in $\mathbb{R}^2$.

	We will now look into this ``discrepancy''.
\end{note}

\subsection{Cauchy-Riemann Equations} % (fold)
\label{sub:cauchy_riemann_equations}

% subsection cauchy_riemann_equations (end)

Consider the following function taken from \cref{eq:holomorphic},
\begin{equation}\label{eq:holomorphic 2}
	f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}
\end{equation}
While $h$ may approach $0 \in \mathbb{C}$ from infinitely many sides on the complex plane, we will consider 2 cases.

\textit{Case 1: }$\mathit{h \to 0}$\textit{ via the real axis}

In this case, $h = x + i(0)$ and $x \to 0 \in \mathbb{R}$. Then \cref{eq:holomorphic 2} gives
\begin{align}
	f'(z_0) &= \lim_{x \to 0} \frac{u(x_0 + x, y_0) + iv(x_0 + x, y_0) - u(x_0, y_0) - iv(x_0, y_0)}{x} \nonumber \\
		&= \lim_{x \to 0} \left[ \frac{u(x_0 + x, y_0) - u(x_0, y_0)}{x} + i \frac{v(x_0 + x, y_0) - v(x_0, y_0)}{x} \right] \nonumber \\
		&= \frac{\partial u}{\partial x} \Bigr|_{(x_0, y_0)} + i \frac{\partial v}{\partial x} \Bigr|_{(x_0, y_0)} \label{eq:holomorphic approach via real axis}
\end{align}

\textit{Case 2: }$\mathit{h \to 0}$\textit{ via the imaginary axis}

In this case, $h = 0 + iy$ and $y \to 0 \in \mathbb{R}$. In a similar fashion, \cref{eq:holomorphic 2} becomes
\begin{align}
	f'(z_0) &= \lim_{y \to 0} \left[ \frac{u(x_0, y_0 + y) - u(x_0, y_0)}{iy} + \frac{v(x_0, y_0 + y) - v(x_0, y_0)}{y} \right] \nonumber \\
		&= \frac{1}{i} \cdot \frac{\partial u}{\partial y} \Bigr|_{(x_0, y_0)} + \frac{\partial v}{\partial y} \Bigr|_{(x_0, y_0)} \label{eq:holomorphic approach via imaginary axis}
\end{align}

Note that since $f'(z_0)$ exists, the real and imaginary part of \cref{eq:holomorphic approach via real axis} and \cref{eq:holomorphic approach via imaginary axis} must equate. Also note that $\frac{1}{i} = -i$. With that, we obtain the following theorem.

\begin{thm}[Cauchy-Riemann Equations]\label{thm:cauchy_riemann_equations}
	If $f(z)$ is holomorphic at $z_0 = x_0 + iy_0 \in \mathbb{C}$ where $x_0, y_0 \in \mathbb{R}$, then, at $(x_0, y_0)$,
	\begin{equation}\label{eq:cauchy_riemann_equations}
		\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} \quad \text{ and } \quad \frac{\partial v}{\partial x} = - \frac{\partial u}{\partial y}.
	\end{equation}
\end{thm}

% section differentiability (end)

% chapter lecture_6_jan_15th_2018 (end)

\chapter{Lecture 7 Jan 17th 2018}
	\label{chapter:lecture_7_jan_17th_2018}

\section{Differentiability (Continued)} % (fold)
\label{sec:differentiability_continued}

\subsection{Cauchy-Riemann Equations (Continued)} % (fold)
\label{sub:cauchy_riemann_equations_continued}

It is natural to wonder if the \hlnotea{converse} of \cref{thm:cauchy_riemann_equations} is true. We present the following example.

\begin{eg}\label{eg:counterexample_for_converse_of_cre}
	Let
	\begin{equation*}
		f(z) = \begin{cases}
			\frac{\bar{z}^2}{z} & \text{if } z \neq 0 \\
			0		& \text{if } z = 0
		\end{cases}
	\end{equation*}
	Check if
	\begin{enumerate}
		\item $f$ is holomorphic at $0$.
		\item \cref{thm:cauchy_riemann_equations} holds at $(0, 0)$.
	\end{enumerate}

	\begin{proof}
		\begin{enumerate}
			\item Observe that by letting $h = x_h + iy_h$ where $x_h, y_h \in \mathbb{R}$,
			\begin{equation*}
				\lim_{h \to 0} \frac{\frac{\bar{0 + h}^2}{0 + h} - 0}{h} = \lim_{h \to 0} \frac{\bar{h}^2}{h} = \lim_{x_h + iy_h \to 0} \left(\frac{x_h - iy_h}{x_h + iy_h} \right)^2
			\end{equation*}
			Consider $y_h = kx_h$, for $k \in \mathbb{R} \setminus \{0\}$. Then
			\begin{equation*}
				\lim_{x_h \to 0} \left( \frac{x_h - ikx_h}{x_h + ikx_h} \right)^2 = \left( \frac{1 - ik}{1 + ik} \right)^2,
			\end{equation*}
			where we see that the limit depends on the value of $k$. Therefore, the limit DNE. Hence $f$ is not holomorphic at $0$.

			\item Let $z = x + iy$ for $x, y \in \mathbb{R}$. Then
			\begin{equation*}
				\frac{\bar{z}^2}{z} = \frac{(x - iy)^2}{x + iy} = \frac{(x - iy)^3}{x^2 + y^2} = \frac{x^3 - 3xy^2}{x^2 + y^2} + i \frac{(-3x^2y + y^3)}{x^2 + y^2}
			\end{equation*}
			Therefore, we obtain
			\begin{align*}
				u(x, y) &= \begin{cases}
					\frac{x^3 - 3xy^2}{x^2 + y^2} & (x, y) \neq (0, 0) \\
					0	& (x, y) = (0, 0)
				\end{cases} \\
				v(x, y) &= \begin{cases}
					\frac{y^3 - 3x^2y}{x^2 + y^2} & (x, y) \neq (0, 0) \\
					0	& (x, y) = (0, 0)
				\end{cases}
			\end{align*}
			Observe that
			\begin{align*}
				\frac{\partial u}{\partial x} \Bigr|_{(0, 0)}
					&= \lim_{x \to 0} \frac{u(x, 0) - u(0, 0)}{x} = 1 \\
				\frac{\partial v}{\partial y} \Bigr|_{(0, 0)}
					&= \lim_{y \to 0} \frac{v(0, y) - v(0, 0)}{y} = 1 \\
					&\text{and} \\
				\frac{\partial u}{\partial y} \Bigr|_{(0, 0)}
					&= \lim_{y \to 0} \frac{u(0, y) - u(0, 0)}{y} = 0 \\
				\frac{\partial v}{\partial x} \Bigr|_{(0, 0)}
					&= \lim_{x \to 0} \frac{v(x, 0) - v(0, 0)}{x} = 0
			\end{align*}
			satisfies \cref{eq:cauchy_riemann_equations}.
		\end{enumerate}
		This illustrates that the converse of \cref{thm:cauchy_riemann_equations} is not true. We will, however, show that the converse will be true given an extra condition.
	\end{proof}
\end{eg}

\begin{thm}[Conditional Converse of CRE]\label{thm:conditional_converse_of_cre}
	Let $z_0 = x_0 + iy_0 \in \Omega \subseteq \mathbb{C}, x_0, y_0 \in \mathbb{R}$, and $u, v : \mathbb{R}^2 \to \mathbb{R}, f = u + iv : \Omega \to \mathbb{C}$. If
	\begin{enumerate}
		\item the partials of $u, v$ exist in a neighbourhood of $(x_0, y_0)$,
		\item the partials of $u, v$ are continuous at $(x_0, y_0)$, and
		\item $\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} \quad \text{and} \quad \frac{\partial v}{\partial x} = - \frac{\partial u}{\partial y} \quad \text{at } (x_0, y_0)$,
	\end{enumerate}
	then $f$ is holomorphic at $z_0$.
\end{thm}

A proof of the theorem is in page 36 of Newman and Bak (recommended text of PMATH352W18). I may include the proof whenever I am free.

% subsection cauchy_riemann_equations_continued (end)

\subsection{Power Series} % (fold)
\label{sub:power_series}

\begin{defn}[Power Series]\label{defn:power_series}
	A \hldefn{power series} in $\mathbb{C}$ is an infinite series of the form
	\begin{equation}\label{eq:generic_power_series_formula}
		\sum_{n \in \mathbb{N}} c_n z^n,
	\end{equation}
	where each $c_n \in \mathbb{C}$ is the coefficient of $z$ of the $n$-th power.
\end{defn}

In this subsection, we are interested to see if \cref{eq:generic_power_series_formula} converges.

Recall the notion of convergence in series from $\mathbb{R}$. \cref{eq:generic_power_series_formula} converges if the sequence of partial sums $\{S_N\}$ converges as $N \to \infty$, where
\begin{equation*}
	S_N := \sum_{n = 0}^{N} c_n z^n
\end{equation*}

In other words, using the same definition of $S_N$,
\begin{gather*}
	\forall \epsilon > 0 \quad \exists N \in \mathbb{N} \setminus	\{0\} \quad \forall n > N \\
	\abs{S_n - L} < \epsilon
\end{gather*}
where $L \in \mathbb{C}$ is the limit that the sequence converges to.

We also know that \cref{eq:generic_power_series_formula} converges absolutely if $\sum_{n = 0}^{\infty} \abs{c_n} \abs{z}^n$ converges. This is a stronger statement (i.e. absolute convergence $\implies$ convergence)
\begin{equation*}
	\because \abs{\sum_{n = 0}^{N} c_n z^n} \leq \sum_{n = 0}^{N} \abs{c_n} \abs{z}^n \quad \text{for each } N \in \mathbb{N}
\end{equation*}

\begin{eg}
	$\sum_{n = 0}^{\infty} z^n$ converges absolutely for $\abs{z} < 1$.

	Note that the partial sum of a geometric series is
	\begin{equation*}
		\sum_{n = 0}^{N} r^n = \frac{1 - r^{N + 1}}{1 - r}
	\end{equation*}
	and so the limit as $N \to \infty$ exists if $\abs{r} < 1$, and hence we see that
	\begin{equation*}
		\sum_{n = 0}^{N} r^n \to \frac{1}{1 - r} 
	\end{equation*}
	if $\abs{r} < 1$ as $N \to \infty$.

	However, if $\abs{z} = 1$, the power series diverges.
\end{eg}

Another note that we shall point out is that if \cref{eq:generic_power_series_formula} converges absolutely for some $z_0 \in \mathbb{C}$, then it converges absolutely for any $z$ where $\abs{z} < \abs{z_0}$.

These notions, in turn, begs the question of \hlwarn{what is the largest possible $\abs{z_0}$ for the series to converge absolutely}.

% subsection power_series (end)

% section differentiability_continued (end)

% chapter lecture_7_jan_17th_2018 (end)

\chapter{Lecture 8 Jan 19 2018}
	\label{chapter:lecture_8_jan_19_2018}

\section{Power Series (Continued)} % (fold)
\label{sec:power_series_continued}

\subsection{Radius of Convergence} % (fold)
\label{sub:radius_of_convergence}

\begin{thm}[Convergence in the Radius of Convergence]\label{thm:convergence_in_the_radius_of_convergence}
	For any power series $\sum_{n \in \mathbb{N}} c_n z^n$, $\exists 0 \leq R < \infty$, such that
	\begin{enumerate}
		\item $\abs{z} < R \implies $ series converges absolutely.
		\item $\abs{z} > R \implies $ series diverges.
	\end{enumerate}
	Moreover, $R$ is given by \hlnotea{Hadamard's Formula}:
	\begin{equation}\label{eq:hadamards_formula}
		\frac{1}{R} := \limsup_{n \to \infty} \abs{c_n}^\frac{1}{n}
	\end{equation}
\end{thm}

\begin{remark}
	\begin{enumerate}
		\item $R$ is called the \hldefn{radius of convergence} of the series. $\{z \in \mathbb{C} : \abs{z} < R\}$ is called the disk of convergence of the series.
		\item Recall the definition of the \hldefn{limit supremum}
		\begin{equation}
			\label{eq:defn_limsup}
			\limsup_{n \to \infty} a_n := \lim_{n \to \infty} (\sup_{m \geq n} a_m)
		\end{equation}
		which we may colloquially say as the ``highest peak `reached' by $a_n$'s as $n \to \infty$''
	\end{enumerate}
\end{remark}

\begin{propo}[A Property of limsup]\label{propo:a_property_of_limsup}\begin{gather*}
		\forall \{a_n\}_{n \in \mathbb{N}} \quad L := \limsup_{n \to \infty} a_n \implies \\
		\forall \epsilon > 0 \quad \exists N > 0 \quad \forall n > N \\
		L - \epsilon < a_n < L + \epsilon
	\end{gather*}
\end{propo}

(Proof to be included)

\begin{proof}[\cref{thm:convergence_in_the_radius_of_convergence}]
	Let $L := \frac{1}{R} = \limsup_{n \to \infty} \abs{c_n}^\frac{1}{n}$. Clearly, $L \geq 0$.
	\begin{enumerate}
		\item Suppose $\abs{z} < R$. $\exists \epsilon > 0, r := \abs{z}(L + \epsilon)$ such that $0 < r < 1$. By \cref{propo:a_property_of_limsup}, $\exists N \in \mathbb{N}, \forall n > N, \abs{c_n}^\frac{1}{n} < L + \epsilon$.

		Now since $L = \frac{1}{R}$,
		\begin{equation*}
			\sum_{n=N}^{\infty} \abs{c_n}\abs{z}^n = \sum_{n=N}^{\infty} (\abs{c_n}^\frac{1}{n} \abs{z})^n < \sum_{n=N}^{\infty} r^n
		\end{equation*}
		and since $0 < r < 1$, the final summation converges (as it is a geometric sum). Thus by comparison test, $\sum_{n=N}^{\infty} \abs{c_n} \abs{z}^n$ converges.

		We may also proceed with noticing that the partial sum of $\sum_{n=N}^{\infty} \abs{c_n}\abs{z}^n$ is \hlnotea{bounded and monotonic}, which shows that the series converges.

		\item Suppose $\abs{z} > R$. $\exists \epsilon > 0, r := \abs{z}(L - \epsilon)$ such that $r > 1$. By \cref{propo:a_property_of_limsup}, $\exists N \in \mathbb{N}, \forall n > N, \abs{c_n}^\frac{1}{n} > L - \epsilon$. Then analogous to the proof above,
		\begin{equation*}
			\sum_{n=N}^{\infty} \abs{c_n}\abs{z}^n = \sum_{n=N}^{\infty} (\abs{c_n}^\frac{1}{n} \abs{z})^n > \sum_{n=N}^{\infty} r^n
		\end{equation*}
		where the final summation diverges, and thus implying that $\sum_{n=N}^{\infty} \abs{c_n}\abs{z}^n$ diverges.
	\end{enumerate}
\end{proof}

\begin{thm}[Power function, holomorphic function, region of convergence]\label{thm:power_function_holomorphic_function_region_of_convergence}
	Suppose $f(z) = \sum_{n\in \mathbb{N}} c_n z^n$ has a radius of convergence $R \in \mathbb{R}$. Then $f'(z)$ exists and equals
	\begin{equation*}
		\sum_{n=1}^{\infty} nc_nz^{n - 1}
	\end{equation*}
	throughout $\abs{z} < R$.

	Moreover, $f'$ has the \hlimpo{same radius of convergence} as $f$.
\end{thm}

\begin{proof}
	Note that $f'$ has the same radius of convergence as $f$ since
	\begin{equation*}
		\limsup_{n \to \infty} \abs{nc_n}^\frac{1}{n} = \limsup_{n \to \infty} \abs{n}^\frac{1}{n} \abs{c_n}^\frac{1}{n} = \limsup_{n \to \infty} \abs{c_n}^\frac{1}{n}
	\end{equation*}
	where note that $\lim_{n \to \infty} \abs{n}^\frac{1}{n} = 1$.

	Let $\abs{z_0} \leq r < R$ and $g(z_0) := \sum_{n=1}^{\infty} nc_n z_0^{n - 1} $.

	\WTS
	\begin{equation*}
		\lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} = g(z_0)
	\end{equation*}
	OR
	\begin{gather*}\label{eq:power_function_defines_holomorphic_function_epsilondelta}\tag{$\dagger$}
		\forall \epsilon > 0 \enspace \exists \delta > 0 \\
		\abs{h} < \delta \implies \abs{\frac{f(z_0 + h) - f(z_0)}{h} -g(z_0)} < \epsilon
	\end{gather*}
	(Note that we would want $\delta > 0$ such that $\abs{z_0 + h} \leq r < R$)

	\WTP \eqref{eq:power_function_defines_holomorphic_function_epsilondelta}. $\forall \epsilon > 0$, choose $\delta > 0$. Suppose $\abs{z_0 + h} < \abs{z_0 + \delta} \leq r < R$. Write
	\begin{equation*}
		f(z) = \sum_{n=0}^{N} c_n z^n + \sum_{n=N}^{\infty} c_n z^n
	\end{equation*}
	and let $S_N(z)$ and $E_N(z)$ be the first and second terms respectively.
	Then we have that
	\begin{equation*}
		S_N'(z) = \sum_{n=1}^{N} nc_n z^{n - 1}
	\end{equation*}
	Consider
	\begin{align}
			&\abs{\frac{S_N(z_0 + h) - S_N(z_0)}{h} + \frac{E_N(z_0 + h) - E_N(z_0)}{h} - g(z_0) + S_N'(z_0) - S_N'(z_0)} \nonumber \\
		\leq &\abs{\frac{S_N(z_0 + h) - S_N(z_0)}{h} - S_N'(z_0)} + \abs{\frac{E_N(z_0 + h) - E_N(z_0)}{h}} + \abs{S_N'(z_0) - g(z_0)} \label{eq:power_function_defines_holomorphic_function_1}
	\end{align}
	For the second term, since $a^2 - b^2 = (a - b)(a^{n - 1} + a^{n - 2}b + \hdots ab^{n - 2} + b^{n - 1})$ and $\abs{z_0}, \abs{z_0 + h} < r \leq R$, we obtain $(z_0 + h)^n - z_0^n \leq hnr^{n - 1}$. Thus
	\begin{equation*}
		\abs{\frac{E_N(z_0 + h) - E_N(z_0)}{h}} = \abs{\frac{1}{h} \sum_{n=N+1}^{\infty} c_n [(z_0 + h)^n - z_0^n]} \leq \sum_{n=N+1}^{\infty} nc_nr^{n-1}.
	\end{equation*}

	Note that
	\begin{equation}\label{eq:power_function_defines_holomorphic_function_2}
		\sum_{n=1}^{\infty} nc_nr^{n-1} = g(r),
	\end{equation}
	and since $r < R$, \cref{eq:power_function_defines_holomorphic_function_2} converges absolutely by \cref{thm:convergence_in_the_radius_of_convergence}, thus the tail
	$\sum_{n=N+1}^{\infty} nc_n r^{n - 1}$ converges absolutely. Therefore, by comparison, we can pick $\frac{\epsilon}{3} > 0$ so that $\exists N_1 \in \mathbb{N}, \forall n > N_1$
	\begin{equation*}
		\abs{\frac{E_N(z_0 + h) - E_N(z_0)}{h}} < \frac{\epsilon}{3}.
	\end{equation*}
	
	For the third term in \cref{eq:power_function_defines_holomorphic_function_1}, we observe that by definition, $S_N'(z_0) = \sum_{n=1}^{N} nc_n z^{n - 1}$. Since
	\begin{equation*}
		\lim_{N \to \infty} S_N'(z_0) = \lim_{N \to \infty} \sum_{n=1}^{N} nc_n z^{n - 1} = \sum_{n=1}^{\infty} nc_n z^{n - 1} = g(z_0)
	\end{equation*}
	we know that we can pick $\frac{\epsilon}{3} > 0, \exists N_2 \in \mathbb{N}, \forall n > N_2$, we have
	\begin{equation*}
		\abs{S_N'(z_0) - g(z_0)} < \frac{\epsilon}{3}
	\end{equation*}.

	For the first term in \cref{eq:power_function_defines_holomorphic_function_1}, note that $(S_N(z_0))' = S_N'(z_0)$. Let $\frac{\epsilon}{3} > 0, \exists \delta > 0, \exists N > \max\{N_1, N_2\}, \forall n > N$, since $\abs{h} < \delta$, we have that
	\begin{equation*}
		\abs{\frac{S_N(z_0 + h) - S_N(z_0)}{h} - S_N'(z_0)} < \frac{\epsilon}{3}
	\end{equation*}

	This completes the proof. \qed
\end{proof}

% subsection radius_of_convergence (end)

% section power_series_continued (end)

% chapter lecture_8_jan_19_2018 (end)

\chapter{Lecture 9 Jan 22nd 2018}
	\label{chapter:lecture_9_jan_22nd_2018}

\section{Power Series (Continued 2)} % (fold)
\label{sec:power_series_continued_2}

\subsection{Radius of Convergence (Continued)} % (fold)
\label{sub:radius_of_convergence_continued}

\begin{eg}
	Let $f(z) = \sum_{n=1}^{\infty} \frac{z^n}{n}$. To find the radius of convergence, we use Hadamard's Formula:
	\begin{equation*}
		\frac{1}{R} = \limsup_{n \to \infty} \left(\frac{1}{n} \right)^\frac{1}{n} = 1 \qquad \because \lim_{n \to \infty} n^\frac{1}{n} = 1
	\end{equation*}
	Therefore $R = 1$. Thus, by \cref{thm:convergence_in_the_radius_of_convergence}, $f$ converges absolutely when $\abs{z} < 1$ and diverges when $\abs{z} > 1$. As for the boundary, i.e. $\abs{z} = 1$, consider the following two cases:
	\begin{enumerate}
		\item If $z = 1$, then $f(1) = \sum_{n=1}^{\infty} \frac{1}{n}$ is a \hlnotea{harmonic series}, and hence $f$ diverges.
		\item If $z = i$, then
		\begin{align*}
			f(i) &= \sum_{n=1}^{\infty} \frac{i^n}{n} \\
				 &= i - \frac{1}{2} + \frac{-i}{3} + \frac{1}{4} + \frac{i}{5} - \frac{1}{6} \\
				 &= \left(-\frac{1}{2} + \frac{1}{4} - \frac{1}{6} + \hdots	\right) + i \left(1 - \frac{1}{3} + \frac{1}{5} + \hdots \right).
		\end{align*}
		Observe that both the real and imaginary parts are alternating series where the absolute values of each term is decreasing, which, by the \hlnotea{alternating series test}, converge. Thus in this case, $f$ converges.
	\end{enumerate}
	Therefore, we observe that \hlnoteb{both convergence and divergence may occur} on the boundary, depending on the value of $z$.
\end{eg}

\begin{note}
	We may not always exchange the position of $\lim$ and $\sum_{a=1}^{b}$ when we consider an infinite sum (i.e. $b = \infty$). Here's an example why this is true. Consider the function $f(x) = \sum_{n=1}^{\infty} (x^n - x^{n - 1})$ for $\abs{x} < 1$. Is
	\begin{equation*}
		\lim_{x \to 1} \sum_{n=1}^{\infty} (x^n - x^{n - 1}) = \sum_{n=1}^{\infty} \lim_{x \to 1} (x^n - x^{n + 1})
	\end{equation*}
	true?

	Clearly, RHS is 0. For LHS, note that
	\begin{align*}
		f(x) &= \lim_{N \to \infty} \sum_{n=1}^{N} (x^n - x^{n + 1}) \\
			&= \lim_{N \to \infty} (x - x^2 + x^2 - x^3 + \hdots + x^N - x^{N + 1}) \\
			&= \lim_{N \to \infty} (x - x^{N + 1}) = x.
	\end{align*}
	So,
	\begin{equation*}
		\text{LHS} = \lim_{x \to 1} x = 1
	\end{equation*}
	And we see that RHS $\neq$ LHS.
\end{note}

\begin{defn}[Entire Function]
	A function $f$ is said to be \hldefn{entire} if $f$ is holomorphic in \hlnoteb{the entire complex plane}.
\end{defn}

\begin{ex}
	Define $e^z = \sum_{n=0}^{\infty} \frac{z^n}{n!}$. Show that
	\begin{enumerate}
		\item the radius of convergence of this series is $\infty$, and hence that $e^z$ is an entire function. (Hint: Use \hlnotea{Stirling's formula}: $n! \sim (\frac{n}{e})^n \sqrt{2 \pi n}$)
		\item $(e^z)' = e^z$
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item Using Stirling's formula, note that we have
			\begin{equation*}
				e^z = \sum_{n=0}^{\infty} \frac{1}{\sqrt{2 \pi n}} \left(\frac{ez}{n} \right)^n
			\end{equation*}
			To find $R$, we have
			\begin{align*}
				\frac{1}{R} &= \limsup_{n \to \infty} \abs{\frac{1}{\sqrt{2 \pi n}} \left(\frac{e}{n} \right)^n }^\frac{1}{n} \\
					&= \limsup_{n \to \infty} \abs{\frac{e}{n}} \limsup_{n \to \infty} \abs{\frac{1}{\sqrt{2 \pi n}}}^\frac{1}{n} \\
					&= 0
			\end{align*}
			since $\limsup_{n \to \infty} \frac{e}{n} = 0$ and $\limsup_{n \to \infty} \abs{\frac{1}{\sqrt{2 \pi n}}}^\frac{1}{n} = 1$. Thus $R = \infty$. By \cref{thm:convergence_in_the_radius_of_convergence}, $e^z$ is an entire function.

			\item Note that
			\begin{align*}
				\lim_{h \to 0} \frac{e^{z + h} - e^z}{h} &= e^z \lim_{h \to 0} \frac{e^h - 1}{h}  \\
					&= e^z \lim_{h \to 0} \frac{1 + h + \frac{h^2}{2} + \frac{h^3}{3} + \hdots - 1}{h} \\
					&= e^z
			\end{align*}
			Thus $(e^z)' = e^z$ as required.
		\end{enumerate}
	\end{solution}
\end{ex}

% subsection radius_of_convergence_continued (end)

% section power_series_continued_2 (end)

% chapter lecture_9_jan_22nd_2018(end)

\chapter{Lecture 10 Jan 24th 2018}
	\label{chapter:lecture_10_jan_24th_2018}

\section{Power Series (Continued 3)} % (fold)
\label{sec:power_series_continued_3}

\subsection{Radius of Convergence (Continued 2)} % (fold)
\label{sub:radius_of_convergence_continued_2}

A power series is infinitely $\mathbb{C}$-differentiable in its radius of convergence. All its derivatives are also power series, obtained by term-wise differentiation.

E.g.

\begin{equation*}
	f(z) - \sum_{n=0}^{\infty} c_n z^n \enspace \text{ then } \enspace f^{(2)}(z) = \sum_{n=0}^{\infty} n(n-1)c_n z^{n - 2}
\end{equation*}

In general, we may have $\sum_{n=0}^{\infty} c_n (z - z_0)^n$, which is a power series centered at $z_0 \in \mathbb{C}$. Then, as before, the radius of convergence of this power series is given by

\begin{equation*}
	\frac{1}{R} = \limsup_{n \to \infty} \abs{c_n}^\frac{1}{n}
\end{equation*}

So instead of having the disc of convergence centered around $0$, we now have one that is centered around $z_0$.

\begin{crly}[Corollary of \cref{thm:power_function_holomorphic_function_region_of_convergence}]
	From \cref{thm:power_function_holomorphic_function_region_of_convergence}, we have shown that

	\begin{tabular}{L{7cm} C{2cm} R{5cm}}
		$f(z)$ has a power series expansion at $z_0$ (i.e. $f(z) = \sum_{n=0}^\infty c_n (z - z_0)^n$ in some neighbourhood of $z_0$) with radius of convergence $R > 0$ &
		$\implies$ &
		$f$ is holomorphic at $z_0$
	\end{tabular}
\end{crly}

The converse of the statement above is true, i.e.

\begin{tabular}{L{5cm} C{2cm} R{7cm}}
	$f$ is holomorphic at $z_0$ &
	$\implies$ &
	$f(z)$ has a power series expansion at $z_0$ (i.e. $f(z) = \sum_{n=0}^\infty c_n (z - z_0)^n$ in some neighbourhood of $z_0$) with radius of convergence $R > 0$
\end{tabular}

This converse, however, is not possible to be proven given the current tools on our belt. And so we now have to venture into integrals in $\mathbb{C}$.

% subsection radius_of_convergence_continued_2 (end)

% section power_series_continued_3 (end)

\section{Integration in \texorpdfstring{$\mathbb{C}$}{C}} % (fold)
\label{sec:integration_in_c}

\subsection{Curves and Paths} % (fold)
\label{sub:curves_and_paths}

Before we begin with the definition of a curve in $\mathbb{C}$, let us consider how a straight line should be described as a vector-valued function in the complex plane. For instance, if we have two points $\alpha, \beta \in \mathbb{C}$, and we want to describe the straight line connecting the two.

\begin{tabular}{C{5cm} L{10cm}}
	\begin{tikzpicture}
		\coordinate (a) at (-1, -1);
		\coordinate (b) at (1, 1);
		\node[label={225:{$\alpha$}},circle,fill=base16-eighties-light,inner sep=1pt] at (a) {};
		\node[label={45:{$\beta$}},circle,fill=base16-eighties-light,inner sep=1pt] at (b) {};
		\draw[midarrow] (a)--(b);
	\end{tikzpicture} &
	Let $\gamma$ be the function that describes this line. We may then define $\gamma : [0, 1] \to \mathbb{C}$ to be either
	\begin{equation*}
		\gamma(t) = \alpha + (\beta - \alpha) t \enspace \text{ or } \enspace \gamma = \alpha(1 - t) + \beta t.
	\end{equation*}
	We would then have the following mapping:
\end{tabular}

\begin{figure}[H]
	\begin{center}
		\begin{tabular}{c c c}
			\begin{tikzpicture}
				\draw[-] (2, 0) -- (-2, 0) node[left] {$\mathbb{R}$};
				\node at (0, -2.5) {};
				\node at (0, 4) {};
				\node[label={270:{a}}] at (-1, 0) {[};
				\node[label={270:{b}}] at (1, 0) {]};
				\node[label={90:{c}},fill=base16-eighties-light,inner sep=1pt] at (0, 0) {};
			\end{tikzpicture} &
			\begin{tikzpicture}
				\draw[->] (0, 0) arc (135:45:2) node[midway,above] {$\gamma$};
				\node[label={0:{}}] at (0,-3) {};
			\end{tikzpicture} &
			\begin{tikzpicture}
				\draw[->] (-3, 0) -- ++(6, 0) node[right] {$\mathbb{C}$};
				\draw[->] (0, -2.5) -- (0, 2);
				\node[label={270:{a}},fill=base16-eighties-light,inner sep=1pt] at (-2.3, -1.287) {};
    			\node[label={90:{b}},fill=base16-eighties-light,inner sep = 1pt] at (1.2, 1.258) {};
    			\node[label={270:{c}},fill=base16-eighties-light,inner sep=1pt] at (-0.55,-1.011375) {};
				\clip (-2.3,-2.5) rectangle (1.2,1.2);
    			\draw[midarrow,domain=-2.5:1.5] plot (\x, {pow(\x,3)+2*pow(\x,2)-\x-2});
			\end{tikzpicture}
		\end{tabular}
	\end{center}
	\caption[loftitle]{Mapping from $\mathbb{R} \to \mathbb{C}$ with $\gamma$, which is called \hlnoteb{the curve $\gamma$}}
	\label{figure:map_from_r_to_c_with_upsilon}
\end{figure}

\begin{defn}[Curves in $\mathbb{C}$]\label{defn:curves_in_c}
	A curve in $\mathbb{C}$ is a continuous function, $\gamma(t): [a, b] \to \mathbb{C}$, where $a, b \in \mathbb{R}$. The image of $\gamma$ in $\mathbb{C}$ is called $\gamma^*$.
\end{defn}

\begin{eg}
	Let $z_0 \in \mathbb{C}, r > 0$. \\
	\begin{tabular}{L{9cm} R{5cm}}
		\begin{enumerate}
			\item Let $\gamma : [0, 2 \pi] \to \mathbb{C}$, such that $\gamma (t) = z_0 + re^{it}$.
			\item Let $\gamma' : [0, 1] \to \mathbb{C}$, such that $\gamma' (t) = z_0 + re^{2 \pi i t}$.
		\end{enumerate}
		The two functions above describe a circle centered at $z_0$ with radius $r$, anticlockwise-oriented. &
		\begin{tikzpicture}
			\draw[->] (-0.5, 0) -- (3, 0);
			\draw[->] (0, -0.5) -- (0, 3);
			\coordinate (z0) at (1.5,1.5);
			\draw (z0) circle (1);
			\node[label={0:{$z_0$}},fill=base16-eighties-light,inner sep=1pt] at (z0) {};
			\draw[<->] (z0)--(0.5,1.5) node[midway,above] {$r$};
			\node[label={0:{$t=0$ or $2 \pi$}},fill=base16-eighties-light,inner sep=1pt] at (2.5,1.5) {};
			\node[label={90:{$t=\frac{\pi}{2}$}},fill=base16-eighties-light,inner sep=1pt] at (1.5,2.5) {};
			\node[label={180:{$t=\pi$}},fill=base16-eighties-light,inner sep=1pt] at (0.5,1.5) {};
			\node[label={270:{$t=\frac{3\pi}{2}$}},fill=base16-eighties-light,inner sep=1pt] at (1.5,0.5) {};
		\end{tikzpicture}
	\end{tabular}
\end{eg}

We say that $\gamma$ and $\gamma'$ are equivalent parameterizations for the same oriented path.

\begin{defn}[Equivalent Parameterization]\label{defn:equivalent_parameterization}
	Let $\gamma_1 : [a, b] \to \mathbb{C}, \gamma_2 : [c, d] \to \mathbb{C}$ where $a, b, c, d \in \mathbb{C}$ describe the path $\gamma^*$. The two \hldefn{paramaterization are said to be equivalent} if $\exists h: [a, b] \to [c, d]$ that is a bijection and a continuous function such that
	\begin{equation*}
		\gamma_1(t) = \gamma_2(h(t))
	\end{equation*}
	where $t \in [a, b]$.
\end{defn}

\begin{note}
 	We will not look at functions like the Weierstrass function in this course.
\end{note}

\begin{defn}[Smooth Curve]\label{defn:smooth_curve}
	Let $\gamma : [a, b] \to \mathbb{C}, a, b, \in \mathbb{C}$. $\gamma$ is said to be smooth if its derivative $\gamma'$ exists and is continuous on $[a, b]$ and $\forall t \in [a, b], \gamma'(t) \neq 0$.
\end{defn}

\begin{defn}[Piecewise Smooth]\label{defn:piecewise_smooth}
	Let $\gamma: [a, b] \to \mathbb{C}$. $\gamma$ is said to be piecewise smooth if it is smooth on $[a, b]$ except on finitely many points in $[a, b]$.
\end{defn}

\begin{remark}
	Piecewise smooth curves shall be called paths.
\end{remark}

% subsection curves_and_paths (end)

\subsection{Integral} % (fold)
\label{sub:integral}

\begin{defn}[Contour]\label{defn:contour}
	Given a path $\gamma: [a, b] \to \mathbb{C}$ and $f : \mathbb{C} \to \mathbb{C}$, a function continuous on $\gamma$. We define the integral $f$ along $\gamma$, called a \hldefn{contour}, as
	\begin{equation}
		\int_{\gamma} f(z) \dif{z} := \int_{a}^{b} f(\gamma(t))\gamma'(t) dt
	\end{equation}
	where we let $z = \gamma(t)$ and hence $\dif{z} = \gamma'(t) dt$.
\end{defn}

\begin{remark}
	\begin{enumerate}
		\item Suppose $g$ is a complex-valued function, then
			\begin{equation*}
				\int_{a}^{b} g(t) dt = \int_{a}^{b} \re(g(t)) dt + i \int_{a}^{b} \im(g(t)) dt
			\end{equation*}\label{remark:integral_into_real_and_imaginary_parts}
		\item The integral of $f$ along $\gamma$ can be shown to be independent of the chosen parameterization for $\gamma^*$.

		\begin{proof}
			Let $a, b, c, d \in \mathbb{R}, \gamma_1 : [a, b] \to \mathbb{C}, \gamma_2 : [c, d] \to \mathbb{C}$ describe the same path $\gamma^*$. By \cref{defn:equivalent_parameterization}, define a bijection $h : [a, b] \to [c, d]$ that is a continuous function such that $t \mapsto \tau$, so that
			\begin{equation*}
				\gamma_1(t) = \gamma_2(h(t)) = \gamma(\tau).
			\end{equation*}
			Note that
			\begin{align*}
				\gamma_1'(t) &= h'(t)\gamma_2'(h(t)) \text{ and } \\
				h(t) &= \tau \implies h'(t) dt = d\tau.
			\end{align*}
			Now since $h$ is a bijection, we claim that $h(a) = c$ while $h(b) = d$.

			We know that $h$ cannot be a constant function. Suppose $h$ is an increasing function, then since $a \leq b$ and $c \leq d$, it is clear that $h(a) = c$ and $h(b) = d$. Similarly, if $h$ is a decreasing function, then $h(a) = d$ and $h(b) = c$. But this is a contradiction to our supposition that $\gamma_1$ and $\gamma_2$ describe the same orientation. Thus $h$ must be an increasing function, and hence we have $h(a) = c$ and $h(b) = d$.

			\hlnotec{(This can be more rigorous but that is an easy proof, and we may use perhaps the Approximation Property of $\mathbb{R}$ to that end, which is a fun exercise that shall not be included within these covers.)}

			Now
			\begin{align*}
				\int_{\gamma_1} f(z) \dif{z}
					&= \int_{a}^{b} f\big( \gamma_1(t) \big) \gamma_1'(t) dt \\
					&= \int_{a}^{b} f\Big( \gamma_2 \big( h(t) \big) \Big) h'(t) \gamma_2'(h(t)) dt \\
					&= \int_{c}^{d} f\big( \gamma_2(\tau) \big) \gamma_2'(\tau) d\tau \\
					&= \int_{\gamma_2} f(z) \dif{z}
			\end{align*}
			This completes the proof. \qed
		\end{proof}
	\end{enumerate}
\end{remark}

% subsection integral (end)

% section integration_in_c (end)

% chapter lecture_10_jan_24th_2018 (end)

\chapter{Lecture 11 Jan 26th 2018}
	\label{chapter:lecture_11_jan_26th_2018}

\section{Integration in \texorpdfstring{$\mathbb{C}$}{C} (Continued)} % (fold)
\label{sec:integration_in_c_continued}

\subsection{Integral (Continued)} % (fold)
\label{sub:integral_continued}

\begin{note}[Recall]
	Let $\gamma: [a, b] \to \mathbb{C}$ be a piecewise smooth curve. For a function $f$ that is continuous on $\gamma$, we defined
	\begin{align*}
		\int_{\gamma} f(z) \dif{z}
			&= \int_{a}^{b} f\big( \gamma(t) \big) \gamma'(t) dt \\
			&= \int_{a}^{b} \re\Big(f\big( \gamma(t) \big) \gamma'(t) \Big) dt + i \int_{a}^{b} \im\Big(f\big( \gamma(t) \big) \gamma'(t) \Big) dt
	\end{align*}
	and have
	\begin{gather*}
		\gamma'(t) = u'(t) + iv'(t) \\
		\text{if } \gamma(t) = u(t) + iv(t)
	\end{gather*}
\end{note}

\begin{eg}
	Let $f(z) = f(x + iy) = x^2 + y^2$ be continuous along $\gamma : [0, 1] \to \mathbb{C} \enspace t \mapsto t + it$. Evaluate $\int_{\gamma} f(z) \dif{z}$.
	\begin{solution}
		\begin{align*}
			\int_{\gamma} f(z) \dif{z}
				&= \int_{0}^{1} f(t + it)(1 + i) dt \\
				&= (1 + i)^2 \int_{0}^{1} t^2 dt \\
				&= (1 + i)^2 \cdot \frac{1}{3} t^3 \Big|_{0}^{1} \\
				&= \frac{2i}{3}
		\end{align*}
	\end{solution}
\end{eg}

\begin{eg}\label{eg:integral_of_path_on_a_circle}
	$\forall n \in \mathbb{Z}$, evaluate $\int_{\gamma} z^n \dif{z}$ that is continue on the path $\gamma$ that describes any circle centered at origin oriented anticlockwise.

	\begin{solution}
		Let $R \in \mathbb{R}$, and define
		\begin{gather*}
			\gamma: [0, 1] \to \mathbb{C} \enspace t \mapsto Re^{2\pi i t} \\
			\gamma'(t) = 2R\pi i e^{2\pi i t} = 2 \pi i \gamma(t)
		\end{gather*}
		Then
		\begin{align*}
			\int_{\gamma} z^n \dif{z}
				&= \int_{0}^{1} R^ne^{2 \pi i n t} \cdot 2 \pi i \cdot Re^{2 \pi i t} dt \\
				&= 2 \pi i R^{n + 1} \int_{0}^{1} e^{2 \pi i (n + 1) t} dt \\
				&= \begin{cases}
					\frac{R^{n + 1}}{n + 1} e^{2 \pi i (n + 1) t} \Big|_{0}^{1} & \text{if } n \in \mathbb{Z} \setminus \{-1\} \\
					2 \pi i t \Big|_{0}^{1} & \text{if } n = -1
				\end{cases} \\
				&= \begin{cases}
					\frac{R^{n + 1}}{n + 1} \left(e^{2 \pi i (n + 1)} - 1 \right) & \text{if } n \in \mathbb{Z} \setminus \{-1\} \\
					2 \pi i 	& \text{if } n = -1
				\end{cases} \enspace \because e^{2 \pi k i} \equiv 1 \mod 2 \pi\\
				&= \begin{cases}
					0	& \text{if } n \in \mathbb{Z} \setminus \{-1\} \\
					2 \pi i & \text{if } n = -1
				\end{cases}
		\end{align*}
		Note that our final answer does not depend on $R$, the radius of the circle.
	\end{solution}
\end{eg}

\begin{propo}[Properties of integrals in $\mathbb{C}$]\label{propo:properties_of_integrals_in_c}
	\begin{enumerate}
		\item \hlnotea{(Linearity)} Let $\alpha, \beta \in \mathbb{C}$. $\int_{\gamma} \left(\alpha f(z) + \beta g(z) \right) \dif{z} = \alpha \int_{\gamma} f(z) \dif{z} + \beta \int_{\gamma} g(z) \dif{z}$. \label{item:linearity_property_of_integrals}
		\item \begin{enumerate}
			\item For any complex-valued function $g$, and $b \geq a$,
				\begin{equation*}
					\abs{\int_{a}^{b} g(t) dt} \leq \int_{a}^{b} \abs{g(t)} dt
				\end{equation*} \label{item:absval_integral_less_than_integral_absval}
			\item For any function $f(z)$ that is continuous on a path $\gamma: [a, b] \to \mathbb{C}$,
					\begin{equation*}
						\abs{\int_{\gamma} f(z) \dif{z}} \leq \sup_{z \in \gamma} \abs{f(z)} \cdot \underbrace{\int_{a}^{b} \abs{\gamma'(t)} dt}_{\text{length of the path}}
					\end{equation*}\label{item:absval_integral_less_than_fnsup_times_arc_length}
		\end{enumerate}
		\item If $\gamma^-$ is the path $\gamma: [a, b] \to \mathbb{C}$ with a reversed direction, then
			\begin{equation*}
				\int_{\gamma^-} f(z) \dif{z} = - \int_{\gamma} f(z) \dif{z}
			\end{equation*}\label{item:reversed_path_orientation_gives_negative_integral}
	\end{enumerate}
\end{propo}

\begin{proof}
	\begin{enumerate}
		\item \begin{align*}
			\text{LHS}=&\int_{\gamma} \alpha f(z) + \beta g(z) \dif{z} \\
				= &\int_{\gamma} \alpha \Big( \re(f(z)) + i \im(f(z)) \Big) + \beta \Big( \re(g(z)) + i \im(g(z)) \Big) \dif{z} \\
				= \begin{split}
					&\alpha \int_{a}^{b} \Big( \re(f(z)) + i \im(f(z)) \Big) dt \\
					&\quad + \beta \int_{a}^{b} \Big( \re(g(z)) + i \im(g(z)) \Big) dt
				\end{split} \tag{$\dagger$}\label{eq:linearity_in_r} \\
				= & \alpha \int_{a}^{b} f(z) \dif{z} + \beta \int_{a}^{b} g(z) \dif{z}
			\end{align*}
			where \eqref{eq:linearity_in_r} is because of \cref{remark:integral_into_real_and_imaginary_parts} from an earlier remark and Linearity of Integrals in $\mathbb{R}$.\qed

		\item \begin{enumerate}
			\item Let $R \in \mathbb{R}$. Suppose $\int_{a}^{b} g(t) dt = Re^{i \theta}$. Then
				\begin{align*}
					\text{LHS} &= R = \int_{a}^{b} \underbrace{e^{-i \theta} g(t)}_{u(t) + iv(t)} dt \\
						&= \int_{a}^{b} u(t) dt + i \int_{a}^{b} v(t) dt
				\end{align*}
				Since $R \in \mathbb{R}$, we have $\int_{a}^{b} v(t) dt = 0$, so
				\begin{align*}
					R &= \int_{a}^{b} u(t) dt \\
					  &\leq \int_{a}^{b} \abs{u(t)} dt \quad \because a \leq b \land u(t) \leq \abs{u(t)} \in \mathbb{R} \\
					  &\leq \int_{a}^{b} \abs{u(t) + iv(t)} dt \\
					  &= \int_{a}^{b} \abs{e^{-i \theta} g(t)} \dif{t} \\
					  &= \int_{a}^{b} \abs{g(t)} dt = \text{RHS}
				\end{align*} \qed
			\item \begin{align*}
					\text{LHS} &= \abs{\int_{\gamma} f(z) \dif{z}} \\
						&= \abs{\int_{a}^{b} f\big( \gamma(t) \big) \gamma'(t) dt} \\
						&\leq \int_{a}^{b} \abs{f\big( \gamma(t) \big) \gamma'(t)} dt \quad \text{by } \cref{item:absval_integral_less_than_integral_absval} \\
						&\leq \int_{a}^{b} \sup_{z \in \gamma} \abs{f(z)} \abs{\gamma'(t)} dt \quad \text{since } \abs{f(z)} \leq \sup_{z \in \gamma} \abs{f(z)} \\
						&= \sup_{z \in \gamma} \abs{f(z)} \cdot \int_{a}^{b} \abs{\gamma'(t)} dt = \text{RHS}
				\end{align*}\qed
		\end{enumerate}
		\item Let $\gamma^-: [b, a] \to \mathbb{C}$ such that $\gamma^- = \gamma(b - t + a)$. Let $k = b - t + a$ so that $dk = -dt$, and $t = b \implies k = a \land t = a \implies k = b$. Note that $k \in [a, b]$ with this definition. Then
			\begin{align*}
				\text{LHS} &= \int_{\gamma^-} f(z) \dif{z} \\
					&= \int_{b}^{a} f\big( \gamma^- (t) \big) \gamma^{-'}(t) dt \\
					&= \int_{b}^{a} f\big( \gamma(b - t + a) \big) \gamma'(b - t + a) dt \\
					&= - \int_{a}^{b} f\big(\gamma(k)\big) \gamma'(k) dk \\
					&= - \int_{\gamma} f(z) \dif{z} = \text{RHS}
			\end{align*}
			as required. \qed
	\end{enumerate}
\end{proof}

We are now in a position to generalize the \hlnotea{Fundamental Theorem of Calclus} for $\mathbb{C}$.

% subsection integral_continued (end)

% section integration_in_c (end)

% chapter lecture_11_jan_26th_2018 (end)

\chapter{Lecture 12 Jan 29th 2018}
	\label{chapter:lecture_12_jan_29th_2018}

\section{Integration in \texorpdfstring{$\mathbb{C}$}{C} (Continued 2)} % (fold)
\label{sec:integration_in_c_continued_2}

\subsection{Fundamental Theorem of Calculus} % (fold)
\label{sub:fundamental_theorem_of_calculus}

To simplify statements from hereon, we shall use the following notations.

\begin{notation}
	Let $\Omega \subseteq \mathbb{C}$ be an open set in $\mathbb{C}$. We denote $f \in H(\Omega) \iff f $ is holomorphic on $\Omega$.
\end{notation}

\begin{thm}[Fundamental Theorem of Calculus]\label{thm:fundamental_theorem_of_calculus}
	Let $\gamma:[a,b] \to \mathbb{C}$ be a path inside an open set $\Omega \subseteq \mathbb{C}$. Suppose $f(z)$ is continuous on $\gamma$, and has an antiderivative $F \in \Omega$. Then
	\begin{equation}
		\label{eq:fundamental_theorem_of_calculus}
		\int_{\gamma} f(z) \dif{z} = F\big(\gamma(b)\big) - F\big(\gamma(a)\big)
	\end{equation}
\end{thm}

\begin{proof}
	Let $G = F \circ \gamma$ and suppose $\gamma$ is a smooth function. Since $\gamma$ is smooth, $\gamma'$ exists and is continuous on $[a, b]$ and $\gamma'(t) \neq 0$ for all $t \in [a, b]$, and since $f$ is continuous on $[a, b]$, $G(t) = F'(\gamma(t))\gamma'(t)$ is continuous as well.

	Now
	\begin{align*}
		\int_{\gamma} f(z) \dif{z}
			&= \int_{a}^{b} f\big(\gamma(t)\big)\gamma'(t) dt \\
			&= \int_{a}^{b} F'\big(\gamma(t)\big)\gamma'(t) dt \\
			&= \int_{a}^{b} G'(t) dt \\
			&= G(b) - G(a) \quad \text{by applying FTC in } \mathbb{R} \text{ to real and imaginary parts} \\
			&= F(\gamma(b)) - F(\gamma(a))
	\end{align*}

	If $\gamma$ is piecewise smooth, then we can simply apply the above to each of the smooth paths separately and sum up all of the integrals. \qed
\end{proof}

\begin{defn}[Closed Path]\label{defn:closed_path}
	A path $\gamma: [a, b] \to \mathbb{C}$ is said to be \hldefn{closed} if $\gamma(a) = \gamma(b)$.
\end{defn}

\begin{crly}[Corollary of FTC]\label{crly:corollary_of_ftc}
	If $F \in H(\Omega), \Omega \subseteq \mathbb{C}$ (hence $F'$ is continuous on $\Omega$), then
	\begin{equation*}
		\int_{\gamma} F'(z) \dif{z} = 0
	\end{equation*}
	on any closed path $\gamma$ on $\Omega$.
\end{crly}

\begin{proof}
	A closed path $\gamma: [a, b] \to \mathbb{C}$ has $\gamma(a) = \gamma(b)$. By \cref{thm:fundamental_theorem_of_calculus}, $\int_{\gamma} F'(z) \dif{z} = F(\gamma(b)) - F(\gamma(a)) = 0$ as required. \qed
\end{proof}

\begin{eg}
	Take $f(z) = z^n$ where $n \in \mathbb{Z} \setminus \{-1\}$ as in \cref{eg:integral_of_path_on_a_circle}. Then $f$ is continuous on $\mathbb{C} \setminus \{0\}$ \hlwarn{(not sure why this would be problematic when we've already excluded -1 for n)}. Then $f = F'$ for $F(z) = \frac{z^{n + 1}}{n + 1}$ and $F \in H(\mathbb{C} \setminus \{0\})$. Therefore by \cref{crly:corollary_of_ftc}, $\int_{\gamma} z^n \dif{z} = 0$ for any closed path $\gamma$ not passing through $0$.

	If we do include $-1$ for $n$, note that $F'$ would not be continuous on $0$, and thus the corollary would not apply. We have also shown in the earlier example that $\int_{\gamma} \frac{1}{z} \dif{z} = 2 \pi i$.
\end{eg}

\begin{note}[Recall]
	The \hldefn{interior} of a set $\Omega$ is defined as $\{z \in \Omega : \forall \epsilon > 0 \; B(z, \epsilon) \subseteq \Omega\}$, and denoted as $\Omega^0$.
\end{note}

\begin{thm}[Goursat's Theorem / Cauchy's Theorem for a triangle]\label{thm:goursat_s_theorem}
	Let $\Omega \subseteq \mathbb{C}$ be an open set. Suppose $\Delta \subseteq \Omega$ is a closed triangle whose interior is also contained in $\Omega$. Let $f \in H(\Omega)$. Then
	\begin{equation*}
		\int_{\Delta} f(z) \dif{z} = 0
	\end{equation*}
\end{thm}

This theorem holds more meaning than the presented statement, as it implies that, essentially, given any two points connected by two different paths in an open set in $\mathbb{C}$, and a function that is holomorphic over the two paths, the \hlnoteb{two path integrals of the function will yield the same result}!

\begin{proof}
	Let $\Delta_1^{(1)}, \Delta_2^{(1)}, \Delta_3^{(1)}, \Delta_4^{(1)}$ be smaller triangles by bisecting each side of $\Delta$. $\forall i \in \{1, 2, 3, 4\}$, orient $\Delta_i^{(1)}$ anticlockwise. Then we have
	\begin{equation}\label{eq:goursat_eq1}
		J := \int_{\Delta} f(z) \dif{z} = \sum_{i=1}^{4} \int_{\Delta_i^{(1)}} f(z) \dif{z} 
	\end{equation}
	Note that there must at least one of the $\Delta_i^{(1)}$ such that $\abs{\int_{\Delta_i^{(1)}} } \geq \frac{\abs{J}}{4}$, since $\forall i \in \{1, 2, 3, 4\}, \abs{\int_{\Delta_i^{(1)}} } < \frac{\abs{J}}{4}$ would contradict \cref{eq:goursat_eq1}. Without loss of generality, let $\Delta_1^{(1)}$ be the largest triangle of the four.

	Now note that each of the perimeter of $\Delta_i^{(1)}$ is half of the perimeter of $\Delta$. Let $\ell(x)$ be the perimeter of $x$. Continue with taking bisectors of $\Delta_1^{(1)}, \Delta_1^{(2)}, \hdots$ such that
	\begin{equation*}
		\Delta \supseteq \Delta_1^{(1)} \supseteq \Delta_1^{(2)} \supseteq \hdots,
	\end{equation*}
	then we have that for each $j \in \mathbb{N} \setminus \{0\}$, $\Delta_i^{(j)}$ is such that
	\begin{equation*}
		\abs{\int_{\Delta_i^{(j)}} f(z) \dif{z}} \geq \frac{\abs{J}}{4^j} 
	\end{equation*}
	and $\ell(\Delta_i^{(j)}) = \frac{1}{2^j} \ell (\Delta)$. By the \hlnotea{Nested Rectangle Theorem from Real Analysis}, $\exists z_0 \in \mathbb{C}$ such that $z_0 \in \Delta_i^{(j)}$ for all $j \in \mathbb{N} \setminus \{0\}$ that is a limit point. Since $z_0 \in \Omega \; \land \; f \in H(\Omega)$, we have that
	\begin{gather*}
		\forall z \in \Omega \enspace \forall \epsilon > 0 \enspace \exists \delta > 0 \\
		0 < \abs{z - z_0} < \delta \implies \abs{\frac{f(z) - f(z_0)}{z - z_0} - f'(z_0)} < \epsilon
	\end{gather*}

	Consider $D(z_0, \delta), \exists n \in \mathbb{N} \setminus \{0\}, \Delta_1^{(n)} \subseteq D(z_0, \delta)$. Consider
	\begin{equation*}
		\int_{\Delta_1^{(n)}} f(z) \dif{z} = \int_{\Delta_1^{(n)}} \big(f(z) - f(z_0) - f'(z_0)(z - z_0)\big) \dif{z},
	\end{equation*}
	where we note that $\int_{\Delta_1^{(n)}} f(z_0) \dif{z} = \int_{\Delta_1^{(n)}} f'(z_0)(z - z_0) \dif{z} = 0$ by \cref{crly:corollary_of_ftc}.

	By \cref{item:absval_integral_less_than_integral_absval} in \cref{propo:properties_of_integrals_in_c},
	\begin{align*}
		\abs{\int_{\Delta_1^{(n)}} f(z) \dif{z}}
			&\leq \int_{\Delta_1^{(n)}} \abs{f(z) - f(z_0) - f'(z_0)(z - z_0)} \dif{z} \\
			&\leq \int_{\Delta_1^{(n)}} \epsilon \abs{z - z_0} \dif{z} \leq \epsilon \ell(\Delta_1^{(n)})\int_{\Delta_1^{(n)}} \dif{z} = \epsilon \ell(\Delta_1^{(n)})^2
	\end{align*}
	where we note that $\abs{z - z_0} \leq \ell(\Delta_1^{(n)}$ since $z \in \Delta_1^{(n)}$. This implies that
	\begin{align*}
		\frac{\abs{J}}{4^n} \leq \epsilon \ell(\Delta_1^{(n)})^2 \leq \epsilon \frac{\ell(\Delta)^2}{4^n} = \abs{J} \leq \epsilon
	\end{align*}
	and therefore, $\abs{J} = 0$!
\end{proof}	

% subsection fundamental_theorem_of_calculus (end)

% section integration_in_c_continued_2 (end)

% chapter lecture_12_jan_29th_2018 (end)

\chapter*{Tutorial Jan 31 2018}
\addcontentsline{toc}{chapter}{Tutorial}

\begin{note}
	Consider the power series $\sum_{n \geq 0} a_n (z - z_0)^n$ and let $\frac{1}{R} := \limsup_{n \to \infty} \sup \sqrt[n]{\abs{a_n}} \in [0, \infty)$.
	\begin{itemize}
		\item If $\abs{z - z_0} < R$, $\sum_{n \geq 0} a_n (z - z_0)^n$ converges absolutely.
		\item If $\abs{z - z_0} > R, \sum_{n \geq 0} a_n (z - z_0)^n$ diverges.
		\item If $0 < r < R$, then $\sum_{n \geq 0} a_0 (z - z_0)^n$ converges uniformly on $\{z : \abs{z - z_0} < r\}$.
	\end{itemize}
\end{note}

\section{Practice Problems} % (fold)
\label{sec:practice_problems}

\begin{enumerate}
	\item Parameterize the semicircle $\abs{z - 4 - 5i} = 3$ clockwise, starting from $z = 4 + 8i$ to $z = 4 + 2i$.

		\begin{solution}
			Let $\gamma: [-\frac{\pi}{2}, \frac{\pi}{2}] \to \mathbb{C}$ such that $\gamma(t) = 3e^{-it} + 4 + 5i$. Note that $\gamma$ parameterizes the given semicircle:
			\begin{align*}
				\gamma\left(-\frac{\pi}{2}\right) &= 4 + 8i \\
				\gamma(0) &= 7 + 5i \\
				\gamma\left(\frac{\pi}{2} \right) &= 4 + 2i
			\end{align*}
			\begin{figure}[H]
				\begin{center}
					\resizebox{5cm}{!}{
						\begin{tikzpicture}
							\draw[->] (-0.5, 0) -- (8, 0) node[right] {$\re$};
							\draw[->] (0, -0.5) -- (0, 9) node[above] {$\im$};
							\draw[midarrow={latex}{0.7}] (4, 8) arc(90:-90:3);
							\coordinate (z0) at (4, 5);
							\coordinate (gs) at (6.1213203435596425732, 7.1213203435596425732);
							\node[label={270:{$4 - 5i$}},circle,fill,inner sep=1pt] at (z0) {};
							\node[label={90:{$\gamma(-\frac{\pi}{2})$}},circle,fill,inner sep=1pt] at (4, 8) {};
							\node[label={0:{$\gamma(0)$}},circle,fill,inner sep=1pt] at (7, 5) {};
							\node[label={270:{$\gamma(\frac{\pi}{2})$}},circle,fill,inner sep=1pt] at (4, 2) {};
							\draw[<->] (z0) -- (gs) node[midway,above] {$3$};
						\end{tikzpicture}
					}
				\end{center}
				\caption[loftitle]{Semicircle $\abs{z - 4 - 5i} = 3$ oriented clockwise, parameterized by $\gamma$}
			\end{figure}
		\end{solution}

	\item If the power series $f(z) = \sum_{n=0}^{\infty} c_n (z - z_0)^n$ centered at $z_0$ has a non-zero radius of convergence, then show that
		\begin{equation*}
			c_m = \frac{f^{(m)}(z_0)}{m!}
		\end{equation*}
		for any $m \in \mathbb{Z}, m \geq 0,$ where $f^{(m)}(z_0)$ denotes the $m$th derivative of $f$ at $z_0$. \label{item:tutorial_q2}

		\begin{solution}
			Since $f(z)$ is a power series and the radius of convergence $R \neq 0$, by \cref{thm:power_function_holomorphic_function_region_of_convergence}, $f(z)$ is $\mathbb{C}$-differentiable and each derivative has the same radius of convergence. By induction, it can be shown that
			\begin{equation*}
				f^{(m)}(z) = \sum_{n=m}^{\infty} \frac{n!}{(n - m)!} c_n (z - z_0)^{n - m} 
			\end{equation*}
			Evaluating $f^{(m)}$ at $z_0$, we have
			\begin{align*}
				f^{(m)}(z_0)
					&= \sum_{n=m}^{\infty} \frac{n!}{(n - m)!} c_n (z_0 - z_0)^{n - m} \\
					&= m! c_m
			\end{align*}
			where all terms above $m$ are 0. Then we obtain
			\begin{equation*}
				c_m = \frac{f^{(m)}(z_0)}{m!}
			\end{equation*}
			as desired. \qed
		\end{solution}
	\item Let $\gamma$ be the arc of the unit circle centered at the origin in the first quadrant oriented clockwise (from $i$ to 1). Evaluate the integral
		\begin{equation*}
			\int_{\gamma} \bar{z}^2 \dif{z}
		\end{equation*}
		by parameterizing the curve.

		\begin{solution}
			Consider the parameterization $\gamma: [ -\frac{\pi}{2}, 0 ] \to \mathbb{C}$ given by $\gamma(t) = e^{-it}$. Note that $\bar{e^{-it}} = e^{it}$. Then
			\begin{align*}
				\int_{\gamma} \bar{z}^2 \dif{z}
					&= \int_{-\frac{\pi}{2}}^{0} e^{2it} \cdot \left( -i e^{-it} \right) dt \\
					&= -i \int_{-\frac{\pi}{2}}^{0} e^{it} dt \\
					&= -e^{it} \at{\frac{\pi}{2}}{0} \\
					&= - 1 - i
			\end{align*}\qed
		\end{solution}
	\item Evaluate the above integral by finding an antiderivative. (Hint: Use $\left(\frac{z\bar{z}}{z} \right)^2$)
		\begin{solution}
			Note that $z\bar{z} = \abs{z}^2$, so on the circle, we have $\bar{z} = \frac{1}{z}$. Thus the integral is equivalent to
			\begin{equation*}
				\int_{\gamma} \frac{1}{z^2} \dif{z}
			\end{equation*}
			Note that the antiderivative of $\frac{1}{z^2}$ is $-\frac{1}{z}$. Thus by \cref{thm:fundamental_theorem_of_calculus},
			\begin{equation*}
				\int_{\gamma} \bar{z}^2 \dif{z} = \int_{\gamma} \frac{1}{z^2} = F(\gamma(0)) - F\left(\gamma\left(-\frac{\pi}{2}\right)\right) = -\frac{1}{e^{-i(0)}} + \frac{1}{e^{-i(-\pi/2)}} = - 1 - i 
			\end{equation*}
		\end{solution}

	\item Let $\{c_n\}_{n = 0}^\infty$ be a sequence of positive real numbers such that
		\begin{equation*}
			L = \lim_{n \to \infty} \frac{c_{n + 1}}{c_n}
		\end{equation*}
		exists. Then show that
		\begin{equation*}
			\lim_{n \to \infty} c_n^{\frac{1}{n}} = L
		\end{equation*}
		This shows that, when applicable, the \hlnotea{ratio test} can be used instead of the root test to calculate the radius of convergence of a power series.

		\begin{solution}
			Suppose that
			\begin{equation*}
				L = \lim_{n \to \infty} \frac{c_{n + 1}}{c_n}
			\end{equation*}
			exists. By definition, we have
			\begin{gather*}
				\forall \epsilon > 0 \enspace \exists N \in \mathbb{N} \enspace \forall n > N \\
				\abs{\frac{c_n}{c_{n - 1}} - L} < \epsilon
			\end{gather*}
			Thus for $n \geq N$,
			\begin{align*}
				c_n ^\frac{1}{n}
					&= \left(\frac{c_n}{c_{n - 1}} \cdot \frac{c_{n - 1}}{c_{n - 2}} \hdots \frac{c_N}{c_{N - 1}} \cdot c_{N - 1} \right)^\frac{1}{n} \\
					&= \left(\frac{c_n}{c_{n - 1}}\right)^\frac{1}{n} \left(\frac{c_{n - 1}}{c_{n - 2}} \right)^\frac{1}{n} \hdots \left(\frac{c_N}{c_{N - 1}} \right)^\frac{1}{n} c_{N - 1}^\frac{1}{n}
			\end{align*}
			Now
			\begin{alignat*}{2}
				(L - \epsilon)^\frac{1}{n} (L - \epsilon)^\frac{1}{n} \hdots (L - \epsilon)^\frac{1}{n}  c_{N - 1}^\frac{1}{n} &\leq c_n^\frac{1}{n} &&\leq (L + \epsilon)^\frac{1}{n} (L + \epsilon)^\frac{1}{n}  \hdots (L + \epsilon)^\frac{1}{n} c_{N - 1}^\frac{1}{n} \\
				(L - \epsilon)^\frac{n - N + 1}{n} c_{N - 1}^\frac{1}{n} &\leq c_n^\frac{1}{n} &&\leq (L + \epsilon)^\frac{n - N + 1}{n} c_{N - 1}^\frac{1}{n} 
			\end{alignat*}
			Note that
			\begin{align*}
				\lim_{n \to \infty} (L - \epsilon)^\frac{n - N + 1}{n} c_{N - 1}^\frac{1}{n} &= L - \epsilon \\
				\lim_{n \to \infty} (L + \epsilon)^\frac{n - N + 1}{n} c_{N - 1}^\frac{1}{n} &= L + \epsilon
			\end{align*}
			Thus we have
			\begin{gather*}
				L - \epsilon \leq c_n^\frac{1}{n} \leq L + \epsilon \\
				\abs{c_n^\frac{1}{n} - L} \leq \epsilon
			\end{gather*}
			as desired. \qed
		\end{solution}

	\item Find the radius of convergence of
		\begin{enumerate}
			\item $\sum_{n=0}^{\infty} \frac{n^n z^n}{n!}$
			\item $\sum_{n=0}^{\infty} z^{2^n}$
		\end{enumerate}

		\begin{solution}
			\begin{enumerate}
				\item By Stirling's Approximation, i.e. $n! \sim (\frac{n}{e})^n \sqrt{2 \pi n}$, we have that Hadamard's formula is
					\begin{align*}
						\frac{1}{R} &= \limsup_{n \to \infty} \abs{\frac{n^n}{n!}}^\frac{1}{n} \\
							&= \limsup_{n \to \infty} \abs{\frac{n^n}{\left(\frac{n}{e} \right)^n \sqrt{2 \pi n}}}^\frac{1}{n} \\
							&= \limsup_{n \to \infty} \abs{\frac{e^n}{\sqrt{2 \pi n}} }^\frac{1}{n} \\
							&= e \limsup_{n \to \infty} \abs{\frac{1}{\sqrt{2 \pi n}} }^\frac{1}{n} = e
					\end{align*}
					Therefore, $R = \frac{1}{e}$.

					\item \hlwarn{no solution yet: current problem, not being able to express the sum as a power series, in turn failing to get $c_n$ which is needed for $\frac{1}{R}$.}
			\end{enumerate}
		\end{solution}
	\item Show that for any path $\gamma: [a, b] \to \mathbb{C}$ and $f(z)$ continuous on $\gamma$, we have
		\begin{equation*}
			\abs{\int_{\gamma} f(z) \dif{z}} \leq \sup_{z \in \gamma} \abs{f(z)} \int_{a}^{b} \abs{\gamma'(t)} dt
		\end{equation*}

		\begin{solution}
			\begin{align*}
				\text{LHS} &= \abs{\int_{\gamma} f(z) \dif{z}} \\
					&= \abs{\int_{a}^{b} f\big( \gamma(t) \big) \gamma'(t) dt} \text{ by definition} \\
					&\leq \int_{a}^{b} \abs{f\big( \gamma(t) \big) \gamma'(t)} dt \quad \text{by } \cref{item:absval_integral_less_than_integral_absval} \text{ of } \cref{propo:properties_of_integrals_in_c} \\
					&\leq \int_{a}^{b} \sup_{z \in \gamma} \abs{f(z)} \abs{\gamma'(t)} dt \quad \text{since } \abs{f(z)} \leq \sup_{z \in \gamma} \abs{f(z)} \\
					&= \sup_{z \in \gamma} \abs{f(z)} \cdot \int_{a}^{b} \abs{\gamma'(t)} dt = \text{RHS}
			\end{align*}
		\end{solution}
\end{enumerate}

% section practice_problems (end)

% chapter tutorial_jan_31_2018 (end)

\chapter{Lecture 13 Feb 9th 2018}
	\label{chapter:lecture_13_feb_9th_2018}

\section{Cauchy's Integral Formula} % (fold)
\label{sec:cauchy_s_integral_formula}

\begin{defn}[Convex Set]\label{defn:convex_set}
	A set $S \subseteq \mathbb{C}$ is called a \hldefn{convex set} if the line segment joining any pair of points in $S$ lies entirely in $S$.
\end{defn}

\begin{thm}[Cauchy's Theorem for Convex Set]\label{thm:cauchy_s_theorem_for_convex_set}
	Let $\Omega \subseteq \mathbb{C}$ be a convex open set, and $f \in H(\Omega)$. Then
	\begin{enumerate}
	 	\item $f = F'$ for some $F \in H(\Omega)$.
	 	\item $\int_{\gamma} f(z) \dif{z} = 0$ for any closed path $\gamma \in \Omega$.
	 \end{enumerate} 
\end{thm}

\begin{proof}
	Note that it is sufficient to prove 1 since 1 $\implies$ 2 by \cref{thm:fundamental_theorem_of_calculus}.

	Let $a \in \Omega$, and let $[a, z]$ denote the straight line from $a$ to $z$. Since $\Omega$ is a convex set, $[a, z]$ is in $\Omega$. Define $F(z)$\footnote{It can be verified that $F$ is continuous.} $= \int_{[a, z]} f(z) \dif{z}$\footnote{This is a keys step: defining an ``antiderivative'' as how we would expect it to be.}.

	\begin{tabular}{L{9cm} C{6cm}}
		\hlnotec{WTS} $F \in H(\Omega), F'(z_0) = f(z_0)$ for any $z_0 \in \Omega$.

		Now by \cref{thm:goursat_s_theorem},

		{$\!\begin{aligned}
			0 &= \int_{\Delta} f(z) \dif{z} \\
			  &= \int_{[a, z]} f(z) \dif{z} + \int_{[z, z_0]} f(z) \dif{z} +  \int_{[z_0, a]} f(z) \dif{z} \\
			  &= F(z) + \int_{[z, z_0]} f(z) \dif{z} + \left(- F(z_0) \right)
		\end{aligned}$}
		&
		\resizebox{6cm}{!}{
		\begin{tikzpicture}
            \draw  plot[smooth, tension=.7] coordinates {(-3.5,0.5) (-3,2.5) (-1,3.5) (1.5,3.75) (4,3.5) (5,2.5) (5,0.5) (2.5,-2) (0,-3) (-3,-2) (-3.5,0.5)};
            \node[label={45:{$\Omega$}}] at (5,3) {};
            \coordinate (a) at (-1, 2);
            \coordinate (z) at (-1.5, -1);
            \coordinate (z0) at (4, 0.5);
            \node[label={90:{$a$}},circle,fill=base16-eighties-light,inner sep=1pt] at (a) {};
            \node[label={270:{$z$}},circle,fill=base16-eighties-light,inner sep=1pt] at (z) {};
            \node[label={0:{$z_0$}},circle,fill=base16-eighties-light,inner sep=1pt] at (z0) {};
            \draw[midarrow] (a) -- (z);
            \draw[midarrow] (z) -- (z0);
            \draw[midarrow] (z0) -- (a);
            \node[label={60:{$\Delta$}}] at (1.5, 1.5) {};
        \end{tikzpicture}
        }
	\end{tabular}

	This implies that
	\begin{equation*}
		F(z) - F(z_0) = \int_{[z_0, z]} f(z) \dif{z}.
	\end{equation*}
	Divide both sides by $z - z_0$, then
	\begin{align*}
		\frac{F(z) - F(z_0)}{z - z_0} - f(z_0)
			&= \frac{1}{z - z_0} \int_{[z_0, z]} f(z) \dif{z} - f(z_0) \\
			&= \frac{1}{z - z_0} \int_{[z_0, z]} f(z) - f(z_0) \dif{z} \quad \text{since } \int_{[z_0, z]} \dif{z} = z - z_0
	\end{align*}
	Since $f \in H(\Omega)$ and is hence continuous, we have that
	\begin{gather*}
		\forall \epsilon > 0 \; \exists \delta > 0 \\
		\abs{z - z_0} < \delta \implies \abs{f(z) - f(z_0)} < \epsilon
	\end{gather*}
	which in turn implies that
	\begin{equation*}
		\abs{\frac{F(z) - F(z_0)}{z - z_0} - f(z_0)} = \abs{\frac{1}{z - z_0} \int_{[z_0, z]} \left[f(z) - f(z_0)\right] \dif{z}} \leq \frac{1}{\abs{z - z_0}} \abs{\int_{[z_0, z]} \epsilon \; \dif{z}} = \epsilon
	\end{equation*}
	Hence, by first principle, $F'(z_0) = f(z_0)$. \qed
\end{proof}

\begin{thm}[Cauchy's Integral Formula 1]\label{thm:cauchy_s_integral_formula_1}
Let $\Omega \subseteq \mathbb{C}$ be a convex open set, and $C$ be a closed circle path in $\Omega$. If $w \in \Omega \setminus \partial C$, where $\partial C$ is the \hldefn{boundary of $C$}, and $f \in H(\Omega)$, then

	\begin{tabular}{L{10cm} C{5cm}}
		\begin{equation*}
			f(w) \ind{w}{C} = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w} \dif{z}
		\end{equation*}
		where
		\begin{equation*}
			\ind{w}{C} = \frac{1}{2 \pi i} \int_{\gamma} \frac{\dif{z}}{z - w}
		\end{equation*}
		denotes the number of times the countour $C$ winds around the point $w$.
		&
		\resizebox{5cm}{!}{
		\begin{tikzpicture}
			\draw  plot[smooth, tension=.7] coordinates {(-3.5,0.5) (-3,2.5) (-1,3.5) (1.5,3.5) (3,2.5) (2.5,-1) (0,-2) (-3,-1.5) (-3.5,0.5)};
            \node[label={45:{$\Omega$}}] at (3,3) {};
            \draw[midarrow={latex}{0.1}] (0, 0) circle (1.5);
            \node[label={45:{$C$}}] at (1, 1) {};
            \node[label={315:{$w$}},circle,fill=base16-eighties-light,inner sep=1pt] at (0.5, 0.5) {};
		\end{tikzpicture}
		}
	\end{tabular}
	is called the \hlnoteb{index of $w$ with respect to $C$}, or the \hldefn{winding number} of $C$ around $w$.
\end{thm}

\begin{proof}
	$ $ \\
	\begin{tabular}{L{9cm} C{6cm}}
		Let $w \in \Omega \setminus \partial C$. Define
		\begin{equation*}
			g(w) = \begin{cases}
				\frac{f(z) - f(w)}{z - w} & \text{if } z \neq w \\
				f'(w) 	& \text{if } z = w
			\end{cases}
		\end{equation*}
		By the construction of $g$, $g$ is continuous on $\Omega$, and $g \in H(\Omega \setminus \{w\})$.

		\hlnotec{We need to} construct a convex set $\Omega' \subseteq \Omega$ that contains $\gamma_{\delta, \epsilon}$ such that $g \in H(\Omega')$.
		&
		\resizebox{6cm}{!}{
		\begin{tikzpicture}
			\draw  plot[smooth, tension=.7] coordinates {(-3.5,0.5) (-3,2.5) (-1,3.5) (1.5,3.5) (3,2.5) (2.5,-1) (0,-2) (-3,-1.5) (-3.5,0.5)};
            \node[label={45:{$\Omega$}}] at (3,3) {};
            \centerarc[midarrow={latex}{0.5}](0,0)(50:400:1.5);
            \node[label={180:{$\gamma_{\delta, \epsilon}$}}] at (-1.4,-0.5) {};
            \node[label={0:{$C$}}] at (1.5,0) {};
            \coordinate (w) at (0.5, 0.5);
            \node[label={270:{$w$}},circle,fill=base16-eighties-light,inner sep=1pt] at (w) {};
            \centerarc[midarrow={latex}{0.5}](w)(390:60:0.5);
            \node[label={270:{$C_\epsilon$}}] at (0.5, 0) {};
            \draw[<->] (w) -- (0, 0.5) node[midway,above] {$\epsilon$};
            \draw[midarrow] (0.75, 0.93301270189221932338) -- (0.96418141452980898948, 1.1490666646784670528);
            \draw[midarrow] (1.1490666646784670528, 0.96418141452980898948) -- (0.93301270189221932338, 0.75);
            \draw[<->] (1.07418141452980898948, 1.2590666646784670528) -- (1.2590666646784670528, 1.07418141452980898948) node[midway,above] {$\delta$};
		\end{tikzpicture}
		}
	\end{tabular}

	We now follow a similar argument as in the proof for \cref{thm:cauchy_s_theorem_for_convex_set}. Let $\epsilon > 0$ such that $\exists \delta > 0$,  so that we can define the ``keyhole'' $\gamma_{\delta, \epsilon}$ which omits $w$. Consider $D(w, \epsilon)$, call the image of the border of $D(w, \epsilon)$ as $C_\epsilon$, let $\delta$ be the width of the ``corridor'', and the two paths that are the ``sides of the corridor'' be called $C_{\delta_1}, C_{\delta_2}$ respectively. Define $G(z) = \int_{[a, z]} g(z) \dif{z}$, where $a$ and $z$ are in the interior of $C$ but not in the interior of $C_\epsilon$. Then if we define a set $\Omega'$ such that it contains the interior of $\gamma_{\delta, \epsilon}$, we have that $\Omega'$ is a convex open set, and $G \in H(\Omega')$. By \cref{thm:cauchy_s_theorem_for_convex_set}, $G' = g$.

	Also from \cref{thm:cauchy_s_theorem_for_convex_set}, we have that $\int_{\gamma_{\delta, \epsilon}} g(z) \dif{z} = 0$ for any $\epsilon, \delta > 0$. As $\delta \to 0^+$, we have that the integrals over $C_{\delta_1}$ and $C_{\delta_2}$ cancel out. Hence, we are left with

	\begin{equation*}
		\int_{C} g(z) \dif{z} + \int_{C_\epsilon} g(z) \dif{z} = 0
	\end{equation*}

	Let's put our focus on the smaller circle, $C_\epsilon$. Now as $\epsilon \to 0^+$, $\frac{f(z) - f(w)}{z - w} \to 0$, and thus
	\begin{equation*}
		\int_{C_\epsilon} g(z) \dif{z} = \int_{C_\epsilon} \frac{f(z) - f(w)}{z - w} \dif{z} \to 0
	\end{equation*}
	Therefore,
	\begin{equation*}
		\int_{C} g(z) \dif{z} = 0
	\end{equation*}
	which implies, in the limit, that
	\begin{equation*}
		\int_{C} \frac{f(z)}{z - w} \dif{z}
			= \int_{C} \frac{f(w)}{z - w} \dif{z}
			= f(w) \int_{C} \frac{\dif{z}}{z - w}
	\end{equation*}
	
	We now require $\int_{C} \frac{\dif{z}}{z - w} = 2 \pi i$, but we shall prove for a more general case as a lemma.
\end{proof}

% section cauchy_s_integral_formula (end)

% chapter lecture_13_feb_9th_2018 (end)

\chapter{Lecture 14 Feb 12 2018}
	\label{chapter:lecture_14_feb_12_2018}

\section{Cauchy's Integral Formula (Continued)} % (fold)
\label{sec:cauchy_s_integral_formula_continued}

\begin{lemma}\label{lemma:integral_equals_2_pi_i}
	\hlnoteb{(Lemma and proof from Newman \& Bak on Complex Analysis, 3rd Ed.)}

	Suppose $a \in C_\rho^0$ such that $\exists \alpha \in C_\rho$ that is the center of the circle $C_\rho$, where $\rho$ is the radius of $C_\rho$, and hence $\abs{a - \alpha} < \rho$. Then
	\begin{equation*}
		\int_{C_\rho} \frac{\dif{z}}{z - a} = 2 \pi i
	\end{equation*}
\end{lemma}

\begin{proof}
	Let $z \equiv \alpha + \rho e^{i \theta}$, then $\dif{z} = i \rho e^{i \theta} d \theta$. Thus
	\begin{equation*}
		\int_{C_\rho} \frac{\dif{z}}{z - \alpha} = \int_{0}^{2 \pi} \frac{i \rho e^{i \theta}}{\rho e^{i \theta}} d \theta = 2 \pi i
	\end{equation*}
	while
	\begin{equation}\label{eq:lemma_integral_equals_2_pi_i_eq1}
		\int_{C_\rho} \frac{\dif{z}}{(z - \alpha)^{k + 1}} = 0 \quad \text{for } k = 1, 2, 3, ... \; .
	\end{equation}
	The \cref{eq:lemma_integral_equals_2_pi_i_eq1} follows not only from a direct evaluation of the integral
	\begin{equation*}
		\int_{C_\rho} \frac{\dif{z}}{(z - \alpha)^{k + 1}} = \int_{0}^{2 \pi} \frac{i \rho e^{i \theta}}{(\rho e^{i \theta})^{k + 1}} d \theta = \frac{i}{\rho^k} \int_{0}^{2 \pi} e^{-i k \theta} d \theta = 0
	\end{equation*}
	but also the fact that $\frac{1}{(z - \alpha)^{k + 1}}$ is the derivative of $-\frac{1}{k(z - \alpha)^k}$, which can be verified to be holomorphic on $C_\rho$, which simply makes \cref{eq:lemma_integral_equals_2_pi_i_eq1} true by \cref{thm:fundamental_theorem_of_calculus}.

	To evaluate $\int_{C_\rho} \frac{\dif{z}}{z - a}$, write
	\begin{align*}
		\frac{1}{z - a}
			&= \frac{1}{(z - \alpha) - (a - \alpha)} = \frac{1}{(z - \alpha)[1 - \frac{a - \alpha}{z - \alpha}]} \\
			&= \frac{1}{z - \alpha} \cdot \frac{1}{1 - \omega} 
	\end{align*}
	where
	\begin{equation}\label{eq:lemma_integral_equals_2_pi_i_eq2}
		\omega = \frac{a - \alpha}{z - \alpha} \text{ has fixed modulus } \frac{\abs{a - \alpha}}{\rho} < 1 \text{ throughout } C_\rho 
	\end{equation}
	By \cref{eq:lemma_integral_equals_2_pi_i_eq2} and by the \hlnotea{Infinite Geometric Sum} that $\frac{1}{1 - \omega} = 1 + \omega + \omega^2 + \hdots$, we get
	\begin{align*}
		\frac{1}{z - a}
			&= \frac{1}{z - \alpha} \left[ 1 + \frac{a - \alpha}{z - \alpha} + \frac{(a - \alpha)^2}{(z - \alpha)^2} + \hdots \right] \\
			&= \frac{1}{z - \alpha} + \frac{a - \alpha}{(z - \alpha)^2} + \frac{(a - \alpha)^2}{(z - \alpha)^3} + \hdots . 
	\end{align*}
	Since the convergence is uniform throughout $C_\rho$,
	\begin{equation*}
		\int_{C_\rho} \frac{1}{z - a} \dif{z}
			= \int_{C_\rho} \frac{1}{z - \alpha} \dif{z} + \sum_{k=1}^{\infty} \int_{C_\rho} \frac{(a - \alpha)^k}{(z - \alpha)^{k + 1}} \dif{z} = 2 \pi i
	\end{equation*}\qed
\end{proof}

We may now continue with completing the previous proof.

\begin{proof}[Continued - \cref{thm:cauchy_s_integral_formula_1}]
	\cref{lemma:integral_equals_2_pi_i} completes the part where we required $\int_{C} \frac{\dif{z}}{z - w} = 2 \pi i$.

	We now have
	\begin{equation*}
		f(w) = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w} \dif{z}
	\end{equation*}

	Now note that if we further generalize the number of times the contour $C_\rho$ made around $a$, where in this case $C_\rho$ is a closed path instead of a simple circle in $\Omega$, in \cref{lemma:integral_equals_2_pi_i}, we would get $\int_{C_\rho} \frac{\dif{z}}{z - a} = 2k \pi i$ where $k$ would represent that number.

	In this case, we would get
	\begin{equation*}
		f(w) k = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w} \dif{z}
	\end{equation*}
	where $k = \ind{w}{C} = \frac{1}{2 \pi i} \int_{C} \frac{\dif{z}}{z - w}$ which represents the number of times the contour $C$ winds around $w$. \qed
\end{proof}

\begin{remark}
	As noted, \cref{thm:cauchy_s_integral_formula_1} holds for any closed path $\gamma \in \Omega$ instead of a simple circle $C$. If $w \in \Omega \setminus \gamma^*$, we get
	\begin{equation*}
		\frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{z - w} \dif{z} = f(w) \ind{w}{\gamma}
	\end{equation*}
\end{remark}

\begin{propo}[Holomorphic Functions can be expressed as Power series]\label{propo:holomorphic_functions_can_be_expressed_as_power_series}
	Let $\Omega \subseteq \mathbb{C}$ be an open set, $f \in H(\Omega)$. Then $f$ can be expressed as a power series.
\end{propo}

\begin{proof}
	$\forall w \in \Omega, \exists C \subseteq \Omega$ that is a closed circle path with $w \in C^0$. By \cref{thm:cauchy_s_integral_formula_1}, and since $C$ is a circle, i.e. the contour winds around $w$ only once, we have
	\begin{equation*}
		f(w) = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w} \dif{z}.
	\end{equation*}
	Let $w_0 \in \Omega$ be the center of $C$. Then $\forall z \in \partial C, 0 < \abs{w - w_0} < \abs{z - w_0}$\footnote{This is the key step}. This implies that
	\begin{align*}
		&0 < \frac{\abs{w - w_0}}{\abs{z - w_0}} < 1 \\
		&\implies \sum_{n=0}^{\infty} \left(\frac{w - w_0}{z - w_0} \right)^n = \frac{1}{1 - \frac{w - w_0}{z - w_0}} = \frac{z - w_0}{z - w} \enspace \text{by the Infinite Geometric Sum} \\
		&\implies \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w} \dif{z} = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w_0} \frac{z - w_0}{z - w} \dif{z} = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{z - w_0} \sum_{n=0}^{\infty} \left( \frac{w - w_0}{z - w_0} \right)^n \dif{z}
	\end{align*}
	Note that each of the terms in the integrand of the last expression are absolutely convergent, thus by \hlnotea{Fubini's Theorem}, we can interchange the summation and integral sign to get
	\begin{equation*}
		f(w) = \sum_{n=0}^{\infty} \underbrace{\left[ \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{(z - w_0)^{n + 1}} \dif{z} \right]}_{a_n} (w - w_0)^n
	\end{equation*}
	which is a power series centered at $w_0$ with coefficient $a_n$.
\end{proof}

\begin{note}[Recall]
	Consider the power series $f(w) = \sum_{n=0}^{\infty} a_n (w - w_0)^n$. Recall \cref{item:tutorial_q2} from \cref{sec:practice_problems} that
	\begin{equation*}
		a_n = \frac{f^{(n)}(w_0)}{n!}
	\end{equation*}

	Applying this to \cref{propo:holomorphic_functions_can_be_expressed_as_power_series}, we get
	\begin{equation*}
		\frac{f^{(n)} (w_0)}{n!} = \frac{1}{2 \pi i} \int_{C} \frac{f(z)}{(z - w_0)^{n + 1}} \dif{z}
	\end{equation*}
	which holds for any $w_0 \in \Omega$ by having $C \subseteq \Omega$ centered at $w_0$.
\end{note}

\begin{thm}[Cauchy's Integral Formula 2]\label{thm:cauchy_s_integral_formula_2}
	Let $\Omega \subseteq \mathbb{C}$ be open, $f \in H(\Omega)$. Then
	\begin{enumerate}
		\item $\forall w \in \Omega$, $f$ has a power series expansion at $w$.
		\item $f$ is differentiable infinitely many times in $\Omega$.
		\item $\forall C \subseteq \Omega$ that is a closed circle oriented anticlockwise, we have that $\forall w \in C^0$, \label{item:true_cauchy_s_integral_formula}
			\begin{equation}\label{eq:cauchy_s_integral_formula_2}
			 	f^{(n)} (w) = \frac{n!}{2 \pi i} \int_{C} \frac{f(z)}{(z - w)^{n + 1}} \dif{z} 
			\end{equation} 
	\end{enumerate}
\end{thm}

\begin{remark}
	\cref{item:true_cauchy_s_integral_formula} is the actual Cauchy's Integral Formula in the theorem.
\end{remark}

\begin{proof}
	We have shown 1 from \cref{propo:holomorphic_functions_can_be_expressed_as_power_series} and 2 from \cref{thm:power_function_holomorphic_function_region_of_convergence}. It remains to prove 3, which we shall prove by induction.

	When $n = 0$, it is simply \cref{thm:cauchy_s_integral_formula_1}. Suppose $f$ has up to $n - 1$ complex derivatives and that
	\begin{equation*}
		f^{(n - 1)} (w) = \frac{(n - 1)!}{2 \pi i} \int_{C} \frac{f(z)}{(z - w)^n} \dif{z} .
	\end{equation*}
	Consider $h > 0$, the difference of the quotient for $f^{(n - 1)}$ is
	\begin{equation}\label{eq:cauchy_s_integral_formula_2_eq1}
		\frac{f^{(n-1)} (w - h) - f^{(n - 1)}(w)}{h} = \frac{(n-1)!}{2 \pi i} \int_{C} f(z) \frac{1}{h} \left[ \frac{1}{z - w - h} - \frac{1}{z - w} \right] \dif{z} 
	\end{equation}
	Note that
	\begin{equation*}
		A^n - B^n = (A - B)(A^{n-1} + A^{n - 2}B + \hdots + AB^{n - 2} + B^{n - 1})
	\end{equation*}
	Let $A = \frac{1}{z - w -h}, B = \frac{1}{z - w}$\footnote{Key step}, then the term in square brackets in \cref{eq:cauchy_s_integral_formula_2_eq1} becomes
	\begin{equation*}
		\frac{h}{(z - w - h)(z - w)} \left[A^{n - 1} + A^{n - 2}B + \hdots + AB^{n - 2} + B^{n - 1}\right]
	\end{equation*}
	Thus as $h \to 0$, we have
	\begin{equation*}
		f^{(n)} = \frac{(n - 1)!}{2 \pi i} \int_{C} f(z) \left[\frac{1}{(z - w)^2} \right] \left[ \frac{n}{(z - w)^{n - 1}} \right] \dif{z} = \frac{n!}{2 \pi i} \int_{C} \frac{f(z)}{(z - w)^{n + 1}} \dif{z}
	\end{equation*}
	which completes the induction proof and proves 3. \qed
\end{proof}

\begin{crly}[Taylor Expansion of Entire Functions]\label{crly:taylor_expansion_of_entire_functions}
	If $f$ is an entire function, then $\forall z_0 \in \mathbb{C}$, we have
	\begin{equation*}
		f(z) = f(z_0) + f'(z_0)(z - z_0) + \frac{f''(z_0)}{2!} (z - z_0)^2 + \hdots
	\end{equation*}
	which is a \hlnotea{Taylor Expansion} of $f$ around $z_0$.
\end{crly}

\begin{proof}
	By \cref{propo:holomorphic_functions_can_be_expressed_as_power_series}, we have that
	\begin{align}
		f(z) &= \sum_{n=0}^{\infty} \left[ \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^{n + 1}} \dif{w} \right] (z - z_0)^n \nonumber \\
			&= \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{w - z_0} \dif{w} + \left[ \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^2} \right](z - z_0) \label{eq:crly_taylor_expansion_of_entire_functions_eq1} \\
			&\quad\quad + \left[ \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^3} \dif{w} \right] (z - z_0)^2 + \hdots \nonumber \\
			&\quad\quad + \left[ \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^{k + 1}} \dif{w} \right] (z - z_0)^k + \hdots \nonumber
	\end{align}
	Now by \cref{thm:cauchy_s_integral_formula_2}, we have
	\begin{align*}
		f(z_0) &= f^{(0)}(z_0) = \frac{0!}{2 \pi i} \int_{C} \frac{f(w)}{w - z_0} \dif{w} = \frac{1}{2 \pi i} \int_{C} \frac{f(w)}{w - z_0} \dif{w} \\
		f^{(1)}(z_0) &= \frac{1!}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^2} \dif{w} \\
		f^{(2)}(z_0) &= \frac{2!}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^3} \dif{w} \\
		&\quad\vdots \\
		f^{(k)}(z_0) &= \frac{k!}{2 \pi i} \int_{C} \frac{f(w)}{(w - z_0)^{k + 1}} \dif{w} \\
		&\quad\vdots
	\end{align*}
	Thus \cref{eq:crly_taylor_expansion_of_entire_functions_eq1} becomes
	\begin{equation*}
		f(z) = f(z_0) + f^{(1)}(z_0)(z - z_0) + \frac{f^{(2)}(z_0)}{2!} (z - z_0)^2 + \hdots + \frac{f^{(k)}(z_0)}{k!} (z - z_0)^k + \hdots
	\end{equation*}
	as required. \qed
\end{proof}

% section cauchy_s_integral_formula_continued (end)

% chapter lecture_14_feb_12_2018 (end)

\chapter{Lecture 15 Feb 14th 2018} % (fold)
\label{cha:lecture_15_feb_14th_2018}

\section{Cauchy's Integral Formula (Continued 1)} % (fold)
\label{sec:cauchy_s_integral_formula_continued_1}

At this point, it is important that we provide the following definition:

\begin{defn}[Analytic Functions]\label{defn:analytic_functions}
  We say that $f$ is \hldefn{analytic} in $\Omega$ if $f$ has a power series expansion at every $z \in \Omega$.
\end{defn}

\begin{remark}
	\begin{enumerate}
		\item We have proven, in the previous lecture, that Holomorphicity $\implies$ Analyticity
		\item Should we have defined, in \cref{thm:cauchy_s_integral_formula_2}, that the closed circle orients clockwise, then we would have a negative equation for \cref{eq:cauchy_s_integral_formula_2}.
	\end{enumerate}
\end{remark}

\subsection{Applications of Cauchy's Integral Formula} % (fold)
\label{sub:applications_of_cauchy_s_integral_formula}

\begin{ex}\label{ex:applications_of_cauchy_s_integral_formula}
	\begin{enumerate}
		\item \hlnotea{(Cauchy's Inequality)}\footnote{In a sense, this inequality implies that as we take higher derivatives, the value of the derivatives become smaller.} Prove that $\forall z_0 \in \mathbb{C} \; \forall R > 0 \in \mathbb{R} \; \forall f \in H(C = D(z_0, R))$ \label{item:cauchy_s_inequality}
			\begin{equation*}
				f^{(n)}(z_0) \leq \frac{n!}{R^n} \cdot \sup_{z \in \mathbb{C}} \abs{f(z)}
			\end{equation*}

			\begin{proof}
				From \cref{eq:cauchy_s_integral_formula_2}, we have
				\begin{equation*}
					f^{(n)}(z_0) = \frac{n!}{2 \pi i} \int_{C} \frac{f(z)}{(z - z_0)^{n + 1}} \dif{z} 
				\end{equation*}
				Parameterize $C$ with $\gamma: [0, 2 \pi] \to \mathbb{C}$, where $t \mapsto z_0 + Re^{it}$. Then
				\begin{align*}
					f^{(n)} (z_0) &= \frac{n!}{2 \pi i} \int_{0}^{2 \pi} \frac{f(z_0 + Re^{it})}{(Re^{it})^{n + 1}} Rie^{it} \dif{t} \\
					\abs{f^{(n)}(z_0)} &\leq \frac{n!}{2 \pi} \int_{0}^{2 \pi} \frac{\abs{f(z_0 + Re^{it})}}{R^n} \dif{t} \quad \because \abs{Re^{it}} = R \\
						&\leq \frac{n!}{2 \pi R^n} \sup_{z \in C} \abs{f(z)} \int_{0}^{2\pi} \dif{t} \\
						&= \frac{n!}{R^n} \sup_{z \in C} \abs{f(z)}
				\end{align*}
				This completes the proof. \qed
			\end{proof}

		\item \hlnotea{(Liouville's Theorem)} A bounded entire function $f: \mathbb{C} \to \mathbb{C}$ is a constant\footnote{The theorem is not true in $\mathbb{R}$, since $\sin x$ is a bounded function differentiable everywhere, but is not a constant.} \footnote{The theorem also implies that ``trigonometry'' in $\mathbb{C}$ is unbounded, whatever the definition of ``trigonometry'' may be.}.\label{item:liouville_s_theorem}

			\begin{proof}
				Since $f$ is entire, we may take $R$, in \cref{item:cauchy_s_inequality}, to be any large value. Let $M$ be the bound of $f$, i.e. $\exists M \in \mathbb{C}, \, \forall z_0 \in \mathbb{C}, \, \abs{f^{(n)}(z_0)} \leq \frac{n!}{R^n} \sup_{z \in \mathbb{C}} \abs{f(z)} = \frac{n!}{R^n} \sup_{z \in \mathbb{C}} M$. Let $n = 1$, then $\abs{f'(z_0)} = \frac{M}{R}$. Thus we observe that $R \to \infty \implies f(z_0) \to 0$ for any $z_0 \in \mathbb{C}$. By A2Q5(a), $f$ is a constant.
			\end{proof}

		\item \hlnotea{(Parseval's Theorem)} Let $\Omega \subseteq \mathbb{C}$ be open, $f \in H(\Omega), \, \bar{D(z_0, R)} \subseteq \Omega$. Then $\forall z \in \bar{D(z_0, R)}, \, f(z) = \sum_{n=0}^{\infty} c_n (z - z_0)^n$, which in turn implies that \footnote{This is why the $L^2$-norm is perserved, as seen in AMATH231.}\label{item:parseval_s_theorem}
		\begin{equation*}\tag{$\dagger$}\label{eq:parseval_s_l2_norm}
			\forall z \in \bar{D(z_0, R)} \quad f(z_0 + re^{i \theta}) = \sum_{n=0}^{\infty} c_n (re^{i \theta})^n
		\end{equation*}

			Consider (the $L^2$norm)
			\begin{align*}
				& \frac{1}{2 \pi} \int_{0}^{2 \pi} \abs{f(z_0 + re^{i \theta})}^2 \dif{\theta} \\
				=&\frac{1}{2 \pi} \int_{0}^{2 \pi} \abs{\sum_{n=0}^{\infty} c_n (re^{i \theta})^n}^2 \dif{\theta} \\
				=& \frac{1}{2 \pi} \int_{0}^{2 \pi} \left[ \sum_{n=0}^{\infty} c_n r^n e^{i n \theta} \right] \left[\sum_{m=0}^{\infty} \bar{c_m} r^m e^{-i n \theta} \right] \dif{\theta} \\
				=& \frac{1}{2 \pi} \int_{0}^{2 \pi} \sum_{n=0}^{\infty} \sum_{m=0}^{\infty} c_n \bar{c_m} r^{n + m} e^{i (n - m) \theta} \dif{\theta} 
			\end{align*}
			Since the series are absolutely convergent, use may use Fubini's Theorem, and thus
			\begin{align*}
				=& \frac{1}{2 \pi} \sum_{n,m = 0}^{\infty} c_n \bar{c_m} r^{n + m} \int_{0}^{2 \pi} e^{i (n - m) \theta} \dif{\theta} \\
				=& \begin{cases}
					\frac{1}{2 \pi} \sum_{n,m = 0}^{\infty} c_n \bar{c_m} r^{n + m} 2 \pi & \text{if } n = m \\
					\frac{1}{2 \pi} \sum_{n,m = 0}^{\infty} c_n \bar{c_m} r^{n + m} \frac{e^{i (n - m) \theta}}{i (n - m)} \at{0}{2 \pi} = 0 & \text{if } n \neq m
				\end{cases} \\
				=& \sum_{n=0}^{\infty} \abs{c_n}^2 r^{2n} \quad \text{if } n = m
			\end{align*}
			Therefore, we have what is known as \hlnotea{Parseval's Identity}:
			\begin{equation}\label{eq:parseval_s_identity}
				\frac{1}{2 \pi} \int_{0}^{2 \pi} \abs{f(z_0 + re^{i \theta})}^2 \dif{\theta} = \sum_{n=0}^{\infty} \abs{c_n}^2 r^{2n}
			\end{equation}

			Parseval's Theorem states that:

			$L^2$-norm of LHS in \cref{eq:parseval_s_identity} $=$ $L^2$-norm of RHS of \cref{eq:parseval_s_l2_norm}
	\end{enumerate}

	Before going into the next application, please see \cref{lemma:principle_of_analytic_continuation}.

	\begin{enumerate}
		\setcounter{enumi}{3}
		\item \hlnotea{(Maximum Modulus Principle)} Let $\Omega \subseteq \mathbb{C}$ be open and connected, and $f \in H(\Omega)$. Then \label{item:maximum_modulus_principle}
		\begin{equation*}
			\sup_{z \in \Omega} \abs{f(z)} = \max_{z \in \partial \Omega} \abs{f(z)}.
		\end{equation*}
		This implies that $f$ cannot attain its maximum value in $\Omega^0$.

			\begin{proof}
				Suppose not, i.e. $\exists z_0 \in \Omega^0, \, \forall z \in \Omega$ such that $\abs{f(z_0)} = \max_{z \in \Omega} \abs{f(z)} \geq \abs{f(z)}$
				\begin{align*}
					&\implies \exists r > 0 \quad \bar{D(z_0, r)} \subseteq \Omega \\
					&\implies \forall z \in \bar{D(z_0, r)} \quad f(z) = \sum_{n=0}^{\infty} c_n (z - z_0)^n
				\end{align*}
				Note that $c_0 = \frac{f^{(0)}(z_0)}{0!} = f(z_0)$. By \cref{item:parseval_s_theorem},
				\begin{align*}
					\sum_{n=0}^{\infty} \abs{c_n}^2 r^{2n} &= \frac{1}{2\pi} \int_{0}^{2 \pi} \abs{f(z_0 + re^{i \theta})}^2 \dif{\theta} \\
					\implies  f(z_0)^2 + \sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n} & = \frac{1}{2 \pi} \abs{f(z_0 + re^{i \theta})}^2 \dif{\theta} \\
					& \leq \frac{1}{2 \pi} \abs{f(z_0)}^2 (2 \pi) \quad \because f(z_0) = \max_{z \in \Omega} f(z) \\
					\implies  f(z_0)^2 + \sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n} & \leq \abs{f(z_0)}^2 \\
					\implies  \sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n} & \leq 0 \\
					\implies c_1, c_2, ... & = 0 \\
					\implies & f \text{ is a constant in } \bar{D(z_0, r)} \\
					\implies & f \text{ is a constant in } \Omega \text{ by } \cref{lemma:principle_of_analytic_continuation}
				\end{align*}
				which is a contradiction. \qed
			\end{proof}
	\end{enumerate}

\end{ex}

\begin{lemma}[Principle of Analytic Continuation]\label{lemma:principle_of_analytic_continuation}
	Let $\Omega \subseteq \mathbb{C}$ be open and connected, and $f \in H(\Omega)$. Let $Z(f) = \{a in \Omega : f(a) = 0 \}$. Then either
	\begin{itemize}
		\item $Z(f) = \Omega$, i.e. $\forall z \in \Omega, \, f(z) = 0$; or
		\item $Z(f)$ has no limit point, i.e. points where $f = 0$ are isolated
	\end{itemize}
\end{lemma}

This is a powerful result, since if we can find a small region for where $f$ is $0$ in $\Omega$, then $f$ would be $0$ in the entirety of $\Omega$. If not, then $f$ is only $0$ at isolated points, i.e. points where $f = 0$ are all apart from each other.

% subsection applications_of_cauchy_s_integral_formula (end)

% section cauchy_s_integral_formula_continued_1 (end)

% chapter lecture_15_feb_14th_2018 (end)

\chapter{Lecture 16 Feb 16th 2018}
	\label{chapter:lecture_16_feb_16th_2018}

\section{Cauchy's Integral Formula (Continued 3)} % (fold)
\label{sec:cauchy_s_integral_formula_continued_3}

\subsection{Applications of Cauchy's Integral Formula (Continued)} % (fold)
\label{sub:applications_of_cauchy_s_integral_formula_continued}

\textbf{\cref{ex:applications_of_cauchy_s_integral_formula} (Continued)}

We shall restate the \cref{item:maximum_modulus_principle} in the following manner.

\begin{enumerate}
	\setcounter{enumi}{3}
	\item \hlnotea{Maximum Modulus Principle (MMP)} Let $\Omega \subseteq \mathbb{C}, \, f \in H(\Omega), \, D_{z_0} = \bar{D(z_0, r)} \subseteq \Omega$. Then $\abs{f(z_0)} \leq \max_{z \in \partial D_{z_0}} \abs{f(z)}$ with \label{item:maximum_modulus_principle_strong}
	\begin{equation*}
		\abs{f(z_0)} = \max_{z \in \partial D_{z_0}} \abs{f(z)} \iff f \text{ is a constant on } \Omega
	\end{equation*}

	\begin{remark}
		\begin{enumerate}
			\item This implies that for a non-constant analytic function $f$, $\forall z \in \Omega^0, \, f(z) \neq \max_{w \in \Omega} f(w)$.
			\item Since a global maximum is also a local maximum, we observe that for any smaller region $\Omega_0 \subseteq \Omega$, $f$ cannot attain its maximum value for any point in $\Omega_0^0$. This is a stronger statement than the our previous statement about the MMP.
		\end{enumerate}
	\end{remark}

	\begin{proof}
		Suppose for \Lightning that $f$ has a maximum in $\Omega^0$, say at $z_0$. Hence $\exists r > 0, D_{z_0} = \bar{D(z_0, r)}$ where
		\begin{equation*}
			\abs{f(z_0)} \geq \max_{z \in D_{z_0}} \abs{f(z)}
		\end{equation*}
		On $D_{z_0}$, we have
		\begin{equation}\label{eq:mmp_strong_eq1}
			f(z) = \sum_{n=0}^{\infty} c_n (z - z_0)^n
		\end{equation}
		Note that $c_0 = f(z_0)$. By \cref{item:parseval_s_theorem}, on $D_{z_0}$,
		\begin{align*}
			\sum_{n=0}^{\infty} \abs{c_n}^2 r^{2n}
				&= \frac{1}{2 \pi} \int_{0}^{2 \pi} \abs{f(z_0 + re^{i \theta})}^2 \dif{\theta} \quad \text{by } \cref{eq:parseval_s_identity} \\
				&\leq \frac{1}{2 \pi} \int_{0}^{2 \pi} \abs{f(z_0)}^2 \dif{\theta} \quad \text{by } \cref{eq:mmp_strong_eq1} \\
				&= \abs{f(z_0)}^2.
		\end{align*}
		Then we have
		\begin{align*}
			\abs{c_0}^2 + \sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n}
				&= \abs{f(z_0)}^2 \\
			\abs{f(z_0)}^2 + \sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n}
				&= \abs{f(z_0)}^2 \\
			\sum_{n=1}^{\infty} \abs{c_n}^2 r^{2n} &= 0
		\end{align*}
		which $\implies c_1 = c_2 = \hdots = 0$. Thus $\forall z \in D_{z_0}, \, f(z) \equiv c_0 \mod 2 \pi$. Then by \cref{lemma:principle_of_analytic_continuation}, since $f(z_0) - c_0$, as $f(z) - c_0)$ contains $\bar{D(z-0, r)}$, we see that $f(z) - c_0 \equiv 0$ in $\Omega$, which implies the equality of \cref{item:maximum_modulus_principle_strong}. \qed
 	\end{proof}

 	\item \hlnotea{Fundamental Theorem of Algebra (FTA)} Any polynomial $P(z) \in \mathbb{C}[z]$ of degree greater than $1$ has precisely $n$ roots in $\mathbb{C}$, given by $\alpha_1, \alpha_2, ..., \alpha_n$. $P(z)$ can be factored as $P(z) = A(z - \alpha_1)\hdots(z - \alpha_n)$ for some $A \in \mathbb{C}$. \label{item:fta}

 	\begin{proof}
 		We may write $P(z) = A(z^n + a_{n - 1} z^{n - 1} + \hdots a_1 z + a_0)$, which then
 		\begin{equation*}
 			\frac{P(z)}{z^n} = A \left(1 + \frac{a_{n + 1}}{z} + \hdots \frac{a_1}{z^{n-1}} + \frac{a_0}{z^n} \right)
 		\end{equation*}
 		which then, by the Reverse Triangle Inequality,
 		\begin{equation}\label{eq:fta_eq1}
 			\implies \abs{\frac{P(z)}{z^n} } \geq \abs{A} \left[1 - \frac{\abs{a_{n - 1}}}{\abs{z}} - \hdots - \frac{\abs{a_1}}{\abs{z^{n - 1}}} - \frac{\abs{a_0}}{\abs{z^n}} \right]
 		\end{equation}
 		So as $\abs{z} \to \infty, \, \abs{\frac{P(z)}{z^n} } \to \abs{A}$, from \cref{eq:fta_eq1}. Since $\abs{z} \to \infty, \, \exists R > 0, \, \forall \abs{z} > R$, then $\forall \theta \in [0, 2 \pi]$,
 		\begin{equation*}
 			\abs{P(Re^{i \theta})} = \abs{P(z)} \geq \frac{\abs{A}}{2} \abs{z}^n \geq \frac{\abs{A}}{2} R^n 
 		\end{equation*}
 		Taking $R$ to be even larger if necessary, we can get
 		\begin{equation}\tag{$\dagger$}\label{eq:fta_eq2}
 			\abs{{P(Re^{i \theta})}} \geq \abs{P(0)}
 		\end{equation}
 		Suppose, for contradiction, $P(z)$ has no root in $\mathbb{C}$. Then $g(z) = \frac{1}{P(z)}$ is an entire function. By \cref{eq:fta_eq2}, we have that $\abs{g(Re^{i \theta})} \leq \abs{g(0)}$ for all $\theta \in [0, 2 \pi]$. But this contradicts \cref{item:maximum_modulus_principle_strong} unless if $g(z)$ is constant on $\mathbb{C}$, which in turn implies that $P$ is a constant, but that contradicts that $P$ has degree greater than $1$.

 		$\therefore \, P(z)$ has to have a zero in $\mathbb{C}$, say $\alpha_1$. This implies that
 		\begin{equation*}
 			P(z) = A (z - \alpha_1) P_1 (z)
 		\end{equation*}
 		where $P_1(z) \in \mathbb{C} [z]$. By repeatedly taking the above steps, inductively so, for $P_1, P_2, ...$, the proof is completed. \qed
 	\end{proof}
\end{enumerate}

% subsection applications_of_cauchy_s_integral_formula_continued (end)

% section cauchy_s_integral_formula_continued_3 (end)

% chapter lecture_16_feb_16th_2018 (end)

\chapter{Lecture 17 Feb 26th 2018}
	\label{chapter:lecture_17_feb_26th_2018}

\section{Analytic Continuity} % (fold)
\label{sec:analytic_continuity}

We shall restate the important lemma that we have been using in the last two lectures, and proceed to prove this lemma.

\begin{lemma*}[Principle of Analytic Continuity]\label{lemma*:principle_of_analytic_continuity}
	Let $\Omega \subseteq \mathbb{C}$ be open and connected, and $f \in H(\Omega)$. Let $Z(f) = \{a in \Omega : f(a) = 0 \}$. Then either
	\begin{itemize}
		\item $Z(f) = \Omega$, i.e. $\forall z \in \Omega, \, f(z) = 0$; or
		\item $Z(f)$ has no limit point, i.e. points where $f = 0$ are isolated
	\end{itemize}
\end{lemma*}

\begin{proof}
	Let $z_0 \in Z(f)^*$.

	\textbf{Step 1:} Show that $z_0 \in Z(f)^0$, i.e. $f$ is identically $0$ on some $\bar{D(z_0, r)} \subseteq \Omega$ for $r > 0$. 

	On $\bar{D(z_0, r)}, \, f(z) = \sum_{n=0}^{\infty} c_n (z - z_0)^n$. Suppose $f$ is not identically $0$ on $\bar{D(z_0, r)}$. Then $\exists m \in \mathbb{N}, \, c_m \neq 0, \, \forall j < m , \, c_j = 0$, i.e. $f(z) = c_m(z - z_0)^m + c_{m + 1}(z - z_0)^{m + 1} + \hdots$.

	Define, in $\Omega$,
	\begin{equation*}
		g(z) = \begin{cases}
			\frac{f(z)}{(z - z_0)^m} 	& z \in \Omega \setminus \{z_0\} \\
			c_m 						& z = z_0
		\end{cases}
	\end{equation*}
	Clearly, $g \in H(\Omega \setminus \{z_0\})$. But on $\bar{D(z_0, r)}$,
	\begin{equation*}
		g(z) = c_m + c_{m + 1}(z - z_0) + c_{m + 2}(z - z_0)^2 + \hdots
	\end{equation*}
	which implies $g \in H(\Omega)$. Now $g(z_0) = c_m \neq 0$, so there exists a neighbourhood $U_{z_0}$ of $z_0$, such that $g \neq 0$ on $U_{z_0}$.

	$\forall a \neq z_0 \in Z(f)$, we have that $g(a) = 0$ by defintion of $Z(f)$, which implies that $a \notin U_{z_0}$, which contradicts that $z_0 \in Z(f)^*$. This implies $f \equiv 0$ in $\bar{D(z_0, r)}$.

	\textbf{Step 2:} $Z(f)^0$ is both open and closed.

	Note that
	\begin{equation*}
		Z(f)^0 := \left\{a \in Z(f) \, : \, \exists r > 0, \, \bar{D(a, r)} \subseteq Z(f) \right\}
	\end{equation*}
	is open by definition.

	\hlnotec{WTP} $[Z(f)^0]^* \subseteq [Z(f)]^*$.

	From \textbf{Step 1}, we know that $[Z(f)^0]^* \subseteq Z(f)^0$. Thus $Z(f)^0$ contains its limit points and is hence closed by definition.

	\textbf{Step 3: } $Z(f) = \emptyset$ or $\Omega$.

	\begin{align*}
		& \Omega \text{ is connected} \\
		&\implies \Omega = Z(f)^0 \, \sqcup \, \left( Z(f)^0 \right)^c \\
		&\implies \left( Z(f)^0 \right)^c \text{ is open and closed by } \textbf{Step 2} 
	\end{align*}
	A connected set cannot be expressed as a disjoint union of non-trivial open sets. Therefore, either $Z(f)^0 = \emptyset$ or $Z(f)^0 = \Omega$.
	\begin{align*}
		Z(f)^0 = \emptyset &\implies Z(f)^* = \emptyset \text{ by } \textbf{Step 1} \implies Z(f) = \emptyset \\
		Z(f)^0 = \Omega &\implies Z(f) = \Omega \text{ by } \textbf{Step 1}  
	\end{align*} \qed
\end{proof}

\begin{crly}[Uniqueness of a Function]\label{crly:uniqueness_of_a_function}
	Let $\Omega \subseteq \mathbb{C}$ be open and connected. $\forall f, g \in H(\Omega)$ with $f(z) = g(z)$ for $z \in \Omega_1 \subseteq \Omega$ where $\Omega_1$ has limit points. Then $\forall z \in \Omega$, $f(z) = g(z)$.
\end{crly}

\begin{proof}
	Apply \cref{lemma:principle_of_analytic_continuation} to the function $f - g$.
\end{proof}

\begin{remark}
	\begin{enumerate}
		\item In $\mathbb{C}$, we cannot have two functions sharing a region of points in their images. (But this is possible in $\mathbb{R}$)
		\item Suppose $f \in H(\Omega), \, \Omega \in \mathbb{C}$ is open and connected, $F \in H(\Omega')$ with $\Omega \subseteq \Omega'$. If $f, F$ agree on $\Omega$, then $F$ is called an analytic continuation of $f$ in $\Omega'$ (i.e. $F$ `extends' $f$ in $\Omega'$). \cref{lemma:principle_of_analytic_continuation} states that $F$ is uniquely determined by $f$, i.e. there is a unique way to analytically `continue' $f$.
	\end{enumerate}
\end{remark}

% section analytic_continuity (end)

\section{Morera's Theorem} % (fold)
\label{sec:morera_s_theorem}

\begin{remark}[Recall]
	From \hyperref[thm:cauchy_s_theorem_for_convex_set]{Cauchy's Theorem}, we know that $\forall f \in H(\Omega) \implies \forall \gamma \in \Omega \; \int f = 0$. We used \hyperref[thm:goursat_s_theorem]{Goursat's Theorem}, i.e. $\forall \Delta \in \Omega \; \int_{\Delta} f = 0$ to proof this, and in the process we constructed an antiderivative. Now, our question is, is the converse of the said Cauchy's Theorem true?
\end{remark}
Unfortunately for us, that is not true (\hlwarn{example needed}). But a ``partial'' converse exists.

\begin{thm}[Morera's Theorem]\label{thm:morera_s_theorem}
	Let $f$ be continuous on $\Omega \subseteq \mathbb{C}$, which is an open set, and $\forall \Delta \in \Omega, \, \int_{\Delta} f = 0$, where $\Delta$ is a triangular path. Then $f \in H(\Omega)$.
\end{thm}

\begin{proof}
	Use the same construction as in \hyperref[thm:cauchy_s_theorem_for_convex_set]{Cauchy's Theorem for Convex Sets} to get an antiderivative $F$ for $f$, where $F \in H(\Omega)$, i.e.
	\begin{equation*}
		F(z) := \int_{[a, z]} f(z) \dif{z}
	\end{equation*}
	Then $F'(z) = f(z)$, which in turn implies that $f \in H(\Omega)$ since $F$ is $\mathbb{C}$-differentiable on $\Omega$ by \cref{thm:cauchy_s_integral_formula_2}.
\end{proof}

% section morera_s_theorem (end)

% chapter lecture_17_feb_26th_2018 (end)

\chapter{Lecture 18 Feb 28th 2018}
	\label{chapter:lecture_18_feb_28th_2018}

\section{Winding Numbers} % (fold)
\label{sec:winding_numbers}

Recall \hyperref[thm:cauchy_s_integral_formula_1]{Cauchy's Integral Formula}. We claimed that
\begin{equation*}
	\ind{w}{C} = \begin{cases}
		1	& w \in C^0 \\
		0	& w \notin C
	\end{cases}
\end{equation*}

We will now formally define this index.

\begin{defn}[Winding Numbers]\label{defn:winding_numbers}
	Let $\gamma : [a, b] \to \mathbb{C}$ be a closed and oriented anti-clockwise, and $\gamma^*$ be the image of $\gamma$ in $\mathbb{C}$. Let $\Omega = \mathbb{C} \setminus \gamma^*$. $\forall w \in \Omega$, define the index of $w$ with respect to $\gamma$ as
	\begin{equation*}
		\frac{1}{2 \pi i} \int_{\gamma} \frac{\dif{z}}{z - w} 
	\end{equation*}
	in which shall be called the winding number of $\gamma$ around $w$.
\end{defn}

\begin{thm}[Winding Number Theorem]\label{thm:winding_number_theorem}
	We shall use notation as the definition above. $\ind{w}{\gamma}$ is
	\begin{enumerate}
		\item always an integer;
		\item constant on any connected component of $\Omega$; and
		\item zero on the unbounded component of $\Omega$.
	\end{enumerate}
\end{thm}

\begin{note}
	$\gamma$ is compact in $\mathbb{C}$ (since it creates a ring from $[a, b]$ under $\gamma$). So for some disc $D$, $\gamma^* \subseteq D$. Let $\Omega \supset \mathbb{C} \setminus D$, where we note that the contained set is connected and unbounded. Then $\Omega$ contains one unbounded component, while other components of $\Omega$ are inside $D$. Therefore, we know that components in $D$ are bounded.
\end{note}

\begin{proof}
	\begin{enumerate}
		\item By definition,
		\begin{align*}
			\ind{w}{\gamma}
				&= \frac{1}{2 \pi i } \int_{\gamma} \frac{\dif{z}}{z - w} \\
				&= \frac{1}{2 \pi i} \int_{a}^{b} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w} 
		\end{align*}
		\hlnotec{WTS} $\ind{w}{\gamma} \in \mathbb{Z} \equiv \int_{a}^{b} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w} \in 2 \pi i \mathbb{Z}$.

		Note that $z \in 2 \pi i \mathbb{Z} \iff e^z = 1$. Thus it suffices to show that
		\begin{equation*}
			e^{\int_{a}^{b} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w}} = 1
		\end{equation*}

		\textbf{Idea: } Think of $\exp\left(\int_{a}^{u} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w}\right)$ as a function of $u$, call it $\phi(u)$. Then we just need to show that $\phi(b) = 1$. We know that $\phi(a) = \exp\left(\int_{a}^{a} \hdots \right) = 1$. This motivates us to find the derivative of $\phi$.

		Define $\phi$ accordingly, and then since $(e^{f(u)})' = e^{f(u)} \cdot f'(u)$,
		\begin{align*}
			\phi'(u) &= \phi(u) \cdot \frac{d}{du} \int_{a}^{u} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w} \\
			\text{by FTC} &\implies \frac{\phi'(u)}{\phi(u)} = \frac{\gamma'(u)}{\gamma(u) - w} \\
			&\implies \phi'(u) \left( \gamma(u) - w \right) - \gamma'(u) \phi(u) = 0 \\
			&\implies \frac{d}{du} \left( \frac{\phi(u)}{\gamma(u) - w} \right) = 0 \quad \text{by quotient rule} \\
			&\implies \frac{\phi(b)}{\gamma(b) - w} = \frac{\phi(a)}{\gamma(a) - w} \enspace \text{since } \frac{\phi(u)}{\gamma(u) - w} \text{ is a constant function of } u \\
			&\implies \phi(b) = \phi(a) = 1 \enspace \because \gamma \text{ is closed.}
		\end{align*}

		We will prove that $\ind{w}{\gamma}$ is continuous.
		\begin{gather*}
			\forall w \in \Omega \; \forall z \in \gamma^* \enspace \exists M > 0 \; \abs{w - z} > M \\
			\forall \epsilon > 0 \; \exists \delta = \frac{M^2 \pi \epsilon}{ \int_{\gamma} \dif{z} } > 0 \; \forall w_0 \in \Omega \\
			\abs{w - w_0} < \delta \, \land \, \abs{w_0 - z} > \frac{M}{2}
		\end{gather*}
		then
		\begin{align*}
			\abs{\ind{w}{\gamma} - \ind{w_0}{\gamma}} &= \abs{\frac{1}{2 \pi i} \int_{\gamma} \frac{\dif{z}}{z - w}	- \frac{1}{2 \pi i} \int_{\gamma} \frac{\dif{z}}{z - w_0} } \\
				&= \frac{1}{2 \pi} \abs{\int_{\gamma} \frac{w - w_0}{(z - w)(z - w_0)} \dif{z} } \\
				&\leq \frac{1}{2 \pi} \int_{\gamma} \abs{\frac{w - w_0}{(z - w)(z - w_0)} } \dif{z} \\
				&< \frac{1}{2 \pi} \delta \int_{\gamma} \abs{\frac{2}{M \cdot M} } \dif{z} \\
				&= \frac{1}{M^2 \pi} \delta \int_{\gamma} \dif{z} = \epsilon
		\end{align*}

		\item Also $\ind{w}{\gamma}$ takes only integer values, thus it must be constant on each open connected component\footnote{We may invoke \cref{lemma:principle_of_analytic_continuation} but it is, to an extent, unnecessary for such a powerful statement.} (\hlwarn{why?}).

		\item Note that
		\begin{equation*}
			\abs{\ind{w}{\gamma}} = \frac{1}{2 \pi} \abs{\int_{a}^{b} \frac{\gamma'(t) \dif{t}}{\gamma(t) - w}}
		\end{equation*}
		Let $w$ be in the unbounded component in the complement of $\gamma$ such that $\abs{w} \to \infty$. Then $\forall t \in [a, b], \, \exists M > 0$ such that
		\begin{equation*}
			\frac{1}{\abs{\gamma(t) - w}} \leq \frac{1}{M}
		\end{equation*}
		which implies that
		\begin{align*}
			&\abs{\ind{w}{\gamma}} \leq \frac{1}{2 \pi} \frac{1}{M} \cdot \underbrace{\int_{a}^{b} \abs{\gamma'(t)} \dif{t}}_{\parbox{3cm}{is a fixed constant as $\gamma$ is a fixed path}} \\
			&\implies (\abs{w} \to \infty \implies M \to \infty \implies \abs{\ind{w}{\gamma}} \to 0)
		\end{align*}
		Then by parts 1 and 2, the proof is completed. \qed
	\end{enumerate}
\end{proof}

\begin{remark}
	Note that by 2, we have that $\forall w \in C^0$,
	\begin{equation*}
		\frac{1}{2 \pi i} \int_{C} \frac{\dif{z}}{z - w} = \frac{1}{2 \pi i} \int_{C} \frac{\dif{z}}{z - z_0} = \frac{1}{2 \pi i } \int_{0}^{2 \pi} \frac{Rie^{i \theta}}{Re^{i \theta}} \dif{\theta} = 1  
	\end{equation*}
	where $z_0$ is the center of the circle path $C$.
\end{remark}

% section winding_numbers (end)

% chapter lecture_18_feb_28th_2018 (end)

\chapter{Lecture 19 Mar 2nd 2018}
	\label{chapter:lecture_19_mar_2nd_2018}

\section{Singularities} % (fold)
\label{sec:singularities}

\begin{ex}
	Let $C : [0, 2 \pi] \to \mathbb{C}$ such that $\forall t \in [0, 2 \pi], \, t \to e^{it}$. Suppose $f \in H(\Omega)$, then by Cauchy
	\begin{equation*}
		\int_{C} f(z) \dif{z} = 0
	\end{equation*}

	Let $f(z) = \frac{1}{z}$, then $\int_{C} \frac{1}{z} \dif{z} = 2 \pi i \ind{0}{C} = 2 \pi i$ when it is ``supposed'' to be $0$ by the argument above. Then in this case, $f \notin H(\Omega)$. In fact, $f$ is undefined at $0$.
\end{ex}

The example above introduces us to the study of such exceptional points.

\begin{defn}[(Isolated) Singularity]\label{defn:singularity}
	$\forall a \in \mathbb{C}, \, \exists r > 0 , \, \exists D = D(a, r)$.
	\begin{gather*}
		f \in H(D \setminus \{a\}) \; \land \; f(a) \text{ is undefined} \iff
	\end{gather*}
	$f$ has a(n) \hldefn{point/isolated singularity} at $z = a$.
\end{defn}

\begin{eg}
	\begin{enumerate}
		\item Given $f \in H(\mathbb{C} \setminus \{0\})$, define $f(z) = \frac{e^z - 1}{z}$. Clearly, $z$ is a singularity. Consider the function $(e^z - 1) \in H(\mathbb{C})$. Then we have that the function has a power series expansion around $z = 0$. So $\forall z \in \mathbb{C}$,
		\begin{equation*}
			e^z - 1 = z + \frac{z^2}{2!} + \frac{z^3}{3!} + \hdots
		\end{equation*}
		And for $z \neq 0$, we have
		\begin{equation}\label{eq:eg_singularity_eq1}
			\frac{e^z - 1}{z} = 1 + \frac{z}{2!} + \frac{z^2}{3!} + \hdots
		\end{equation}
		This motivates us to define
		\begin{equation*}
			g(z) = \begin{cases}
				\frac{e^z - 1}{z} 	& z \in \mathbb{C} \setminus \{0\} \\
				1					& z = 0
			\end{cases}
		\end{equation*}
		Clearly then $g \in H(\mathbb{C})$, where in $\mathbb{C} \setminus \{0\}$ its holomorphicity is given by $f$, and in a neighbourhood of $0$, from \cref{eq:eg_singularity_eq1}. Therefore, y assigning $f$ the value of $1$ at $z = 0$, we can make $f$ ``entire''.

		We call such a point $z$ as a \hlnotea{removable singularity} for $f$.

		\item Given $f \in H(\mathbb{C} \setminus \{0\})$, define $f(z) = \frac{1}{z}$. Is the singularity at $0$ removable?

		Suppose $\exists g \in H(\mathbb{C})$ such that
		\begin{equation}\label{eq:eg_singularity_eq2}
			\forall z \in \mathbb{C} \setminus \{0\} \quad g(z) = f(z)
		\end{equation}
		$\therefore \exists r > 0 \; \forall z \in D(0, r)$
		\begin{equation}\label{eq:eg_singularity_eq3}
			g(z) = c_0 + c_1 z + c_2 z^2 + \hdots
		\end{equation}
		Consider the function $zg(z)$. By \cref{eq:eg_singularity_eq2},
		\begin{equation*}
			\forall z \in \mathbb{C} \setminus \{0\} \quad zg(z) = 1
		\end{equation*}
		By \cref{eq:eg_singularity_eq3}, $z = 0 \implies zg(z) = 0$. But this cannot happen since $zg(z) \in H(\mathbb{C})$ (\hlnoteb{if we pick an open ball of, say, $\frac{1}{2}$ around $0$, then there are no points in the entirety of $\mathbb{C}$ that is close to $0$}). Therefore $z = 0$ is not a removable singularity for $f$.
	\end{enumerate}
\end{eg}

\begin{defn}[Removable Singularity, Pole, Essential Singularity]\label{defn:removable_singularity_pole_essential_singularity}
	Let $f$ have a singularity at $z_0 \in \mathbb{C}$.
	\begin{enumerate}
		\item $\exists r > 0 \enspace \forall z \in D = D(z_0, r) \enspace \exists g(z) \in H(D) \enspace \forall z \in D \setminus \{z_0\} \enspace g(z) = f(z) \\ \implies f$ has a \hldefn{removable singularity} at $z_0$\footnote{For the laymen, "the value of $f$ at $z_0$ can be corrected or defined to make it holomorphic in its designated region."}.

		\item $\exists r > 0 \enspace \forall z \in D = D*(z_0, r) \enspace \exists A, B \in H(D) \enspace A(z_0) \neq 0 \, \land \, B(z_0) = 0 \enspace f(z) = \frac{A(z)}{B(z)} \\ \implies f$ has a \hldefn{pole} at $z_0$ (a non-removable singularity)\footnote{For the laymen, "the singularity of $f$ comes from a zero of its denominator."}

		\item $f$ has a singularity at $z_0$ which is neither removeable nor a pole $\implies$ $f$ has an \hldefn{essential singularity} at $z_0$.
	\end{enumerate}
\end{defn}

\begin{eg}
	To show an example of an essential singularity, consider the function $f(z) = e^{\frac{1}{z}}$. If we attempt to do a ``Taylor expansion'' on the function (which is invalid at $z = 0$), we have
	\begin{equation*}
		f(z) = 1 + \frac{1}{z} + \frac{1}{2! z^2} + \frac{1}{3! z^3} + \hdots
	\end{equation*}
	The point $0$ for $f$ is said to be a ``pole of infinite order'' (this shall be defined later on)
\end{eg}

While \hldefn{removable singularities} are nice to have, they are not as interesting to us. On the other hand, we are more interested in their non-removable counterpart, the \hldefn{poles}. This motivates the study of zeros of holomorphic functions.

\begin{thm}[Theorem 9]\label{thm:theorem_9}
	Let $\Omega \subseteq \mathbb{C}$ be open and connected. Suppose that $f \in H(\Omega)$ with $f \not\equiv 0$ on $\Omega$ and that $f$ has a zero at $z_0 \in \Omega$. Then
	\begin{gather}
		\exists r > 0 \enspace \forall z \in D = D(z_0, r) \enspace \exists g \in H(D) \enspace g(z_0) \neq 0 \enspace \exists! n \in \mathbb{N} \nonumber \\
		f(z) = (z - z_0)^n \cdot g(z) \label{eq:theorem_9_eq}
	\end{gather}
\end{thm}

\begin{proof}
	By \hyperref[lemma:principle_of_analytic_continuation]{Analytic Continuation}, zeros of $f$ are isolated since $f \not\equiv 0$. So $\exists r > 0$ such that $\exists D = D(z_0, r)$, in which $\forall z \in D \setminus \{z_0\}$, $f(z) \neq 0$.

	Since $f \in H(\Omega)$, $\forall z \in D$,
	\begin{equation*}
		f(z) = \sum_{k=0}^{\infty} c_k (z - z_0)^k
	\end{equation*}
	As $f \not\equiv 0$ in $D$, $\exists n \in \mathbb{N \setminus \{0\}}$ that is the smallest such that $c_n \neq 0$\footnote{$n \neq 0$ since we have $f(z_0) = 0$ which implies $c_0 = 0$.}.

	\begin{align*}
		\therefore f(z)
			&= c_n(z - z_0)^n + c_{n + 1} (z - z_0)^{n + 1} + \hdots \\
			&= (z - z_0)^n \underbrace{\left[ c_n + c_{n + 1} (z - z_0) + \hdots \right]}_{\text{call this } g(z)}
	\end{align*}
	Note that $g(z_0) \neq 0$ since $c_n \neq 0$. Thus $g(z) \in H(D)$ since it has the same radius of convergence as $f$.

	To prove uniqueness, suppose that we may write
	\begin{equation*}
		f(z) = \sum_{k=0}^{\infty} (z - z_0)^n \cdot g(z) = (z - z_0)^m \cdot h(z)
	\end{equation*}
	for some $m \in \mathbb{N}$ and that $h(z) \neq 0$. If $m > n$, dividing both sides by $(z - z_0)^n$,
	\begin{equation*}
		g(z) = (z - z_0)^{m - n} h(z).
	\end{equation*}
	As $z \to z_0$, we would have $g(z_0) = 0$, which is a contradiction. If $m < n$, we can perform a similar argument and have that as $z \to z_0$, $h(z_0) = 0$, also a contradiction. Therefore, $m = n$ and $h = g$. \qed
\end{proof}

We say that $f$ has a \hlnotea{zero of order $n$} at $z_0$ if \cref{eq:theorem_9_eq} holds.

% section singularities (end)

% chapter lecture_19_mar_2nd_2018 (end)

\chapter{Lecture 20 Mar 5th 2018}
	\label{chapter:lecture_20_mar_5th_2018}

\section{Singularity (Continued)} % (fold)
\label{sec:singularity_continued}

Recall the definition of a \hldefn{removable singularity} from \cref{defn:removable_singularity_pole_essential_singularity}.

\begin{thm}[Theorem 10]\label{thm:theorem_10}
	If $f \in H(\Omega \setminus \{z_0\})$ has an isolated singularity at $z_0$ and $\lim_{z \to z_0} (z - z_0) f(z) = 0$, then the singularity at $z_0$ is removable.
\end{thm}

\begin{proof}
	Since $f(z_0)$ is undefined, set
	\begin{equation*}
		h(z) = \begin{cases}
			(z - z_0)^2 f(z)	& \forall z \in \Omega \setminus \{z_0\} \\
			0					& z = z_0
		\end{cases}
	\end{equation*}
	Clearly $h \in H(\Omega \setminus \{z_0\})$. At $z_0$,
	\begin{align*}
		\lim_{z \to z_0} \frac{h(z) - h(z_0)}{z - z_0}
			&= \lim_{z \to z_0} \frac{(z - z_0)^2 f(z)}{z - z_0} \enspace \footnotemark \\
			&= 0 \text{ by assumption}
	\end{align*}

	\footnotetext{Goes to show that the definition of $h$ is no foresight.}

	$\therefore h'(z_0)$ exists and equals $0$. Clearly then that $h \in H(\Omega)$. So $\exists r > 0$ such that $\exists D = D(z_0, r)$, so that $\forall z \in D$,
	\begin{equation*}
		h(z) = c_0 + c_1 (z - z_0) + c_2 (z - z_0)^2 + \hdots
	\end{equation*}
	But $c_0 = h(z_0) = 0$ and $c_1 = h'(z_0) = 0$. Thus the power series can be written as
	\begin{align*}
		h(z) &= c_2 (z - z_0)^2 + c_3 (z - z_0)^3 + \hdots \\
			&= (z - z_0)^2 \left[ c_2 + c_3 (z - z_0) + \hdots \right]
	\end{align*}
	Hence by the definition of $h$, $\forall z \in \Omega \setminus \{z_0\}, \enspace f(z) = c_2 + c_3 (z - z_0) + \hdots$. Therefore, by redefining $f(z_0) = c_2$, we see that the singularity at $z_0$ is removable.

	We may also complete the proof by defining a function $g$ as, $\forall z \in \Omega$,
	\begin{equation*}
		g(z) = \begin{cases}
			f(z)		& z \neq z_0 \\
			c_2 		& z = z_0
		\end{cases}
	\end{equation*}\qed
\end{proof}

\textbf{Recall \cref{thm:theorem_9}}

Let $\Omega \subseteq \mathbb{C}$ be open and connected, and $f \in H(\Omega)$ where $\forall z \in \Omega, \, f(z) \neq 0$.

$f(z_0) = 0 \implies$
\begin{gather*}
	\exists r > 0 \enspace \exists D = D(z_0, r) \enspace \forall z \in D \enspace \exists! n \in \mathbb{N} \\
	\exists! g \in H(D) \enspace g(z_0) \neq 0 \\
	f(z) = (z - z_0)^n g(z)
\end{gather*}

\begin{defn}[Zero of Order $n$ \& Simple Zero]\label{defn:zero_of_order_n_&_simple_zero}
	By the above setting, we say that $f$ has a \hldefn{zero of order $n$} at $z_0$.\footnote{In laymen terms, "Rate at which the function vanishes at $z_0$. The greater $n$ is, the greater the rate."}

	If $n = 1$, we say that $z_0$ is a \hldefn{simple zero}.
\end{defn}

\textbf{Recall definition of} \hldefn{a pole} \textbf{from \cref{defn:removable_singularity_pole_essential_singularity}}

Suppose $f$ has an isolated singularity at $z_0$, and that there exists a neighbourhood $D$ around $z_0$ where $A, B \in H(D)$, in which $A$ and $B$ are defined such that $\forall z \neq z_0 \in D$, $A(z_0) \neq 0 \, \land \, B(z_0) = 0$, so that we can let $f(z) = \frac{A(z)}{B(z)}$. Then $f$ has a pole at $z_0$.

\begin{thm}[Theorem 9.1]\label{thm:theorem_9.1}
	If $f$ has a pole at $z_0 \in \Omega$, then in a neighbourhood of that point there exists a non-vanishing holomorphic function $h$ and a unique positive integer $n$ such that
	\begin{equation*}
		f(z) = (z - z_0)^{-n} h(z)
	\end{equation*}
\end{thm}

\textit{Stein \& Shakarchi - Complex Analysis (pg. 74)}

\begin{proof}
	By \cref{thm:theorem_9}, we have $\frac{1}{f(z)} = (z - z_0)^n g(z)$, where $g$ is holomorphic and non-vanishing in a neighbourhood of $z_0$, so the result follows with $h(z) = \frac{1}{g(z)}$. \qed
\end{proof}

\begin{defn}[Pole of order $n$ \& Simple Pole]\label{defn:pole_of_order_n_&_simple_pole}
	With the above setting, we say that $f$ has a \hldefn{pole of order $n$} at $z_0$ if the function $B$ has a zero of order $n$\footnote{In laymen terms, "Rate at which $f$ `grows' near $z_0$."}

	If $n = 1$, then $z_0$ is a simple pole.
\end{defn}

\begin{thm}[Theorem 11]\label{thm:theorem_11}
	Let $f$ have a pole of order $n$ at $z_0$. Then $\exists r > 0, \, \exists D = D(z_0, r)$, such that $\forall z \in D \setminus \{z_0\}$,
	\begin{equation*}
		f(z) = \frac{c_{-n}}{(z - z_0)^n} + \frac{c_{-(n - 1)}}{(z - z_0)^{n - 1}} + \hdots + \frac{c_{-1}}{z - z_0} + G(z)
	\end{equation*}
	for some $G \in H(D)$.
\end{thm}

\begin{proof}
	By \cref{thm:theorem_9.1}, write the holomorphic function $h$ as $h(z) = a_0 + a_1 (z - z_0) + a_2 (z - z_0)^2 + \hdots$, then
	\begin{equation*}
		f(z) = \frac{1}{(z - z_0)^n} \left[ a_0 + a_1 (z - z_0) + a_2 (z - z_0)^2 + \hdots \right].
	\end{equation*}
	The proof is complete by expanding the equation. \qed
\end{proof}

\begin{defn}[Principal Part]\label{defn:principal_part}
	In \cref{thm:theorem_11}, the sum $\sum_{j=1}^{n} = \frac{c_{-j}}{(z - z_0)^j}$ is called the \hldefn{principal part} of $f$ at the pole $z_0$.
\end{defn}

\begin{defn}[Residue]\label{defn:residue}
	In \cref{thm:theorem_11}, the coefficient $c_{-1}$ is called the \hldefn{residue} of $f$ at the pole $z_0$, denoted $\res{f}{z_0}$.
\end{defn}

The \hldefn{residue} shall be more carefully studied later on.

% section singularity_continued (end)

% chapter lecture_20_mar_5th_2018 (end)

\chapter{Lecture 21 Mar 7th 2018}
	\label{chapter:lecture_21_mar_7th_2018}

\section{Singularity (Continued 2)} % (fold)
\label{sec:singularity_continued_2}

\begin{thm}[Casorati-Weierstrass]\label{thm:casorati_weierstrass}
	Let $z_0 \in \Omega$ and $f \in H(\Omega \setminus \{z_0\})$. Suppose $f$ has a singularity at $z_0$. Then one of the following occurs:
	\begin{enumerate}
		\item $f$ is a removable singularity at $z_0$;
		\item $\exists m \in \mathbb{N}, \, \{c_j\}_{j = 1}^m \subseteq \mathbb{C}, \, f(z) - \sum_{j=1}^{m} c_j (z - z_0)^-j$ has a removable singularity at $z_0$; or
		\item $\forall r > 0, \, B(z_0, r) \subseteq$ such that $f(B^0(z_0, r))$ is dense in $\mathbb{C}$ (Note: $B^0(z_0, r)$ is the punctured ball)
	\end{enumerate}
\end{thm}

\begin{proof}
	Suppose 3. does not hold, i.e. $f(B^0(z_0, r))$ is not dense in $\mathbb{C}$ for some $r > 0$. Then $\exists w \in \mathbb{C}, \, \exists \delta > 0$, such that
	\begin{gather*}
		f\big( B^0(z_0, r) \big) \, \cap \, B(w, \delta) = \emptyset \\
		\implies \forall z \in B^0(z_0, r) \quad \abs{f(z) - w} > \delta
	\end{gather*}
	Consider $g(z) = \frac{1}{f(z) - w}$ for $z \in B^0(z_0, r)$, in which $g \in H(B^0(z_0, r))$. Then $\abs{g(z)} \leq \frac{1}{\delta}$ for all $z \in B^0(z_0, r)$, which implies that
	\begin{equation*}
		\lim_{z \to z_0} (z - z_0) g(z) = 0
	\end{equation*}
	By \cref{thm:theorem_10}, $g$ has a removable singularity at $z_0$, thus we can extend the function to a function $\tilde{g} \in H(B(z_0, r))$. From here, we try to construct a function that extends on $f$ onto the singularity $z_0$, say, $\tilde{f}$. The construction of $\tilde{g}$ satisfies the equation $\frac{1}{\tilde{g}(z)} + w = f(z)$ except, possibly, at $z_0$.

	\textbf{Case 1:} Suppose $\tilde{g}(z_0) \neq 0$.

	We can simply define
	\begin{equation*}
		\frac{1}{\tilde{g}(z)} = \begin{cases}
			f(z) - w 	& z \in B^0(z_0, r) \\
			\frac{1}{\tilde{g}(z_0)} & z = z_0
		\end{cases}
	\end{equation*}
	Clearly then $\frac{1}{\tilde{g}} \in H(B^0(z_0, r))$. At $z_0$,
	\begin{equation*}
		\tilde{g}(z_0) \neq 0 \implies \exists r_1 > 0 \; D = D(z_0, r_1) \; \forall z \in D \; \tilde{g}(z) \neq 0 \implies \frac{1}{\tilde{g}} \in H(D)
	\end{equation*}
	Therefore, $\frac{1}{\tilde{g}} \in H(B(z_0, r))$\footnote{$\frac{1}{\tilde{g}}$ is the inverse of a non-zero holomorphic function.}

	Since $\forall z \in B^0(z_0, r) \; f(z) = \frac{1}{\tilde{g}(z)} + w$ by construction, we may define
	\begin{equation*}
		\tilde{f}(z) = \frac{1}{\tilde{g}(z)} + w
	\end{equation*}
	such that $f(z_0)$ is defined as $\frac{1}{\tilde{g}(z_0)} + w$. By this construction, 1. holds.

	\textbf{Case 2:} $\tilde{g}(z_0) = 0$.

	$\because \tilde{g} \in H(B^0(z_0, r)) \, \land \, \tilde{g}(z_0) = 0 \, \land \, (\forall z \in B^0(z_0, r) \; \tilde{g}(z_0) \neq 0)$

	By \cref{thm:theorem_9}, 
	\begin{gather}
		\exists ! m \in \mathbb{N} \enspace \exists 0 < r_1 < r \enspace \exists g_1 \in H\big(B(z_0, r_1)\big) \nonumber \\
		\forall z \in B(z_0, r_1) \enspace \tilde{g}(z) = (z - z_0)^m g_1(z) \label{eq:casorati-weierstrass_pf:eq1}
	\end{gather}

	$\because g_1 \in H\big( B(z_0, r_1) \big) \, \land \, g(z_0) \neq 0$, we can repeat the argument as in \textbf{Case 1} for $g_1$ to get that $\frac{1}{g_1} \in H\big( B(z_0, r_1) \big)$, which implies
	\begin{equation*}
		\forall z \in B(z_0, r_1) \enspace \frac{1}{g_1(z)} = a_0 + a_1 (z - z_0) + \hdots
	\end{equation*}
	By construction given by \cref{eq:casorati-weierstrass_pf:eq1}, $\forall z \in B^0(z_0, r_1)$
	\begin{align*}
		f(z) - w
			&= \frac{1}{g_1(z) (z - z_0)^m} \\
			&= \frac{a_0}{(z - z_0)^m} + \frac{a_1}{(z - z_0)^{m - 1}} + \hdots + \frac{a_{m - 1}}{z - z_0} + a_m + a_{m + 1} (z - z_0) + \hdots 
	\end{align*}
	$\therefore \forall z \in B^0(z_0, r_1)$,
	\begin{equation*}
		f(z) - \frac{a_0}{(z - z_0)^m} - \hdots - \frac{a_{m - 1}}{z - z_0} = w + a_m + a_{m + 1}(z - z_0) \hdots 
	\end{equation*}
	Thus we may define an ``extended'' function of $f$, $\tilde{f}$ to be $w + a_m$ at the singularity $z_0$. Therefore, $f(z) - \sum_{j=1}^{m} a_j (z - z_0)^{-j}$ has a removable singularity at $z_0$, i.e. 2. holds. \qed
\end{proof}

% section singularity_continued_2 (end)

% chapter lecture_21_mar_7th_2018 (end)

\chapter{Lecture 22 Mar 9th 2018}
\label{chp:lecture_22_mar_9th_2018}

\section{Singularity (Continued 3)} % (fold)

\begin{crly}\label{crly:casorati_weierstrass_crly}
If $f$ has an essential singularity at $z_0$ and is holomorphic in some $B^0(z_0, r)$ where $r > 0$, then $f(B^0(z_0, r))$ is dense in $\mathbb{C}$.
\end{crly}

\begin{proof}
  Suppose not, i.e. 3. of \cref{thm:casorati_weierstrass} does not hold. Then either 1., which implies that $z_0$ is removable, or 2., which implies that $z_0$ is a pole, is true. This contradicts the assumption that $z_0$ is an essential singularity. \qed
\end{proof}

\begin{remark}
  There are a lot more that are actually true from \cref{thm:casorati_weierstrass}! \hlnotea{Picard} showed that in any such punctured ball $B^0(z_0, r)$ around the essential singularity $z_0$, $f$ takes on every complex value (except possibly one value) infinitely often.
\end{remark}

% section singularity_continued_3 (end)

\section{The Residue Theorem} % (fold)

\begin{note}[Recall]
  If $f$ has a pole at $z_0$, $f \in H(\Omega \setminus \{z_0\})$, then in some open neighbourhood $D$ of $z_0$, we can write $\forall z \in D \setminus \{z_0\}$
  \begin{equation}\label{eq:function_at_a_pole}
    f(z) = \underbrace{ \frac{c_{-k}}{(z - z_0)^k} + \hdots + \frac{c_{-1}}{(z - z_0)} }_{\text{Principal Part}} + \underbrace{ c_0 + c_1 (z - z_0) + \hdots }_{G(z)}
  \end{equation}
  with $G \in H(D)$.
\end{note}

\begin{thm}[Cauchy's Residue Theorem]\label{thm:cauchy_s_residue_theorem}
  Let $\Omega \subseteq \mathbb{C}$ be open, $f \in H(\Omega \setminus \{z_0\})$ where $z_0 \in \Omega$ is a pole. If $\gamma$ is a closed path in $\Omega \setminus \{z_0\}$ such that $\forall w \not\in \Omega, \, \ind{w}{\gamma} = 0$. Then
  \begin{equation*}
    \frac{1}{2 \pi i} \int_{\gamma} f(z) \dif(z) = \left( \res{f}{z_0} \right) \ind{z_0}{\gamma}
  \end{equation*}
  where $\ind{z_0}{\gamma} := \frac{1}{2 \pi i} \int_{\gamma} \frac{1}{z - z_0} \dif{z}$.
\end{thm}

\begin{proof}
  Using notation of \cref{eq:casorati-weierstrass_pf:eq1}, define $g(z)$ such that
  \begin{equation*}
    g(z) := \begin{cases}
      f(z) - \sum_{j = 1}^{k} \frac{c_{-j}}{(z - z_0)^j}  & z \in \Omega \setminus \{z_0\} \\
      c_0                                                 & z = z_0
    \end{cases}
  \end{equation*}
  Clearly, $g \in H(\Omega \setminus \{z_0\})$, since $f(z)$ minus finitely many polynomials with non-zero denominators is still a holomorphic function. At $z_0$, with a neighbourhood D around the point, we have, from \cref{eq:casorati-weierstrass_pf:eq1},
  \begin{equation*}
    g(z) = c_0 + c_1 (z - z_0) + \hdots
  \end{equation*}
  which $g(z_0)$ agrees with $c_0$ and for any point $z \in D \setminus \{z_0\}$, by definition of $g$ using \cref{eq:casorati-weierstrass_pf:eq1}. This implies that $g \in H(D) \implies g \in H(\Omega)$.

  Thus, by \cref{thm:cauchy_s_theorem_for_convex_set},
  \begin{equation*}
    \int_{\gamma} g(z) \dif{z} = 0
  \end{equation*}
  Then $\forall z \in \gamma$ and since $z_0 \not\in \gamma$, we get
  \begin{equation*}
    \int_{\gamma} f(z) \dif{z} = \int_{\gamma} \sum_{j = 1}^k \frac{c_{-j}}{(z - z_0)^j} \dif{z}
  \end{equation*}

  Consider each term of RHS in turn. Note that for $m \geq 2$, since $\frac{-1}{(m - 1)(z - z_0)^{m - 1}}$ is the antiderivative of $\frac{1}{(z - z_0)^m}$,
  \begin{align*}
    \int_{\gamma} \frac{1}{(z - z_0)^m} \dif{z}
      &= F\big( \gamma(b) \big) - F\big( \gamma(a) \big) \quad \text{by } \cref{thm:fundamental_theorem_of_calculus} \\
      &= 0 \quad \text{since } \gamma \text{ is closed.}
  \end{align*}
  If $m = 1$, then
  \begin{equation*}
    \frac{1}{2 \pi i} \int_{\gamma} \frac{1}{z - z_0} \dif{z} = \ind{z_0}{\gamma} \text{ by definition}
  \end{equation*}
  \begin{align*}
    \therefore \frac{1}{2 \pi i} \int_{\gamma} f(z) \dif{z}
      &= c_{-1} \ind{z_0}{\gamma} \\
      &= \left( \res{f}{z_0} \right) \ind{z_0}{\gamma}
  \end{align*}\qed
\end{proof}

\begin{defn}[Meromorphic Functions]\label{defn:meromorphic_functions}
  A function $f$ is said to be meromorphic on $\Omega$ if $\exists \mathscr{A} \subseteq \Omega$ such that
  \begin{enumerate}
    \item $A^* = \emptyset$
    \item $f \in H(\Omega \setminus \mathscr{A})$
    \item $\forall z \in \mathscr{A} \quad f$ has a pole of finite order on $z$.
  \end{enumerate}
\end{defn}

\begin{remark}
  Holomorphicity $\subseteq$ Meromorphicity (let $\matscr{A} = \emptyset$)
\end{remark}

% section the_residue_theorem (end)

% chapter lecture_22_mar_9th_2018

\end{document}
