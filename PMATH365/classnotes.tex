% !TEX TS-program = pdflatex
\documentclass[notoc,notitlepage]{tufte-book}
% \nonstopmode % uncomment to enable nonstopmode

\input{latex-classnotes-preamble.tex}

\usepackage{classnotetitle}
\usepackage{tikz-3dplot}

\title{PMATH365 --- Differential Geometry}
\author{Johnson Ng}
\subtitle{Classnotes for Winter 2019}
\credentials{BMath (Hons), Pure Mathematics major, Actuarial Science Minor}
\institution{University of Waterloo}

\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\D}{D}

\begin{document}
\input{latex-classnotes-header.tex}

\chapter*{Preface}%
\addcontentsline{toc}{chapter}{Preface}
\label{chp:preface}
% chapter preface

\nocite{karigiannis2019}

This course is a post-requisite of MATH 235/245 
(\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-MATH.html\#MATH235}{Linear Algebra II})
and AMATH 231 
(\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-AMATH.html#AMATH231}{Calculus IV})
or MATH 247 
(\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-MATH.html#MATH247}{Advanced Calculus III}).
In other words, familiarity with vector spaces and calculus is expected.

The course is spiritually separated into two parts. The first part shall be called 
\textbf{Exterior Differential Calculus}, which allows for a natural, metric-independent
generalization of \hlnotea{Stokes' Theorem}, \hlnotea{Gauss's Theorem}, and 
\hlnotea{Green's Theorem}. Our end goal of this part is to arrive at Stokes' Theorem,
that renders the \hlnotea{Fundamental Theorem of Calculus} as a special case of the theorem.

The second part of the course shall be called in the name of the course: 
\textbf{Differential Geometry}. This part is dedicated to studying geometry using techniques from
differential calculus, integral calculus, linear algebra, and multilinear algebra.

% chapter preface (end)

\tuftepart{Exterior Differential Calculus}

\chapter{Lecture 1 Jan 07th}%
\label{chp:lecture_1_jan_07th}
% chapter lecture_1_jan_07th

\section{Linear Algebra Review}%
\label{sec:linear_algebra_review}
% section linear_algebra_review

\begin{defn}[Linear Map]\index{Linear Map}\label{defn:linear_map}
  Let $V, W$ be finite dimensional real vector spaces. A map $T : V \to W$ is called \hlnoteb{linear} if $\forall a, b \in \mathbb{R}$, $\forall v \in V$ and $\forall w \in W$,
  \begin{equation*}
    T(av + bw) = aT(v) + bT(w).
  \end{equation*}
  We define L(U, W) to be the set of all linear maps from $V$ to $W$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item Note that $L(U, W)$ is itself a finite dimensional real vector space.
    \item The structure of the vector space $L(V, W)$ is such that $\forall T, S \in L(V, W)$, and $\forall a, b \in \mathbb{R}$, we have
      \begin{equation*}
        aT + bS : V \to W
      \end{equation*}
      and
      \begin{equation*}
        (aT + bS)(v) = aT(v) + bS(v).
      \end{equation*}
    \item A special case: when $W = V$, we usually write
      \begin{equation*}
        L(V, W) = L(V),
      \end{equation*}
      and we call this the \hldefn{space of linear operators on $V$}.
  \end{itemize}
\end{note}

Now suppose $\dim(V) = n$ for some $n \in \mathbb{N}$. This means that there exists a basis $\{ e_1 , \ldots, e_n \}$ of $V$ with $n$ elements.

\begin{defn}[Basis]\index{Basis}\label{defn:basis}
  A basis $\mathcal{B} = \{ e_1, \ldots, e_n \}$ of an $n$-dimensional vector space $V$ is a subset of $V$ where
  \begin{enumerate}
    \item $\mathcal{B}$ \hlnotea{spans} $V$, i.e. $\forall v \in V$
      \begin{equation*}
        v = \sum_{i=1}^{n} v^i e_i. \quad \footnotemark
      \end{equation*}
      \footnotetext{We shall use a different convention when we write a linear combination. In particular, we use $v^i$ to represent the $i$\textsuperscript{th} coefficient of the linear combination instead of $v_i$. Note that this should not be confused with taking powers, and should be clear from the context of the discussion.}
    \item $e_1, \ldots, e_n$ are linearly independent, i.e.
      \begin{equation*}
        v^i e_i = 0 \implies v^i = 0 \text{ for every } i.
      \end{equation*}
  \end{enumerate}
\end{defn}

\begin{note}
  We shall abusively write
  \begin{equation*}
    v^i e_i = \sum_{i} v^i e_i.
  \end{equation*}
  Again, this should be clear from the context of the discussion.
\end{note}

The two conditions that define a basis implies that any $v \in V$ can be expressed as $v^i e_i$, where $v^i \in \mathbb{R}$.

\begin{defn}[Coordinate Vector]\index{Coordinate Vector}\label{defn:coordinate_vector}
  The $n$-tuple $(v^1, \ldots, v^n) \in \mathbb{R}^n$ is called the \hlnoteb{coordinate vector} $[v]_{\mathcal{B}} \in \mathbb{R}^n$ of $v$ with respect to the basis $\mathcal{B} = \{ e_1, \ldots, e_n \}$.
\end{defn}

\begin{note}
  It is clear that the coordinate vector $[v]_{\mathcal{B}}$ is dependent on the basis $\mathcal{B}$. Note that we shall also assume that the basis is ``ordered'', which is somewhat important since the same basis (set-wise) with a different ``ordering'' may give us a completely different coordinate vector.
\end{note}

\begin{eg}
  Let $V = \mathbb{R}^n$, and $\hat{e}_i = ( 0, \ldots, 0, 1, 0, \ldots, 0 )$, where $1$ is the $i$\textsuperscript{th} compoenent of $\hat{e}_1$. Then
  \begin{equation*}
    \mathcal{B}_{\std} = \left\{ \hat{e}_1, \ldots, \hat{e}_n \right\}
  \end{equation*}
  is called the \hldefn{standard basis} of $\mathbb{R}^n$.
\end{eg}

\begin{note}
  Let $v = (v^1, \ldots, v^n) \in \mathbb{R}^n$. Then
  \begin{equation*}
    v = v^1 \hat{e}_1 + \hdots v^n \hat{e}_n.
  \end{equation*}
  So $\mathbb{R}^n \ni [v]_{\mathcal{B}_{\std}} = v \in V = \mathbb{R}^n$.

  This is a privilege enjoyed by the $n$-dimensional vector space $\mathbb{R}^n$.
\end{note}

Now if we choose a \hldefn{non-standard basis} of $\mathbb{R}^n$, say $\tilde{\mathcal{B}}$, then $[v]_{\tilde{\mathcal{B}}} \neq v$.

\begin{note}
  It does not make sense to ask if a standard basis exists for an arbitrary space, as we have seen above. A geometrical way of wrestling with this notion is as follows:

  \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \draw[->] (0, 0, 0) -- (2, 0, 0) node[right] {$x$};
      \draw[->] (0, 0, 0) -- (0, 2, 0) node[above] {$z$};
      \draw[->] (0, 0, 0) -- (0, 0, 2) node[below,left] {$y$};

      \draw[-,fill=be-blue,fill opacity=0.5] (1.5, 0, 0.5) -- (0.5, 0, 1.5) -- (-1.5, 1, -0.5) -- (-0.5, 1, -1.5) -- (1.5, 0, 0.5);
      \node[above,right] at (1.5, 1.5, 0) {a $2$-dimensional subspace in $\mathbb{R}^3$};
    \end{tikzpicture}
    \caption{An arbitrary $2$-dimensional subspace in a $3$-dimensional space}
    \label{fig:an_arbitrary_2-dimensional_subspace_in_a_3-dimensional_space}
  \end{figure}

  While the subspace is embedding in a vector space of which has a standard basis, we cannot establish a ``standard'' basis for this $2$-dimensional subspace. In laymen terms, we cannot tell which direction is up or down, positive or negative for the subspace, without making assumptions.
\end{note}

However, since we are still in a finite-dimensional vector space, we can still make a connection to a Euclidean space of the same dimension.

\begin{defn}[Linear Isomorphism]\index{Linear Isomorphism}\label{defn:linear_isomorphism}
  Let $V$ be $n$-dimensional, and $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ be some basis of $V$. The map
  \begin{equation*}
    v = v^i e_i \mapsto [v]_{\mathcal{B}}
  \end{equation*}
  from $V$ to $\mathbb{R}^n$ is a \hlnoteb{linear isomorphism} of vector spaces.
\end{defn}

\begin{ex}
  Prove that the said linear isomorphism is indeed linear and bijective\sidenote{i.e. we are right in calling it linear and being an isomorphism}.
\end{ex}

\begin{note}
  Any $n$-dimensional real vecotr space is isomorphic to $\mathbb{R}^n$, but not \hlnotea{canonically} so, as it requires the knowledge of the basis that is arbitrarily chosen. In other words, a different set of basis would give us a different isomorphism.
\end{note}

% section linear_algebra_review (end)

\section{Orientation}%
\label{sec:orientation}
% section orientation

Consider an $n$-dimensional vector space $V$. Recall that for any linear operator $T \in L(V)$, we may associate a real number $\det(T)$, called the \hldefn{determinant} of $T$, such that $T$ is said to be \hldefn{invertible} iff $\det(T) \neq 0$.

\begin{defn}[Same and Opposite Orientations]\index{Same orientation}\index{Opposite orientation}\label{defn:same_and_opposite_orientations}
  Let
  \begin{equation*}
    \mathcal{B} = \left\{ e_1, \ldots, e_n \right\} \quad \text{ and } \quad \tilde{\mathcal{B}} = \left\{ \tilde{e}_1, \ldots, \tilde{e}_n \right\}
  \end{equation*}
  be two ordered bases of $V$. Let $T \in L(V)$ be the linear operator defined by
  \begin{equation*}
    T(e_i) = \tilde{e}_i
  \end{equation*}
  for each $i = 1, 2, \ldots, n$. This mapping is clearly invertible, and so $\det(T) \neq 0$, and $T^{-1}$ is also linear, such that $T^{-1} \left( \tilde{e}_i \right) = e_i$, for each $i$.

  We say that $\mathcal{B}$ and $\tilde{\mathcal{B}}$ determine the \hlnoteb{same orientation} if $\det(T) > 0$, and we say that they determine the \hlnoteb{opposite orientations} if $\det(T) < 0$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item This notion of orientation only works in real vector spaces,
      as, for instance, in a complex vector space,
      there is no sense of ``positivity'' or ``negativity''.
    \item Whenever we talk about same and opposite orientation(s),
      we are usually talking about 2 sets of bases.
      It makes sense to make a comparison to the standard basis in a Euclidean space,
      and determine that the compared (non-)standard basis is
      ``positive'' (same direction) or ``negative'' (opposite),
      but, again, in an arbitrary space, we do not have this convenience.
  \end{itemize}
\end{note}

\begin{ex}[A1Q1]
  Show that any $n$-dimensional real vector space $V$ admits exactly 2 orientations.
\end{ex}

\begin{eg}
  On $\mathbb{R}^n$, consider the standard basis
  \begin{equation*}
    \mathcal{B}_{\std} = \{ \hat{e}_1, \ldots, \hat{e}_n \}.
  \end{equation*}
  The orientation determined by $\mathcal{B}_{\std}$ is called the \hldefn{standard orientation} of $\mathbb{R}^n$.
\end{eg}

% section orientation (end)

\section{Dual Space}%
\label{sec:dual_space}
% section dual_space

\begin{defn}[Dual Space]\index{Dual Space}\label{defn:dual_space}
  Let $V$ be an $n$-dimensional vector space.
  Then $\mathbb{R}$ is a $1$-dimensional real vector space.
  Thus we have that $L(V, \mathbb{R})$ is also a real vector space
  \sidenote{Note that $L(V, \mathbb{R})$ is also finite dimensional since both the domain and codomain are finite dimensional.}.
  The \hlnoteb{dual space} $V^*$ of $V$ is defined to be
  \begin{equation*}
    V^* := L(V, \mathbb{R}).
  \end{equation*}
\end{defn}

Let $\mathcal{B}$ be a basis of $V$. For all $i = 1, 2, \ldots, n$, let $e^i \in V^*$ such that
\begin{equation*}
  e^i(e_j) = \delta^i_j = \begin{cases}
    1 & i = j \\
    0 & i \neq j
  \end{cases}.
\end{equation*}
This $\delta^i_j$ is known as the \hldefn{Kronecker Delta}.

In general, we have that for every $v = v^j e_j \in V$, where $v^i \in \mathbb{R}$,
by the linearity of $e^i$, we have
\begin{equation*}
  e^i(v) = e^i(v^j e_j) = v^j e^i(e_j) = v_j \delta^i_j = v^i.
\end{equation*}
So each of the $e^i$, when applied on $v$,
gives us the $i$\textsuperscript{th} component of $[v]_{\mathcal{B}}$,
where $\mathcal{B}$ is a basis of $V$, in particular
\begin{equation}\label{eq:v_wrt_basis}
  v = v^i e_i, \text{ where } v^i = e^i(v).
\end{equation}

% section dual_space (end)

% chapter lecture_1_jan_07th (end)

\chapter{Lecture 2 Jan 09th}%
\label{chp:lecture_2_jan_09th}
% chapter lecture_2_jan_09th

\section{Dual Space (Continued)}%
\label{sec:dual_space_continued}
% section dual_space_continued

\begin{propo}[Dual Basis]\index{Dual Basis}\label{propo:dual_basis}
  The set
  \begin{equation*}
    \mathcal{B}^* := \left\{ e^1, \ldots, e^n \right\}
  \end{equation*}
  \sidenote{Note that the $e^i$'s are defined as in the last part of the last lecture.}
  is a basis of $V^*$, and is called the \hlnoteb{dual basis} of $\mathcal{B}$,
  where $\mathcal{B}$ is a basis of $V$.
  In particular, $\dim V^* = n = \dim V$.
\end{propo}

\begin{proof}
  \hlbnoted{$\mathcal{B}^*$ spans $V^*$}
  Let $\alpha \in V^*$. Let $v = v^j e_j \in V$,
  where we note that
  \begin{equation*}
    \mathcal{B} = \left\{ e_i \right\}_{i = 1}^{n}.
  \end{equation*}
  We have that
  \begin{equation*}
    \alpha(v) = \alpha(v^j e_j) = v^j \alpha( e_j ).
  \end{equation*}
  Now for all $j = 1, 2, \ldots, n$, define $\alpha_j = \alpha(e_j)$.
  Then
  \begin{equation*}
    \alpha(v) = \alpha_j v^j = \alpha_j e^j(v),
  \end{equation*}
  which holds for all $v \in V$. This implies that $\alpha = \alpha_j e^j$,
  and so $\mathcal{B}^*$ spans $V^*$.

  \noindent
  \hlbnoted{$\mathcal{B}^*$ is linearly independent}
  Suppose $\alpha_j e^j = 0 \in V^*$.
  Applying $\alpha_j e^j$ to each of the vectors $e_k$ in $\mathcal{B}$, we have
  \begin{equation*}
    \alpha_j e^j(e_k) = 0(e_k) = 0 \in \mathbb{R}
  \end{equation*}
  and
  \begin{equation*}
    \alpha_j e^j(e_k) = \alpha_j \delta_k^j = \alpha_k.
  \end{equation*}
  By A1Q2, we have that $a_k = 0$ for all $k = 1, 2, \ldots, n$,
  and so $\mathcal{B}^*$ is linearly independent.
\end{proof}

\begin{remark}
  Let $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ be a basis of $V$,
  with dual space $\mathcal{B}^* = \left\{ e^1, \ldots, e^n \right\}$.
  Then the map $T : V \to V^*$ such that
  \begin{equation*}
    T(e_i) = e^i
  \end{equation*}
  is a vector space \hyperref[defn:linear_isomorphism]{isomorphism}.
  And so we have that $V \simeq V^*$, but not \hlnotea{cannonically} so
  since we needed to know what the basis is in the first place.
\end{remark}

We will see later that if we impose an \hlnotea{inner product} on $V$,
then it will induce a canonical isomorphism from $V$ to $V^*$.

\begin{defn}[Natural Pairing]\index{Natural Pairing}\label{defn:natural_pairing}
  The function
  \begin{equation*}
    \langle \cdot, \cdot \rangle : V^* \times V \to \mathbb{R}
  \end{equation*}
  given by
  \begin{equation*}
    \langle \alpha, v \rangle \mapsto \alpha (v)
  \end{equation*}
  is called a \hlnoteb{natural pairing} of $V^*$ and $V$.
\end{defn}

\begin{note}
  A natural pairing is bilinear,
  i.e. it is linear in $\alpha$ and linear in $v$,
  which means that
  \begin{equation*}
    \langle \alpha, t_1 v_1 + t_2 v_2 \rangle
    = t_1 \langle \alpha, v_1 \rangle + t_2 \langle \alpha, v_2 \rangle
  \end{equation*}
  and
  \begin{equation*}
    \langle t_1 \alpha_1 + t_2 \alpha_2, v \rangle
    = t_1 \langle \alpha_1, v \rangle + t_2 \langle \alpha_2, v \rangle,
  \end{equation*}
  respectively.
\end{note}

\begin{propo}[Natural Pairings are Nondegenerate]\label{propo:natural_pairings_are_nondegenerate}
  For a finite dimensional real vector space $V$,
  a natural pairing is said to be nondegenerate if
  \marginnote{This is A1Q2.}
  \begin{equation*}
    \forall v \in V \enspace \langle \alpha, v \rangle = 0 \iff \alpha = 0
  \end{equation*}
  and
  \begin{equation*}
    \forall \alpha \in V^* \enspace \langle \alpha, v \rangle = 0 \iff v = 0.
  \end{equation*}
\end{propo}

\begin{eg}
  Fix a basis $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ of $V$.
  Given $T \in L(V)$,
  there is an associated $n \times n$ matrix $A = [T]_{\mathcal{B}}$ defined by
  \begin{equation*}
    T(e_i) = A_{\tikzmark{a}i}^{\tikzmark{b}j} e_j.
  \end{equation*}
  \begin{tikzpicture}[remember picture,overlay]
    \draw[latex'-]
      ([shift={(1pt,-2pt)}]pic cs:a) |- ([shift={(-10pt,-10pt)}]pic cs:a)
      node[anchor=east] {row index};
    \draw[latex'-]
      ([shift={(2pt,6pt)}]pic cs:b) |- ([shift={(12pt,12pt)}]pic cs:b)
      node[anchor=west] {column index};
  \end{tikzpicture}
  In particular,
  \begin{equation*}
    A = \overbrace{
      \begin{bmatrix}
        [ T(e_1) ]_{\mathcal{B}} & \hdots & [ T(e_n) ]_{\mathcal{B}}
      \end{bmatrix}
    }^{\text{block matrix}}
  \end{equation*}
  and
  \begin{equation*}
    A_i^k = e^k( T(e_i) ) = \langle e^k, T(e_i) \rangle.
  \end{equation*}
\end{eg}

\begin{defn}[Double Dual Space]\index{Double Dual Space}\label{defn:double_dual_space}
  The set
  \begin{equation*}
    V^{**} = L(V^*, \mathbb{R})
  \end{equation*}
  is called the \hlnoteb{double dual space}.
\end{defn}

\begin{propo}[The Space and Its Double Dual Space]\label{propo:the_space_and_its_double_dual_space}
  Let $V$ be a finite dimensional real vector space and $V^{**}$ be its double dual space. There exists a linear map $\xi$ such that
  \begin{equation*}
    \xi : V \to V^{**}
  \end{equation*}
\end{propo}

\begin{proof}
\marginnote{As messy as this may seem, this is really a follow your nose kind of proof.

  Since we are proving that a map exists, we need to construct it.
  Since $\xi : V \to V^{**} = L(V^*, \mathbb{R})$, for any $v \in V$,
  we must have $\xi(v)$ as some linear map from $V^*$ to $\mathbb{R}$.}

  Let $v \in V$. Then $\xi(v) \in V^{**} = L(V^*, \mathbb{R})$,
  i.e. $\xi(v) : V^* \to \mathbb{R}$. Then for any $\alpha \in V^*$,
  \begin{equation*}
    \left( \xi(v) \right)(\alpha) \in \mathbb{R}.
  \end{equation*}
  Since $\alpha \in V^*$, i.e. $\alpha : V \to \mathbb{R}$,
  and $\alpha$ is linear, let us define
  \begin{equation*}
    \xi(v)(\alpha) = \alpha(v).
  \end{equation*}
  To verify that $\xi(v)$ is indeed linear, notice that
  for any $t, s \in \mathbb{R}$, and for any $\alpha, \beta \in V^*$,
  we have
  \begin{align*}
    \xi(v)(t \alpha + s \beta)
      &= (t \alpha + s \beta) (v) \\
      &= t \alpha(v) + s \beta(v) \\
      &= t \xi(v)(\alpha) + s \xi(v)(\beta).
  \end{align*}
  It remains to show that $\xi$ itself is linear:
  for any $t, s \in \mathbb{R}$, any $v, w \in V$,
  and any $\alpha \in V^*$, we have
  \begin{align*}
    \xi(tv + sw)(\alpha)
      &= \alpha (tv + sw) = t \alpha(v) + s \alpha(w) \\
      &= t \xi(v)(\alpha) + s \xi(v)(\alpha) \\
      &= [ t \xi(v) + s \xi(w) ](\alpha)
  \end{align*}
  by addition of functions.
\end{proof}

\begin{propo}[Isomorphism Between The Space and Its Dual Space]\label{propo:isomorphism_between_the_space_and_its_dual_space}
  The linear map in \cref{propo:the_space_and_its_double_dual_space} is an isomorphism.
\end{propo}

\begin{proof}
  From \cref{propo:the_space_and_its_double_dual_space}, $\xi$ is linear.
  Let $v \in V$ such that $\xi(v) = 0$, i.e. $v \in \ker(\xi)$.
  Then by the same definition of $\xi$ as above, we have
  \begin{equation*}
    0 = (\xi(v))(\alpha) = \alpha(v)
  \end{equation*}
  for any $\alpha \in V^*$.
  By \cref{propo:natural_pairings_are_nondegenerate}, we must have that $v = 0$,
  i.e. $\ker(\xi) = \{ 0 \}$.
  Thus by \cref{propo:nullity_of_only_0_and_injectivity}, $\xi$ is injective.

  Now, since
  \begin{equation*}
    V^{**} = L(V^*, \mathbb{R}) = L ( L ( V, \mathbb{R} ), \mathbb{R} ),
  \end{equation*}
  we have that
  \begin{equation*}
    \dim(V^{**}) = \dim(V^*) = \dim(V).
  \end{equation*}
  Thus, by the Rank-Nullity Theorem
  \sidenote{See \cref{sec:rank_nullity_theorem}, and especially \cref{propo:when_rank_equals_the_dimension_of_the_space}.},
  we have that $\xi$ is surjective.

  
\end{proof}

The above two proposition shows to use that we may
identify $V$ with $V^{**}$ using $\xi$, and we can
gleefully assume that $V = V^{**}$.

Consequently, if $v \in V = V^{**}$ and $\alpha \in V^*$,
we have
\begin{equation}\label{eq:commutativity_of_the_vector_and_its_linear_operator}
  \alpha(v) = v(\alpha) = \langle \alpha, v \rangle.
\end{equation}

% section dual_space_continued (end)

\section{Dual Map}%
\label{sec:dual_map}
% section dual_map

\begin{defn}[Dual Map]\index{Dual Map}\label{defn:dual_map}
  Let $T \in L(V, W)$, where $V, W$ are finite dimensional real vector spaces. Let
  \begin{equation*}
    T^* : W^* \to V^*
  \end{equation*}
  be defined as follows: for $\beta \in W^*$, we have $T^*(\beta) \in V^*$.
  Let $v \in V$, and so $(T^*(\beta))(v) \in \mathbb{R}$
  \sidenote{It shall be verified here that $T^*(\beta)$ is indeed linear:
  let $v_1, v_2 \in V$ and $c_1, c_2 \in \mathbb{R}$.
  Indeed
  \begin{align*}
    &T^*(\beta)(c_1 v_1 + c_2 v_2) \\
      &= c_1 T^*(\beta)(v_1) + c_2 T^*(\beta)(v_2)
  \end{align*}}.
  From here, we may define
  \begin{equation*}
    (T^*(\beta))(v) = \beta(T(v)).
  \end{equation*}
  The map $T^*$ is called \hlnoteb{the dual map}.
\end{defn}

\begin{ex}
  Prove that $T^* \in L(W^*, V^*)$, i.e. that $T^*$ is linear.
\end{ex}

\begin{proof}
  Let $\beta_1, \beta_2 \in W^*$, $t_1, t_2 \in \mathbb{R}$, and $v \in V$.
  Then
  \begin{align*}
    T^*(t_1 \beta_1 + t_2 \beta_2)(v)
      &= (t_1 \beta_1 + t_2 \beta_2)(Tv) \\
      &= t_1 \beta_1 (Tv) + t_2 \beta_2 (Tv) \\
      &= t_1 T^*(\beta_1)(v) + t_2 T^*(\beta_2)(v).
  \end{align*}
\end{proof}

\begin{note}
  Note that in \cref{defn:dual_map}, our construction of $T^*$ is canonical,
  i.e. its construction is independent of the choice of a basis.

  Also, notice that in the language of pairings, we have
  \begin{equation*}
    \langle T^*\beta, v \rangle = (T^*(\beta))(v) = \beta(T(v)) = \langle \beta, T(v) \rangle,
  \end{equation*}
  where we note that
  \begin{gather*}
    T^*(\beta) \in V^* \quad v \in V \\
    \beta \in W^* \quad T(v) \in W.
  \end{gather*}
\end{note}

% section dual_map (end)

% chapter lecture_2_jan_09th (end)

\chapter{Lecture 3 Jan 11th}%
\label{chp:lecture_3_jan_11th}
% chapter lecture_3_jan_11th

\section{Dual Map (Continued)}%
\label{sec:dual_map_continued}
% section dual_map_continued

\begin{note}
  Elements in $V^*$ are also called \hldefn{co-vectors}.
\end{note}

Recall from last lecture that if $T \in L(V, W)$, then it
induces a dual map $T^* \in L(W^*, V^*)$ such that
\begin{equation*}
  (T^* \beta)(v) = \beta (T(v)).
\end{equation*}

\begin{propo}[Identity and Composition of the Dual Map]\label{propo:identity_and_composition_of_the_dual_map}
  Let $V$ and $W$ be finite dimensional real vector spaces.
  \begin{enumerate}
    \item Supppose $V = W$ and $T = I_V \in L(V)$, then
      \begin{equation*}
        (I_V)^* = I_{V^*} \in L(V^*).
      \end{equation*}

    \item Let $T \in L(V, W)$, $S \in L(W, U)$. Then
      $S \circ T \in L(V, U)$. Moreover,
      \begin{equation*}
        L \left( U^*, V^* \right) \ni ( S \circ T )^* = T^* \circ S^*.
      \end{equation*}
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item Observe that for any $\beta \in V^*$, and any $v \in V$,
      we have
      \begin{equation*}
        ((I_V)^*(\beta))(v) = \beta((I_V)(v)) = \beta(v).
      \end{equation*}
      Therefore $(I_V)^* = I_{V^*}$.

    \item Observe that for $\gamma \in U^*$ and $v \in V$, we have
      \begin{align*}
        ((S \circ T)^*(\gamma))(v)
          &= \gamma ( ( S \circ T ) (v) ) \\
          &= \gamma ( S ( T ( v ) ) ) \\
          &= S^* ( \gamma T(v) ) \\
          &= ( T^* \circ S^* )(\gamma)(v),
      \end{align*}
      and so $( S \circ T )^* = T^* \circ S^*$ as required.
  \end{enumerate}
\end{proof}

Let $T \in L(V)$, and the dual map $T^* \in L(V^*)$.
Let $\mathcal{B}$ be a basis of $V$, with the dual basis
$\mathcal{B}^*$. We may write
\begin{equation*}
  A = [T]_{\mathcal{B}} \text{ and } A^* = [T^*]_{\mathcal{B}^*}.
\end{equation*}
Note that
\begin{equation*}
  T(e_i) = A_i^j e_j \text{ and } T^*(e^i) = (A^*)_j^i e^j.
\end{equation*}
Consequently, we have
\begin{equation*}
  \langle e^k, T( e_i ) \rangle = A_i^k \text{ and }
  \langle T^*(e^i), e_k \rangle = (A^*)_k^i.
\end{equation*}

From here, notice that by applying $e_k \in V = V^{**}$ to both sides,
we have % TODO: I don't get this
\begin{equation*}
  (A^*)_k^i = e_k ( T^*(e^i) ) = \langle T^* (e^i), e_k \rangle
  \overset{(*)}{=} \langle e^i, T(e_k) \rangle = A_k^i.
\end{equation*}
Thus $A^*$ is the transpose of $A$, and
\begin{equation}\label{eq:coordinate_of_t_star_in_v_star_equals_transpose_of_coordinate_of_t_in_v}
  [ T^* ]_{\mathcal{B}^*} = [ T ]_{\mathcal{B}}^t
\end{equation}
where $M^t$ is the transpose of the matrix $M$.

\subsection{Application to Orientations}%
\label{sub:application_to_orientations}
% subsection application_to_orientations

Let $\mathcal{B}$ be a basis of $V$. Then $\mathcal{B}$ determines an
orientation of $V$. Let $\mathcal{B}^*$ be the dual basis of $V^*$.
So $\mathcal{B}^*$ determines an orientation for $V^*$.

\begin{eg}
  Suppose $\mathcal{B}$ and $\tilde{\mathcal{B}}$ determines the same
  orientation of $V$. Does it follow that the dual bases $\mathcal{B}^*$
  and $\tilde{\mathcal{B}}^*$ determine the same orientation of $V^*$?
\end{eg}

\begin{proof}
  Let
  \begin{align*}
    \mathcal{B} &= \left\{ e_1, \ldots, e_n \right\} &
    \tilde{\mathcal{B}} &= \left\{ \tilde{e}_1, \ldots, \tilde{e}_n \right\} \\
    \mathcal{B}^* &= \left\{ e^1, \ldots, e^n \right\} &
    \tilde{\mathcal{B}}^* &= \left\{ \tilde{e}^1, \ldots, \tilde{e}^n \right\}
  \end{align*}
  Let $T \in L(V)$ such that $T(e_i) = \tilde{e}_i$. By assumption, $\det T > 0$.
  Notice that
  \begin{equation*}
    \delta_j^i = \tilde{e}^i (\tilde{e}_j) = \tilde{e}^i ( Te_j ) = (T^*(\tilde{e}^i))(e_j),
  \end{equation*}
  and so we must have $T^*(\tilde{e}^i) = e^i$. By
  \cref{eq:coordinate_of_t_star_in_v_star_equals_transpose_of_coordinate_of_t_in_v},
  we have that
  \begin{equation*}
    \det T^* = \det T > 0
  \end{equation*}
  as well. This shows that $\mathcal{B}^*$ and $\tilde{\mathcal{B}}^*$ determines
  the same orientation.
\end{proof}

% subsection application_to_orientations (end)

% section dual_map_continued (end)

\section{The Space of \texorpdfstring{$k$}{k}-forms on \texorpdfstring{$V$}{V}}%
\label{sec:the_space_of_k_forms_on_v_}
% section the_space_of_k_forms_on_v_

\begin{defn}[$k$-Form]\index{$k$-Form}\label{defn:_k_form}
  Let $V$ be an $n$dimensional vector space. Let $k \geq 1$. A \hlnoteb{$k$-form} on
  $V$ is a map
  \begin{equation*}
    \alpha : \underbrace{V \times V \times \hdots \times V}_{k \text{ times }} \to \mathbb{R}
  \end{equation*}
  such that
  \begin{enumerate}
    \item (\hlnotea{$k$-linearity / multi-linearity}) if we fix all but one of
      the arguments of $\alpha$, then it is a linear map
      from $V$ to $\mathbb{R}$; i.e. if we fix
      \begin{equation*}
        v_1, \ldots, v_{j - 1}, v_{j + 1}, \ldots, v_k \in V,
      \end{equation*}
      then the map
      \begin{equation*}
        u \mapsto \alpha(v_1, \ldots, v_{j - 1}, u, v_{j + 1}, \ldots, v_k)
      \end{equation*}
       is linear in $u$.

    \item (\hlnotea{alternating property}) $\alpha$ is \hlnotea{alternating}
      (aka \hlnotea{totally skewed-symmetric}) in its $k$ arguments; i.e.
      \begin{equation*}
        \alpha ( v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k )
          = \alpha ( v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k ).
      \end{equation*}
  \end{enumerate}
\end{defn}

\begin{eg}
  The following is an example of the second condition: if $k = 2$, then
  $\alpha : V \times V \to \mathbb{R}$. Then $\alpha(v, w) = - \alpha(w, v)$.

  If $k = 3$, then $\alpha : V \times V \times V \to \mathbb{R}$. Then we
  have
  \begin{align*}
    \alpha(u, v, w) &= - \alpha(v, u, w) = - \alpha(w, v, u) = - \alpha(u, w, v) \\
                    &= \alpha(v, w, u) = \alpha(w, u, v).
  \end{align*}
\end{eg}

\begin{note}
  Note that if $k = 1$, then condition 2 is vacuous. Therefore, a $1$-form of
  $V$ is just an element of $V^* = L(W, \mathbb{R})$.
\end{note}

\begin{remark}[Permutations]\label{remark:permutations}
  From the last example, we notice that the `sign' of the value changes
  as we permute more times. To be precise, we are performing \hlnotea{transpositions}
  on the arguments
  \sidenote{See \href{https://tex.japorized.ink/PMATH347S18/classnotes.pdf\#defn.13}{PMATH 347}.},
  i.e. we only swap two of the arguments in a single move.
  Here are several remarks about permutations from group theory:
  \begin{itemize}
    \item A permutation $\sigma$ of $\left\{ 1, 2, \ldots, k \right\}$ is a
      bijective map.
    \item Compositions of permutations results in a permutation.
    \item The set $S_k$ of permutations on the set $\left\{ 1, 2, \ldots, k \right\}$
      is called a \hlnotea{group}.
    \item There are $k!$ such permutations.
    \item For each transposition, we may assign a \hlnotea{parity} of either $-1$ or $1$,
      and the parity is determined by the number of times we need to perform a transposition
      to get from $(1, 2, \ldots, k)$ to $(\sigma(1), \sigma(2), \ldots, \sigma(k))$. We
      usually denote a parity by $\sgn(\sigma)$.
  \end{itemize}

  The following is a fact proven in group theory: let $\sigma, \tau \in S_k$. Then
  \begin{align*}
    \sgn(\sigma \circ \tau) &= \sgn(\sigma) \cdot \sgn(\tau) \\
    \sgn(\id) &= 1 \\
    \sgn(\tau) &= \sgn(\tau^{-1}).
  \end{align*}
\end{remark}

Using the above remark, we can rewrite condition 2 as follows:

\begin{note}[Rewrite of condition 2 for \cref{defn:_k_form}]
  $\alpha$ is alternating, i.e.
  \begin{equation*}
    \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)}) = \sgn(\sigma) \cdot \alpha(v_1, \ldots, v_k),
  \end{equation*}
  where $\sigma \in S_k$.
\end{note}

\begin{remark}
  If $\alpha$ is a $k$-form on $V$, notice that
  \begin{equation*}
    \alpha(v_1, \ldots, v_k) = 0
  \end{equation*}
  if \hlimpo{any $2$ of the arguments are equal}.
\end{remark}

% section the_space_of_k_forms_on_v_ (end)

% chapter lecture_3_jan_11th (end)

\chapter{Lecture 4 Jan 14th}%
\label{chp:lecture_4_jan_14th}
% chapter lecture_4_jan_14th

\section{The Space of \texorpdfstring{$k$}{k}-forms on \texorpdfstring{$V$}{V} (Continued)}%
\label{sec:the_space_of_k_forms_on_v_continued}
% section the_space_of_k_forms_on_v_continued

\begin{defn}[Space of $k$-forms on $V$]\index{Space of $k$-forms on $V$}\label{defn:space_of_k_forms_on_v_}
  The \hlnoteb{space of $k$-forms on $V$}, denoted as $\wedge^k \left( V^* \right)$, is
  the set of all $k$-forms on $V$, made into a vector space by setting
  \begin{equation*}
    (t\alpha + s\beta)(v_1, \ldots, v_k) :=
      t\alpha(v_1, \ldots, v_k) + s\beta(v_1, \ldots, v_k),
  \end{equation*}
  for $\alpha \beta \in \wedge^k \left( V^* \right)$, $t, s \in \mathbb{R}$.
\end{defn}

\begin{note}
  By convention, we define $\wedge^0 \left( V^* \right) = \mathbb{R}$. The reasoning shall
  we shown later. %TODO : add anchor
\end{note}

\begin{note}
  By the note on page \pageref{remark:permutations}, observe that
  $\wedge^1 \left( V^* \right) = V^*$.
\end{note}

\begin{propo}[A $k$-form is equivalently $0$ if its arguments are linearly dependent]\label{propo:a_k_form_is_equivalently_0_if_its_arguments_are_linearly_dependent}
  Let $\alpha$ be a $k$-form. Then if $v_1, \ldots, v_k$ are linearly dependent, then
  \begin{equation*}
    \alpha(v_1, \ldots, v_k) = 0.
  \end{equation*}
\end{propo}

\begin{proof}
  Suppose one of the $v_1, \ldots, v_k$ is a linear combination of the rest of the
  other vectors; i.e.
  \begin{equation*}
    v_j = c_1 v_1 + \hdots + c_{j - 1} v_{j - 1} + c_{j + 1} v_{j + 1} + \hdots + c_k v_k.
  \end{equation*}
  Then since $\alpha$ is multilinear, and by the last remark in \cref{chp:lecture_3_jan_11th},
  we have
  \begin{equation*}
    \alpha (v_1, \ldots, v_{j - 1}, v_j , v_{j + 1}, \ldots, v_k) = 0.
  \end{equation*}
\end{proof}

\begin{crly}[$k$-forms of even higher dimensions]\label{crly:_k_forms_of_even_higher_dimensions}
  $\wedge^k \left( V^* \right) = \left\{ 0 \right\}$ if $k > n = \dim V$.
\end{crly}

\begin{proof}
  Any set of $k > n$ vectors is necessarily linearly dependent.
\end{proof}

\begin{note}
  \cref{crly:_k_forms_of_even_higher_dimensions} implies that $\wedge^k \left( V^* \right)$
  can only be non-trivial for $0 \leq k \leq n = \dim V$.
\end{note}

% section the_space_of_k_forms_on_v_continued (end)

\section{Decomposable \texorpdfstring{$k$}{k}-forms}%
\label{sec:decomposable_k_forms}
% section decomposable_k_forms

There is a simple way to construct a $k$-form on $V$ using $k$-many $1$-forms from $V$, i.e.
$k$-many elements from $V^*$. Let $\alpha^1, \ldots, \alpha^k \in V^*$. Define a map
\begin{equation*}
  \alpha^1 \land \hdots \land \alpha^k : \underbrace{V \times V \times \hdots \times V}_{k \text{ copies }} \to \mathbb{R}
\end{equation*}
by
\begin{equation}\label{eq:defining_a_k_form}
  \left( \alpha^1 \land \hdots \land \alpha^k \right) ( v_1, \ldots, v_k )
  := \sum_{\sigma \in S_k} \left( \sgn \sigma \right) \alpha^{\sigma(1)}(v_1) \alpha^{\sigma(2)}(v_2) \hdots \alpha^{\sigma(k)}(v_k).
\end{equation}

We need, of course, to verify that the above formula is, indeed, a $k$-form.
Before that, consider the following example:

\begin{eg}
  If $k = 2$, we have
  \begin{equation*}
    \left( \alpha^1 \land \alpha^2 \right)(v_1, v_2)
      = \alpha^1(v_1) \alpha^2(v_2) - \alpha^2(v_1) \alpha^1(v_2).
  \end{equation*}
  and if $k = 3$, we have
  \begin{align*}
    \left( \alpha^1 \land \alpha^2 \land \alpha^3 \right)( v_1, v_2, v_3 )
      &= \alpha^1(v_1) \alpha^2(v_2) \alpha^3(v_3) + \alpha^2(v_1) \alpha^3(v_2) \alpha^1(v_1) \\
      &\quad + \alpha^3(v_1) \alpha^1(v_2) \alpha^2(v_3) - \alpha^1(v_1) \alpha^3(v_2) \alpha^2(v_3) \\
      &\qquad - \alpha^2(v_1) \alpha^1(v_1) \alpha^3(v_3) - \alpha^3(v_1) \alpha^2(v_2) \alpha^1(v_3).
  \end{align*}
\end{eg}

Now consider a general case of $k$. It is clear that \cref{eq:defining_a_k_form} is
$k$-linear: if we fix any one of the arguments, then \cref{eq:defining_a_k_form} is
reduced to a linear equation.

For the alternating property, let $\tau \in S_k$. WTS
\begin{equation*}
  \left( \alpha^1 \land \hdots \land \alpha^k \right) \left( v_{\tau(1)}, \ldots, v_{\tau(k)} \right)
    = ( \sgn \tau )\left( \alpha^1 \land \hdots \land \alpha^k \right) \left( v_1, \ldots, v_k \right).
\end{equation*}
Observe that
\begin{align*}
  &\left( \alpha^1 \land \hdots \land \alpha^k \right) \left( v_{\tau(1)}, \ldots, v_{\tau(k)} \right) \\
  &= \sum_{\sigma \in S_k} \left( \sgn \sigma \right)
    \alpha^{\sigma(1)} \left(v_{\tau(1)}\right) \hdots \alpha^{\sigma(k)} \left( v_{\tau(k)} \right) \\
  &= \sum_{\sigma \in S_k} \left( \sgn \sigma \circ \tau^{-1} \right) \left( \sgn \tau \right)
    \alpha^{\left(\sigma \circ \tau^{-1}\right)(\tau(1))} \left(v_{\tau(1)}\right) \hdots
    \alpha^{\left( \sigma \circ \tau^{-1} \right)(\tau(k))} \left( v_{\tau(k)} \right) \\
  &= \left( \sgn \tau \right) \sum_{\sigma \circ \tau^{-1} \in S_k} \left( \sgn \sigma \circ \tau^{-1} \right)
    \alpha^{\left( \sigma \circ \tau^{-1} \right)(1)} (v_1) \hdots
    \alpha^{\left( \sigma \circ \tau^{-1} \right)(k)} (v_k) \\
  &= \left( \sgn \tau \right) \sum_{\sigma \in S_k} \alpha^{\sigma(1)} (v_1) \hdots \alpha^{\sigma(k)} (v_k) \quad \because \text{ relabelling } \\
  &= \left( \sgn \tau \right) \left( \alpha^1 \land \hdots \alpha^k \right) \left( v_1, \ldots, v_k \right),
\end{align*}
as claimed.

\begin{fullwidth}
\begin{defn}[Decomposable $k$-form]\index{Decomposable $k$-form}\label{defn:decomposable_k_form}
  The $k$-form as discussed above is called a \hlnoteb{decomposable $k$-form}, which for
  ease of reference shall be re-expressed here:
  \begin{equation*}
    \left( \alpha^1 \land \hdots \land \alpha^k \right) ( v_1, \ldots, v_k )
    := \sum_{\sigma \in S_k} \left( \sgn \sigma \right) \alpha^{\sigma(1)}(v_1) \alpha^{\sigma(2)}(v_2) \hdots \alpha^{\sigma(k)}(v_k).
  \end{equation*}
\end{defn}
\end{fullwidth}

\begin{note}
  Not all $k$-forms are decomposable. If $k = 1, n - 1$ and $n$, but not for $1 < k < n - 1$.
\end{note}

In A1Q5(c), we will show that there exists a $2$-form in $n = 4$ that is not decomposable.

\begin{propo}[Permutation on $k$-forms]\label{propo:permutation_on_k_forms}
  Let $\tau \in S_k$. Then
  \begin{equation*}
    \alpha^{\tau(1)} \land \hdots \land \alpha^{\tau(k)}
      = ( \sgn \tau ) \alpha^1 \land \hdots \land \alpha^{k}
  \end{equation*}
\end{propo}

\begin{proof}
  Firstly, note that $\sgn \tau = \sgn \tau^{-1}$. Then for any $(v_1, \ldots, v_k) \in V^k$,
  we have
  \begin{align*}
    &\alpha^{\tau(1)} \land \hdots \land \alpha^{\tau(k)} (v_1, \ldots, v_k) \\
    &= \sum_{\sigma \in S_k} (\sgn \sigma) \alpha^{\sigma \circ \tau(1)} (v_1) \hdots \alpha^{\sigma \circ \tau (k)} (v_k) \\
    &= \sum_{\sigma \circ \tau S_k} (\sgn \sigma \circ \tau) \left(\sgn \tau^{-1}\right)
      \alpha^{\sigma \circ \tau(1)} (v_1) \hdots \alpha^{\sigma \circ \tau(k)} (v_k) \\
    &= (\sgn \tau) \sum_{\sigma \in S_k} (\sgn \sigma)
      \alpha^{\sigma(1)} (v_1) \hdots \alpha^{\sigma(k)} (v_k) \\
    &= (\sgn \tau) (\alpha^1 \land \hdots \land \alpha^k).
  \end{align*}
  This completes our proof.
\end{proof}

\marginnote{Proof for \cref{propo:alternate_definition_of_a_decomposable_k_form} is in A1.}
\begin{propo}[Alternate Definition of a Decomposable $k$-form]\label{propo:alternate_definition_of_a_decomposable_k_form}
  Another way we can define a decomposable $k$-form is
  \begin{equation*}
    (\alpha^1 \land \hdots \land \alpha^k)(v_1, \ldots, v_k)
    = \sum_{\sigma \in S_k} (\sgn \sigma) \alpha^1 (v_{\sigma(1)}) \hdots \alpha^k (v_{\sigma(k)}).
  \end{equation*}
\end{propo}

\begin{thm}[Basis of $\Lambda^k(V^*)$]\label{thm:basis_of_lambda_k_v_}
  Let $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ be a basis of $V$, a $n$-dimensional
  real vector space, and the dual basis $\mathcal{B}^* = \left\{ e^1, \ldots, e^n \right\}$
  of $V^*$. THen the set
  \begin{equation*}
    \left\{ e^{j_1} \land \ldots \land e^{j_k} \mmid 1 \leq j_1 < j_2 < \hdots < j_k \leq n \right\}
  \end{equation*}
  is a basis of $\Lambda^k(V^*)$.
\end{thm}

\begin{crly}[Dimension of $\Lambda^k(V^*)$]\label{crly:dimension_of_lambda_k_v_}
  The dimension of $\Lambda^k(V^*)$ is $\binom{n}{k} = \binom{n}{n - k}$, which is
  also the dimension of $\Lambda^{n - k}(V^*)$. This also works for $k = 0$ \sidenote{
  This is why we wanted $\Lambda^0(V^*) = \mathbb{R}$.}.
\end{crly}

\begin{proof}[\cref{thm:basis_of_lambda_k_v_}]
  Firstly, let $\alpha$ be an arbitrary $k$-form, and let $v_1, \ldots, v_k \in V$. We
  may write
  \begin{equation*}
    v_i = v_i^j e_j,
  \end{equation*}
  where $v_i^j \in \mathbb{R}$. Then
  \begin{align*}
    \alpha(v_1, \ldots, v_k) &= \alpha\left(v_1^{j_1} e_{j_1}, \ldots, v_k^{j_k} e_{j_k}\right) \\
                             &= v_1^{j_1} \hdots v_k^{j_k} \alpha ( e_{j_1}, \ldots, e_{j_k} )
  \end{align*}
  by multilinearity and totally skew-symmetry of $\alpha$, where
  $j_i \in \left\{ 1, \ldots, n \right\}$. Let
  \begin{equation}\label{eq:another_alt_of_a_k_form_scalar}
    \alpha(e_{j_1}, \ldots, e_{j_k}) = \alpha_{j_1, \ldots, j_k},
  \end{equation}
  represent the scalar. Then
  \begin{align*}
    \alpha(v_1, \ldots, v_k) &= \alpha_{j_1, \ldots, j_k} v_1^{j_1} \hdots v_k^{j_k} \\
                             &= \alpha_{j_1, \ldots, j_k} e^{j_1}(v_1) \hdots e^{j_k}(v_k).
  \end{align*}
  Now since $\alpha_{j_1, \ldots, j_k}$ is totally skew-symmetric, $\alpha = 0$ if any of the
  $j_k$'s are equal to one another. Thus we only need to consider the terms where the $j_k$'s
  are distinct. Now for any set of $\{ j_1, \ldots, j_k \}$, there exists a unique
  $\sigma \in S_k$ such that $\sigma$ rearranges the $j_i$'s so that $j_1, \ldots, j_k$ is
  strictly increasing. Thus
  \begin{align*}
    \alpha(v_1, \ldots, v_k)
      &= \sum_{j_1 < \hdots < j_k} \sum_{\sigma \in S_k} \alpha_{j_{\sigma1(), \ldots, \sigma(k)}}
        e^{j_{\sigma(1)}} (v_1) \hdots e^{j_{\sigma(k)}} (v_k) \\
      &= \sum_{j_1 < \hdots < j_k} \sum_{\sigma \in S_k} (\sgn \sigma) \alpha_{j_1, \ldots, j_k}
        e^{j_{\sigma(1)}} (v_1) \hdots e^{j_{\sigma(k)}} (v_k) \\
      &= \sum_{j_1 < \hdots < j_k} \alpha_{j_1, \ldots, j_k} \sum_{\sigma \in S_k} (\sgn \sigma)
        e^{j_{\sigma(1)}} (v_1) \hdots e^{j_{\sigma(k)}} (v_k) \\
      &= \underbrace{ \sum_{j_1 < \hdots < j_k}
          \alpha_{j_1, \ldots, j_k} \left( e^{j_1} \land \hdots \land e^{j_k} \right)
        }_{\alpha}
        (v_1, \ldots, v_k).
  \end{align*}
  Thus we have that
  \begin{equation}\label{eq:k_form_in_general}
    \alpha = \sum_{j_1 < \hdots < j_k} \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k}.
  \end{equation}
  Hence $e^{j_1} \land \hdots \land e^{j_k}$ spans $\Lambda^k(V^*)$.

  Now suppose that
  \begin{equation*}
    \sum_{j_1 < \hdots < j_k} \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k}
  \end{equation*}
  is the zero element in $\Lambda^k(V^*)$. Then the scalar in \cref{eq:another_alt_of_a_k_form_scalar} must be $0$ for any $j_1, \ldots, j_k$. Thus
  \begin{equation*}
    \left\{ e^{j_1} \land \hdots \land e^{j_k} \mmid 1 \leq j_1 < j_2 < \hdots < j_k \leq n \right\}
  \end{equation*}
  is linearly independent.
\end{proof}

% section decomposable_k_forms (end)

% chapter lecture_4_jan_14th (end)

\chapter{Lecture 5 Jan 16th}%
\label{chp:lecture_5_jan_16th}
% chapter lecture_5_jan_16th

\section{Decomposable \texorpdfstring{$k$}{k}-forms Continued}%
\label{sec:decomposable_k_forms_continued}
% section decomposable_k_forms_continued

There exists an equivalent, and perhaps more useful, expression for \cref{eq:k_form_in_general},
which we shall derive here. Sine $\alpha_{j_1, \ldots, j_k}$ and
$e^{j_1} \land \hdots \land e^{j_k}$ are both totally skew-symmetric in their $k$ indices,
and since there are $k!$ elements in $S_k$, we have that
\begin{align*}
  \frac{1}{k!} \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k}
  &= \frac{1}{k!} \sum_{\substack{j_1, \ldots, j_k \\ \text{distinct}}}
    \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k} \\
  &= \frac{1}{k!} \sum_{j_1 < \hdots < j_k} \sum_{\sigma \in S_k}
    \alpha_{\sigma(j_1), \ldots, \sigma(j_k)} e^{\sigma(j_1)} \land \hdots \land e^{\sigma(j_k)} \\
  &= \frac{1}{k!} \sum_{j_1 < \hdots < j_k} \sum_{\sigma \in S_k} (\sgn \sigma)
    \alpha_{j_1, \ldots, j_k} (\sgn \sigma) e^{j_1} \land \hdots \land e^{j_k} \\
  &= \frac{1}{k!} \sum_{j_1 < \hdots < j_k} \sum_{\sigma \in S_k}
    \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k} \enspace \footnotemark \\
  &= \sum_{j_1 < \hdots < j_k} \alpha_{j_1, \ldots, j_k} e^{j_1} \land \hdots \land e^{j_k}.
\end{align*}
\footnotetext{Note that $(\sgn \sigma)(\sgn \sigma) = 1$.}
The major advantage of the expression with $\frac{1}{k!}$ is that all $k$ indices
$j_1, \ldots, j_k$ are summed over all possible values $1, \ldots, n$ instead of
having to start with a specific order.

% section decomposable_k_forms_continued (end)

\section{Wedge Product of Forms}%
\label{sec:wedge_product_of_forms}
% section wedge_product_of_forms

\begin{defn}[Wedge Product]\index{Wedge Product}\label{defn:wedge_product}
  Let $\alpha \in \Lambda^k(V^*)$ and $\beta \in \Lambda^l(V^*)$. We define
  $\alpha \wedge \beta \in \Lambda^{k + l}(V^*)$ as follows. Choose a basis
  $\mathcal{B}^* = \left\{ e^1, \ldots, e^k \right\}$ of  $V^*$. Then we
  may write
  \begin{equation*}
    \alpha = \frac{1}{k!} \alpha_{i_1, \ldots, i_k} e^{i_1} \wedge \hdots \wedge e^{i_k} \quad
    \beta = \frac{1}{l!} \beta_{j_1, \ldots, j_l} e^{j_1} \wedge \hdots \wedge e^{j_l}.
  \end{equation*}
  We define the \hlnoteb{wedge product} as
  \begin{align*}
    \alpha \wedge \beta
      &:= \frac{1}{k! l!} \alpha_{i_1, \ldots, i_k} \beta_{j_1, \ldots, j_l}
      e^{i_1} \wedge \hdots \wedge e^{i_k} \wedge e^{j_1} \wedge \hdots \wedge e^{j_l} \\
      &= \sum_{i_1 < \hdots < i_k} \sum_{j_1 < \hdots < j_l}
        \alpha_{i_1, \ldots, i_k} \beta_{j_1, \ldots, j_k}
      e^{i_1} \wedge \hdots \wedge e^{i_k} \wedge e^{j_1} \wedge \hdots \wedge e^{j_l}.
  \end{align*}
\end{defn}

One can then question if this definition is well-defined, since it appears to be
reliant on the choice of a basis. In A1Q4(a), we will show that this defintiion
of $\alpha \wedge \beta$ is indeed well-defined. In particular, one can show that
we may express $\alpha \wedge \beta$ in a way that does not involve any of the
basis vectors $e^1, \ldots, e^n$.

\begin{defn}[Degree of a Form]\index{Degree of a Form}\label{defn:degree_of_a_form}
  For $\alpha \in \Lambda^k(V^*)$, we say that $\alpha$ has \hlnoteb{degree} $k$,
  and write $\abs{\alpha} = k$.
\end{defn}

\begin{note}
  By our definition of a wedge product above, we have that
  \begin{equation*}
    \abs{\alpha \wedge \beta} = \abs{\alpha} + \abs{\beta}.
  \end{equation*}
  Note that since a $0$-form lies in $\Lambda^k(V^*)$ for all $k$, we let $\abs{k}$
  be anything / undefined.
\end{note}

\begin{remark}\label{remark:wedge_product_operation_properties}
  \begin{enumerate}
    \item $\alpha \wedge \beta$ is linear in $\alpha$ and linear in $\beta$ by its
      definition, i.e. for any $t_1, t_2 \in \mathbb{R}$,
      $\alpha_1, \alpha_2 \in \Lambda^k(V^*)$, and any $\beta \in \Lambda^l(V^*)$,
      \begin{equation*}
        (t_1 \alpha_1 + t_2 \alpha_2) \wedge \beta
        = t_1(\alpha_1 \wedge \beta) + t_2(\alpha_2 \wedge \beta),
      \end{equation*}
      and a similar equation works for linearity in $\beta$.
    \item The wedge product is associative; this follows almost immediately from
      its construction.
    \item The wedge product is not commutative. In fact, if $\abs{\alpha} = k$ and
      $\abs{\beta} = l$, then
      \begin{equation}\label{eq:wedge_product_is_not_commutative}
        \beta \wedge \alpha = \left( -1 \right)^{kl} \alpha \wedge \beta.
      \end{equation}
      We call this property of a wedge product \hldefn{graded commutative},
      \hldefn{super commutative} or \hldefn{skewed-commutative}.

      Note that this also means that \hlimpo{even degree forms commute} with any
      form.

      Also, note that if $\abs{\alpha}$ is odd, then $\alpha \wedge \alpha = 0$.
  \end{enumerate}
\end{remark}

\begin{eg}
  Let $\alpha = e^1 \wedge e^3$ and $\beta = e^2 + e^3$. Then
  \begin{align*}
    \alpha \wedge \beta &= \left( e^1 \wedge e^3 \right) \wedge \left( e^2 + e^3 \right) \\
                        &= e^1 \wedge e^3 \wedge e^2 + e^1 \wedge e^3 \wedge e^3 \\
                        &= - e^1 \wedge e^2 \wedge e^3 + 0 \\
                        &= - e^1 \wedge e^2 \wedge e^3.
  \end{align*}
\end{eg}

\begin{eg}
  Suppose $\alpha^1, \ldots, \alpha^k$ are linearly dependent $1$-forms on $V$.
  Then $\alpha^1 \wedge \hdots \wedge \alpha^k = 0$.
\end{eg}

\begin{proof}
  Suppose at least one of the $\alpha^j$ is a linear combination of the rest, i.e.
  \begin{equation*}
    \alpha^j = c_1 \alpha^1 + \hdots + c_{j - 1} \alpha^{j - 1} +
      c_{j + 1} \alpha^{j + 1} + \hdots + c_k \alpha^k.
  \end{equation*}
  Since all of the $\alpha^i$'s are $1$-forms, we will have
  $\alpha^i \wedge \alpha^i$ in the wedge product, and so our result follows from
  our earlier remark.
\end{proof}

\begin{eg}
  Let $\alpha = \alpha_i e^i, \beta = \beta_j e^j \in V^*$. Then
  \begin{align*}
    \alpha \wedge \beta &= \alpha_i \beta_j e^i \wedge e^j \\
                        &= \frac{1}{2} \alpha_i \beta_j e^i \wedge e^j
                          + \frac{1}{2} \alpha_i \beta_j e^i \wedge e^j \\
                        &= \frac{1}{2} \alpha_i \beta_j e^i \wedge e^j
                          - \frac{1}{2} \alpha_j \beta_i e^i \wedge e^j \\
                        &= \frac{1}{2} (\alpha_i \beta_j - \alpha_j \beta_i)
                          e^1 \wedge e^j \\
                        &= \frac{1}{2} (\alpha \wedge \beta)_{ij} e^i \wedge e^j,
  \end{align*}
  where $(\alpha \wedge \beta)_{ij} = \alpha_i \beta_j - \alpha_j \beta_i$.
\end{eg}

We shall prove the following in A1Q6.

\begin{ex}
  Let $\alpha = \alpha_i e^i \in V^*$, and
  \begin{equation*}
    \eta = \frac{1}{2} \eta_{jk} e^j \wedge e^k \in \Lambda^2(V^*).
  \end{equation*}
  Show that
  \begin{equation*}
    \alpha \wedge \eta = \frac{1}{6!} (\alpha \wedge \eta)_{ijk} e^i \wedge e^j \wedge e^k,
  \end{equation*}
  where
  \begin{equation*}
    (\alpha \wedge \eta)_{ijk} = \alpha_1 \eta_{jk} + \alpha_j \eta_{ki} + \alpha_k \eta_{ij}.
  \end{equation*}
\end{ex}

% section wedge_product_of_forms (end)

\section{Pullback of Forms}%
\label{sec:pullback_of_forms}
% section pullback_of_forms

For a linear map $T \in L(V, W)$, we have seen its induced dual map
$T^* \in L\left(W^*, V^*\right)$. We shall now generalize this dual map to $k$-forms,
for $k > 1$.

\begin{defn}[Pullback]\index{Pullback}\label{defn:pullback}
  Let $T \in L(V, W)$. For any $k \geq 1$, define a map
  \begin{equation*}
    T^* : \Lambda^k(W^*) \to \Lambda^k(V^*),
  \end{equation*}
  called the \hlnoteb{pullback}, as such: let $\beta \in \Lambda^k(W^*)$, and define
  $T^* \beta \in \Lambda^k(V^*)$ such that
  \begin{equation*}
    \left( T^* \beta \right)(v_1, \ldots, v_k) := \beta \left( T(v_1), \ldots, T(v_k) \right).
  \end{equation*}
\end{defn}

\begin{note}
  It is clear that $T^* \beta$ is multilinear and alternating, since $T$ itself is
  linear, and $\beta$ is multilinear and alternating.
\end{note}

The pullback has the following properties which we shall prove in A1Q8.

\begin{propo}[Properties of the Pullback]\label{propo:properties_of_the_pullback}
  \begin{enumerate}
    \item The map $T^* : \Lambda^k(W^*) \to \Lambda^k(V^*)$ is linear, i.e.
      $\forall \alpha, \beta \in \Lambda^k(W^*)$ and $s, t \in \mathbb{R}$,
      \begin{equation}
        T^* (t\alpha + s\beta) = tT^* \alpha + sT^* \beta.
      \end{equation}

    \item The map $T^*$ is compatible iwth the wedge product operation in the
      following sense: if $\alpha \in \Lambda^k(W^*)$ and $\beta \in \Lambda^l(W^*)$,
      then
      \begin{equation*}
        T^* (\alpha \wedge \beta) = \left( T^* \alpha \right) \wedge \left( T^* \beta \right).
      \end{equation*}
  \end{enumerate}
\end{propo}

% section pullback_of_forms (end)

% chapter lecture_5_jan_16th (end)

\tuftepart{The Vector Space \texorpdfstring{$\mathbb{R}^n$}{Rn} as a Smooth Manifold}

\chapter{Lecture 6 Jan 18th}%
\label{chp:lecture_6_jan_18th}
% chapter lecture_6_jan_18th

\section{The space \texorpdfstring{$\Lambda^k(V)$}{Lambda k(V)} of \texorpdfstring{$k$}{k}-vectors and Determinants}%
\label{sec:the_space_lambda_k_v_of_k_vectors_and_determinants}
% section the_space_lambda_k_v_of_k_vectors_and_determinants

Recall that we identified $V$ with $V^{**}$, and so we may consider
$\Lambda^k(V) = \Lambda^k(V^{**})$ as the space of $k$-linear alternating maps
\begin{equation*}
  \underbrace{V^* \times V^* \times \hdots \times V^*}_{k \text{ copies }} \to \mathbb{R}.
\end{equation*}
Consequently (to an extent), the elements of $\Lambda^k(V)$ are called \hldefn{$k$-vectors}.
A $k$-vector is an alternating $k$-linear map that takes $k$ \hlnotea{covectors} (of $1$-forms)
to $\mathbb{R}$.

\begin{eg}
  Let $\{ e_1, \ldots, e_n \}$ be a basis of $V$ with the dual basis
  $\{ e^1, \ldots, e^n \}$, which is a basis of $V^*$. Then any
  $\mathcal{A} \in \Lambda^k(V^*)$ can be written uniquely as
  \begin{equation*}
    \mathcal{A} = \sum_{i_1 < \hdots < i_k} \mathcal{A}^{i_1, \ldots, i_k} \;
      e_{i_1} \land \hdots \land e_{i_k}
  \end{equation*}
  where
  \begin{equation*}
    \mathcal{A}^{i_1, \ldots, i_k} = \mathcal{A} \left( e^{i_1}, \ldots, e^{i_k} \right).
  \end{equation*}
  We also have that
  \begin{equation*}
    \mathcal{A} = \frac{1}{k!} \mathcal{A}^{i_1, \ldots, i_k} \; e_{i_1} \land \hdots \land e_{i_k}.
  \end{equation*}
\end{eg}

\begin{note}
  Note that
  \begin{equation*}
    \dim \Lambda^k(V) = \frac{n!}{k! (n - k)!}.
  \end{equation*}
\end{note}

\begin{defn}[$k$\textsuperscript{th} Exterior Power of $T$]\index{$k$\textsuperscript{th} Exterior Power of $T$}\label{defn:_k_th_exterior_power_of_t_}
  Let $T \in L(V, W)$. Then $T$ induces a linear map
  \begin{equation*}
    \Lambda^k(T) \in L \left( \Lambda^k(V), \Lambda^k(W) \right),
  \end{equation*}
  defined as
  \begin{equation*}
    \left( \Lambda^k T \right)(v_1 \land \hdots \land v_k) = T(v_1) \land \hdots \land T(v_k),
  \end{equation*}
  where $v_1, \ldots, v_k$ are decomposable elements of $\Lambda^k(V)$, and then extended
  by linearity to all of $\Lambda^k(V)$. The map $\Lambda^k T$ is called the
  \hlnoteb{$k$\textsuperscript{th} exterior power of $T$}.
\end{defn}

\begin{note}
  Consider the special case of when $W = V$ and $k = n = \dim V$. Then $T \in L(V)$
  induces a linear operator $\Lambda^n(T) \in L(\Lambda^n(V))$. It is also noteworthy
  to point out that any linear operator on a $1$-dimensional vector space is just
  scalar multiplication.

  Furthermore, notice that in the above special case, we have
  \begin{equation*}
    \dim \Lambda^n(V) = \binom{n}{n} = 1.
  \end{equation*}
\end{note}

\begin{defn}[Determinant]\index{Determinant}\label{defn:determinant}
  Let $\dim V = n$ and $T \in L(V)$. We have that $\dim \Lambda^n(V) = 1$. Then
  $\Lambda^n T \in L \left( \Lambda^n(V) \right)$ is a scalar multiple of the
  identity. We denote this scalar multiple by $\det T$, and call it the
  \hlnoteb{determinant} of $T$, i.e.
  \begin{equation*}
    \Lambda^n (T) \mathcal{A} = (\det T) IA
  \end{equation*}
  for any $\mathcal{A} \in \Lambda^n(V)$, where $I$ is the identity operator.
\end{defn}

\begin{note}
  We should verify that this `new' definition of a determinant agrees with the
  \href{https://en.wikipedia.org/wiki/Determinant\#n_\%C3\%97_n_matrices}{`classical' definition}
  of a determinant.
\end{note}

\begin{proof}
  Let $\mathcal{B} = \{ e_1, \ldots, e_n \}$ be a basis of $V$, and let
  $A = [T]_{\mathcal{B}}$ be the $n \times n$ matrix of $T$ wrt the basis
  $\mathcal{B}$. So $T(e_i) = A_i^j e_j$. Then $\{ e_1 \land \hdots \land e_n \}$
  is a basis of $\Lambda^n(V)$, and
  \begin{align*}
    \left( \Lambda^n T \right)(e_1 \land \hdots \land e_n)
    &= T(e_1) \land \hdots \land T(e_n) \\
    &= A_1^{i_1} e_{i_1} \land \hdots \land A_n^{i_n} e_{i_n} \\
    &= A_1^{i_1} A_2^{i_2} \hdots A_n^{i_n} \; e_{i_1} \land \hdots \land e_{i_n} \\
    &= \sum_{\substack{i_1, \ldots, i_n \\ \text{distinct}}} A_1^{i_1} \hdots A_n^{i_n} \;
      e_{i_1} \land \hdots \land e_{i_n} \\
    &= \sum_{\sigma \in S_n} A_1^{\sigma(1)} \hdots A_n^{\sigma(n)} \;
      e_{\sigma(1)} \land \hdots \land e_{\sigma(n)} \\
    &= \sum_{\sigma \in S_n} A_1^{\sigma(1)} \hdots A_n^{\sigma(n)} \;
      (\sgn \sigma) e_1 \land \hdots \land e_n \\
    &= \left( \sum_{\sigma \in S_n} (\sgn \sigma) A_1^{\sigma(1)} \hdots A_n^{\sigma(n)} \right)
      \left( e_1 \land \hdots \land e_n \right) \\
    &= \left( \sum_{\sigma \in S_n} (\sgn \sigma) \prod_{i = 1}^{n} A_i^{\sigma(i)} \right)
      \left( e_1 \land \hdots \land e_n \right).
  \end{align*}
  We observe that we indeed have
  \begin{equation*}
    \det T = \sum_{\sigma \in S_n} (\sgn \sigma) \prod_{i = 1}^{n} A_i^{\sigma(i)}.
  \end{equation*}
\end{proof}

% section the_space_lambda_k_v_of_k_vectors_and_determinants (end)

\section{Orientation Revisited}%
\label{sec:orientation_revisited}
% section orientation_revisited

Now that we have this notion, we may finally clarify to ourselves what an orientation
is without having to rely on roundabout methods as before.

\marginnote{Basically, we now have a more mathematical way of saying `pick a direction
and consider it as the positive direction of $V$, and that'll be our orientation'.}
\begin{defn}[Orientation]\index{Orientation}\label{defn:orientation}
  Let $V$ be an $n$-dimensional real vector space. Then $\Lambda^n(V)$ is a
  $1$-dimensional real vector space. An \hlnoteb{orientation} on $V$ is defined as a
  \hlimpo{choice} of a non-zero element $\mu \in \Lambda^n(V)$, up to positive scalar
  multiples.
\end{defn}

\begin{note}
  For any two such orientations $\mu$ and $\tilde{\mu}$, we have that
  $\tilde{\mu} = \lambda \mu$ for some non-zero $\lambda \in \mathbb{R}$, and by using
  the definition of having the \hyperref[defn:same_and_opposite_orientations]{same orientation},
  we say that $\mu \sim \tilde{\mu}$ if $\lambda > 0$ and $\mu \not\sim \tilde{\mu}$ if
  $\lambda < 0$.
\end{note}

\begin{ex}
  Check that \cref{defn:orientation} agrees with \cref{defn:same_and_opposite_orientations}.
  (Hint: Let $\mathcal{B} = \{ e_1, \ldots, e_n \}$ be a basis of $V$ and let
  $\mu = e_1 \land \hdots \land e_n$.)
\end{ex}

% section orientation_revisited (end)

\section{Topology on \texorpdfstring{$\mathbb{R}^n$}{Rn}}%
\label{sec:topology_on_r_n_}
% section topology_on_r_n_

We shall begin with a brief review of some ideas from multivariable calculus.

We know that $\mathbb{R}^n$ is an $n$-dimensional real vector space. It has a canonical
\hlnotea{positive-definite} \hldefn{inner product}, aka the \hldefn{Euclidean inner product},
or the \hldefn{dot product}: given
$x = (x_1, \ldots, x_n), y = (y_1, \ldots, y_n) \in \mathbb{R}^n$, we have
\begin{equation*}
  x \cdot y = \sum_{i=1}^{n} x^i y^i = \delta_{ij} x^i y^j.
\end{equation*}

The following properties follow  from above: for any $t, s \in \mathbb{R}$ and
$x, y , w\in \mathbb{R}^n$,
\begin{itemize}
  \item $(tx + sy) \cdot w = t ( x \cdot w ) = s(y \cdot w)$;
  \item $x \cdot (ty + sw) = t(x \cdot y) + t(x \cdot w)$;
  \item $x \cdot y = y \cdot x$;
  \item (positive definiteness) $x \cdot x \geq 0$ with $x \cdot x = 0 \iff x = 0$;
  \item (Cauchy-Schwarz Ineq.) $-\norm{x} \norm{y} \leq x \cdot y \leq \norm{x} \norm{y}$,
    i.e.
    \begin{equation*}
      x \cdot y = \norm{x} \norm{y} \cos \theta
    \end{equation*}
    where $\theta \in [0, \pi]$.
\end{itemize}

\begin{defn}[Distance]\index{Distance}\label{defn:distance}
  The \hlnoteb{distance} between $x, y \in \mathbb{R}^n$ is given as
  \begin{equation*}
    \dist(x, y) = \norm{x - y}.
  \end{equation*}
\end{defn}

\begin{note}[Triangle Inequality]
  Note that the triangle inequality holds for the distance function\sidenote{See
  also \href{https://tex.japorized.ink/PMATH351F18/classnotes.pdf}{PMATH 351}}: for any
  $x, z \in \mathbb{R}^n$, for any $y \in \mathbb{R}^n$,
  \begin{equation*}
    \dist(x, z) \leq \dist(x, y) + \dist(y, z).
  \end{equation*}
\end{note}

\begin{defn}[Open Ball]\index{Open Ball}\label{defn:open_ball}
  Let $x \in \mathbb{R}^n$ and $\epsilon > 0$. The open ball of radius $\epsilon$ centered
  at $x$ is
  \begin{equation*}
    B_{\epsilon}(x) = \left\{ y \in \mathbb{R}^n \mmid \dist(x, y) < \epsilon \right\}.
  \end{equation*}
  A subset $U \subseteq \mathbb{R}^n$ is called \hldefn{open} if $\forall x \in U$,
  $\exists \epsilon > 0$ such that
  \begin{equation*}
    B_{\epsilon}(x) \subseteq U.
  \end{equation*}
\end{defn}

\begin{eg}
  \begin{itemize}
    \item $\emptyset$ and $\mathbb{R}^n$ are open.
    \item If $U$ and $V$ are open, so is $U \cap V$.
    \item If $\{ U_\alpha \}_{\alpha \in A}$ is open, so is $\bigcup_{\alpha \in A} U_\alpha$.
  \end{itemize}
\end{eg}

% section topology_on_r_n_ (end)

% chapter lecture_6_jan_18th (end)

\chapter{Lecture 7 Jan 21st}%
\label{chp:lecture_7_jan_21st}
% chapter lecture_7_jan_21st

\section{Topology on \texorpdfstring{$\mathbb{R}^n$}{Rn} (Continued)}%
\label{sec:topology_on_r_n_continued}
% section topology_on_r_n_continued

\begin{defn}[Closed]\index{Closed}\label{defn:closed}
  A subset $F \subseteq \mathbb{R}^n$ is \hlnoteb{closed} if its complement
  $\mathbb{R}^n \setminus F =: F^C$ is open.
\end{defn}

\begin{warning}
  A subset does not have to be either open or closed. Most subsets are neither.
\end{warning}

\begin{note}
  \begin{itemize}
    \item Arbitrary intersections of closed sets is closed.
    \item Finite unions of closed sets is closed.
  \end{itemize}
\end{note}

\begin{note}[Notation]
  We call
  \begin{equation*}
    \bar{B}_{\epsilon}(x) := \left\{ y \in \mathbb{R}^n \mmid \norm{ x - y } \leq \epsilon \right\}
  \end{equation*}
  the closed ball of radius $\epsilon$ centered at $x$.
\end{note}

\begin{defn}[Continuity]\index{Continuity}\label{defn:continuity}
  Let $A \subseteq \mathbb{R}^n$. Let $f : A \to \mathbb{R}^m$, and $x \in A$. We say
  that $f$ is \hlnoteb{continuous} at $x$ if $\forall \epsilon > 0$, $\exists \delta
  > 0$ such that
  \begin{equation*}
    f(B_{\delta}(x) \cap A) \subseteq B_{\epsilon}(f(x)).
  \end{equation*}
  We say that $f$ is \hlnoteb{continuous} on $A$ if $\forall x \in A$, $f$ is continuous
  on $x$.
\end{defn}

\begin{propo}[Inverse of a Continuous Map is Open]\label{propo:inverse_of_a_continuous_map_is_open}
  \marginnote{For a proof, see \href{https://tex.japorized.ink/PMATH351F18/classnotes.pdf}{PMATH 351}.}
  Let $A \subseteq \mathbb{R}^n$ and $f : A \to \mathbb{R}^m$. Then $f$ is continuous on
  $A$ iff whenever $V \subseteq \mathbb{R}^m$ is open, $f^{-1}(V) = A \cap U$ for some
  $U \subseteq \mathbb{R}^n$ is open.
\end{propo}

\begin{defn}[Homeomorphism]\index{Homeomorphism}\label{defn:homeomorphism}
  Let $A \subseteq \mathbb{R}^n$ and $f : A \to \mathbb{R}^m$. Let $B = f(A)$. We say that
  $f$ is a homeomorphism of $A$ onto $B$ if $f : A \to B$
  \begin{itemize}
    \item is a bijection;
    \item and $f^{-1} : B \to A$ is continuous on $A$ and $B$, respectively.
  \end{itemize}
\end{defn}

% section topology_on_r_n_continued (end)

\section{Calculus on \texorpdfstring{$\mathbb{R}^n$}{Rn}}%
\label{sec:calculus_on_r_n_}
% section calculus_on_r_n_

Let $U \subseteq \mathbb{R}^n$ be open, and $f : U \to \mathbb{R}^m$ be a
continuous map. Also, let
\begin{equation*}
  x = (x^1, \ldots, x^n) \in \mathbb{R}^n \text{ and }
  y = (y^1, \ldots, y^m) \in \mathbb{R}^m.
\end{equation*}
Then the \hldefn{component functions} of $f$ are defined by
\begin{equation*}
  y^k = f^k(x^1, \ldots, x^n), \text{ where }
  y = (y^1, \ldots, y^m) = f(x) = f(x^1, \ldots x^n).
\end{equation*}
Thus $f = (f^1, \ldots, f^m)$ is a collection of $m$-real-valued functions
on $U \subseteq \mathbb{R}^n$.

\begin{defn}[Smoothness]\index{Smoothness}\label{defn:smoothness}
  Let $x_0 \in U$. We say that $f$ is \hlnoteb{smooth} (or \hldefn{$C^\infty$},
  or \hldefn{infinitely differentiable}) if all \hlimpo{partial derivatives} of
  each component function $f^k$ exists and are \hlnotea{continuous} at $x_0$.
  I.e., if we let $\frac{\partial}{\partial x^i} = \partial_i$ denote the operator
  of partial differentiation in the $x^i$ direction, then
  \begin{equation*}
    \partial_1^{\alpha_1} \hdots \partial_n^{\alpha_n} f^k
  \end{equation*}
  exists and is continuous at $x_0$, for all $k = 1, \ldots, n$,
  and all $\alpha_i \geq 0$.
\end{defn}

\begin{defn}[Diffeomorphism]\index{Diffeomorphism}\label{defn:diffeomorphism}
  Let $U \subseteq \mathbb{R}^n$ be open, $f : U \to \mathbb{R}^m$, and $V = f(U)$.
  We say $f$ is a \hlnoteb{diffeomorphism} of $U$ onto $V$ if $f : U \to V$ is
  bijective\sidenote{A function that is \hlimpo{not injective} may not have a surjection
  from its image.}, smooth, and that its inverse $f^{-1}$ is smooth.

  We say that $U$ and $V$ are \hldefn{diffeomorphic} if such a diffeomorphism
  exists.
\end{defn}

\begin{note}
  A diffeomorphism preserves the `smoothness of a structure', i.e. the notion of
  calculus is the same for diffeomorphic spaces.
\end{note}

\begin{eg}
  If $f : U \to V$ is a diffeomorphism , then $g : V \to \mathbb{R}$ is smooth iff
  $g \circ f : U \to \mathbb{R}$ is smooth.
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \coordinate (u) at (0, 0);
      \coordinate (v) at (2, 0);
      \coordinate (r) at (4, 0);
      \node[below=2pt] at (u) {$U$};
      \node[below=2pt] at (v) {$V$};
      \node[below=2pt] at (r) {$\mathbb{R}$};
      \draw[-latex] (u) to [bend left=60] node[midway,above] {$g \circ f$} (r) ;
      \draw[-latex] (u) to [bend left] node[midway,above] {$f$} (v) ;
      \draw[-latex] (v) to [bend left] node[midway,above] {$g$} (r) ;
    \end{tikzpicture}
    \caption{Preservation of smoothness via diffeomorphisms}\label{fig:preservation_of_smoothness_via_diffeomorphisms}
  \end{marginfigure}
\end{eg}

\begin{note}
  A diffeomorphism is also called a \hldefn{smooth reparameterization} (or just a
  \hldefn{parameterization} for short).
\end{note}

\begin{defn}[Differential]\index{Differential}\label{defn:differential}
  Let $f : U \subseteq \mathbb{R}^n \to \mathbb{R}^m$ be a smooth mapping, and
  $x_0 \in U$. The \hlnoteb{differential} of $f$ at $x_0$, denoted $(\dif{f})_{x_0}$,
  is a linear map $(\D f)_{x_0} : \mathbb{R}^n \to \mathbb{R}^m$, or an
  $m \times n$ real matrix, given by
  \begin{equation*}
    (\D f)_{x_0} = \begin{pmatrix}
      \frac{\partial f^1}{\partial x^1} (x_0) & \hdots & \frac{\partial f^1}{\partial x^n} (x_0) \\
      \vdots                                  &        & \vdots \\
      \frac{\partial f^m}{\partial x^1} (x_0) & \hdots & \frac{\partial f^m}{\partial x^n} (x_0)
    \end{pmatrix},
  \end{equation*}
  \marginnote{
    \begin{mnote}[Change of notation]
      We changed the notation for the \hlnotea{differential} on Feb 3rd to using $\D f$.
      The old notation was $\dif{f}$.
    \end{mnote}
  }
  where the notation $(x_0)$ means evaluation at $x_0$, and the $(i, j)$
  \textsuperscript{th} entry of $(\D f)_{x_0}$ is
  $\frac{\partial f^i}{\partial x^j}(x_0)$. $(\D f)_{x_0}$ is also called
  the \hldefn{Jacobian} or \hldefn{tangent map} of $f$ at $x_0$.
\end{defn}

\begin{propo}[Differential of the Identity Map is the Identity Matrix]\label{propo:differential_of_the_identity_map_is_the_identity_matrix}
  Let $f : U \subseteq \mathbb{R}^n \to \mathbb{R}^n$ be the identity mapping
  $f(x) = x$. Then $(\D f)_{x_0} = I_n$, the $n \times n$ matrix, then for
  any $x_0 \in U$.
\end{propo}

\begin{proof}
  Since $f(x) = x$, since $x \in \mathbb{R}^n$, we may consider the function $f$ as
  \begin{equation*}
    f(x) = I_n x = \begin{pmatrix}
      x_1    & 0      & \hdots & 0 \\
      0      & x_2    & \hdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \hdots & x_n
    \end{pmatrix}.
  \end{equation*}
  Then it follows from differentiation that
  \begin{equation*}
    (\D f)_{x_0} = \begin{pmatrix}
      1      & 0      & \hdots & 0 \\
      0      & 1      & \hdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \hdots & 0
    \end{pmatrix},
  \end{equation*}
  and it does not matter what $x_0$ is.
\end{proof}

\begin{note}
  In multivariable calculus, we learned that if $f$ is \hlnotea{smooth} at $x_0$
  \sidenote{Back in multivariable calculus, just being $C^1$ at $x_0$ is sufficient
  for being smooth}, then
  \begin{equation*}
    \underset{m \times 1}{f(x)}
      = \underset{m \times 1}{f(x_0)}
        + \underset{m \times n}{(\D f)_{x_0}} \underset{n \times 1}{(x - x_0)}
        + \underset{m \times 1}{Q(x)},
  \end{equation*}
  where $Q : U \to \mathbb{R}^m$ satisfies
  \begin{equation*}
    \lim_{x \to x_0} \frac{Q(x)}{\norm{x - x_0}} = 0.
  \end{equation*}
\end{note}

\begin{note}
  Note that when $n = m = 1$, the existence of the differential of a continuous
  real-valued function $f(x)$ at a real number $x_0 \in U \subseteq \mathbb{R}$
  is the same of the usual derivative $f'(x)$ at $x = x_0$. In fact,
  $f'(x_0) = (\D f)_{x_0} = \frac{\dif{f}}{\dif{x}}(x_0)$.
\end{note}

\begin{thm}[The Chain Rule]\index{The Chain Rule}\label{thm:the_chain_rule}
  Let
  \begin{gather*}
    f : U \subseteq \mathbb{R}^n \to \mathbb{R}^m \\
    g : V \subseteq \mathbb{R}^m \to \mathbb{R}^p,
  \end{gather*}
  be two smooth maps, where $U, V$ are open in $\mathbb{R}^n$ and $\mathbb{R}^m$,
  respectively, and and such that $V = f(U)$. Then the composition $g \circ f$ is
  also smooth. Further, if $x_0 \in U$, then
  \begin{equation}\label{eq:the_chain_rule}
    (D (g \circ f))_{x_0} = (\D g)_{f(x_0)} (\D f)_{x_0}.
  \end{equation}
\end{thm}

% section calculus_on_r_n_ (end)

\section{Smooth Curves in \texorpdfstring{$\mathbb{R}^n$}{Rn} and Tangent Vectors}%
\label{sec:smooth_curves_in_r_n_and_tangent_vectors}
% section smooth_curves_in_r_n_and_tangent_vectors

We shall now look into tangent vectors and the tangent space at every point of
$\mathbb{R}^n$. We need these two notions to construct objects such as vector
fields and \hlnotea{differential forms}. In particular, we need to consider these
objects in multiple abstract ways so as to be able to generalize these notions in
more abstract spaces, particularly to \hlnotea{submanifolds} of $\mathbb{R}^n$
later on.

\paragraph{Plan} We shall first consider the notion of \hlnotea{smooth curves},
which we shall simply call a curve, and shall always (in this course) assume
curves as smooth objects. We shall then use \hlnotea{velocities} of curves to
define \hlnotea{tangent vectors}.

\begin{defn}[Smooth Curve]\index{Smooth Curve}\label{defn:smooth_curve}
  Let $I \subseteq \mathbb{R}$ be an open interval. A smooth map
  $\phi : I \to \mathbb{R}^n$ is called a \hlnoteb{smooth curve}, or \hlnoteb{curve},
  in $\mathbb{R}^n$. Let $t \in I$. Then each of its component functions
  $\phi^k(t)$ in $\phi(t) = (\phi^1(t), \ldots, \phi^n(t))$ is a smooth
  real-valued function of $t$.
\end{defn}
\begin{marginfigure}
  \centering
  \resizebox{\marginparwidth}{!}{
  \begin{tikzpicture}
    \begin{axis}
      \addplot3[variable=t,mesh,domain=-1:1] (t^3,t^3,t);
    \end{axis}
  \end{tikzpicture}
  }
  \caption{A curve in $\mathbb{R}^3$}\label{fig:a_curve_in_r_3_}
\end{marginfigure}

\begin{eg}
  Let $a, b > 0$. Consider $\phi : I \to \mathbb{R}^3$ given by
  \begin{equation*}
    \phi(t) = (a \cos t, a \sin t, bt).
  \end{equation*}
  Since each of the components are smooth\sidenote{\hlwarn{Wait}, do we actually
  consider $bt$ smooth when it's only $C^1$, in this course?}, we have that $\phi$
  itself is also smooth. The shape of the curve is as shown in \cref{fig:helix_curve}.
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \draw[color=be-blue,thick,decoration={segment length=15,
        amplitude=28,coil},decorate,arrows = {<[bend]-}] (0,1.8,0) --(0,-2,0);
      \draw[-latex] (-2, 0, 0) -- (2, 0, 0) node[right] {$y$};
      \draw[-latex] (0, -2, 0) -- (0, 2, 0) node[above] {$z$};
      \draw[-latex] (0, 0, -2) -- (0, 0, 3) node[below,left] {$x$};
      \draw[dashed] (1, -2, 0) -- (1, 2, 0);
      \draw[dashed] (-1, -2, 0) -- (-1, 2, 0);
    \end{tikzpicture}
    \caption{Helix curve}\label{fig:helix_curve}
  \end{marginfigure}
\end{eg}

% section smooth_curves_in_r_n_and_tangent_vectors (end)

% chapter lecture_7_jan_21st (end)

\chapter{Lecture 8 Jan 23rd}%
\label{chp:lecture_8_jan_23rd}
% chapter lecture_8_jan_23rd

\section{Smooth Curves in \texorpdfstring{$\mathbb{R}^n$}{Rn} and Tangent Vectors (Continued)}%
\label{sec:smooth_curves_in_r_n_and_tangent_vectors_continued}
% section smooth_curves_in_r_n_and_tangent_vectors_continued

\begin{defn}[Velocity]\index{Velocity}\label{defn:velocity}
  Let $\phi : I \to \mathbb{R}^n$ be a curve. The \hlnoteb{velocity} of the curve
  $\phi$ at the point $\phi(t_0) \in \mathbb{R}^n$ for $t_0 \in I$ is defined as
  \begin{equation*}
    \phi'(t_0) = (\dif{\phi})_{t_0} \in \mathbb{R}^{n \times 1} \simeq \mathbb{R}^n.
  \end{equation*}
\end{defn}

\begin{note}
  $\phi'(t_0) = (\dif{\phi})_{t_0}$ is the \hlnotea{instantaneous rate of change}
  of $\phi$ at the point $\phi(t_0) \in \mathbb{R}^n$.
\end{note}

\begin{eg}
  From the last example, we had $\phi(t) = (a \cos t, a \sin t, bt)$ for $a, b > 0$.
  Then
  \begin{equation*}
    \phi'(t) = (-a \sin t, a \cos t, b)
  \end{equation*}
  Let $t_0 = \frac{\pi}{2}$. Then the velocity of $\phi$ at
  \begin{equation*}
    \phi \left( \frac{\pi}{2} \right) = (0, a, \frac{b \pi}{2})
  \end{equation*}
  is
  \begin{equation*}
    \phi'\left(\frac{\pi}{2}\right) = (-a, 0, b).
  \end{equation*}
\end{eg}

\begin{defn}[Equivalent Curves]\index{Equivalent Curves}\label{defn:equivalent_curves}
  Let $p \in \mathbb{R}^n$. Let $\phi : I \to \mathbb{R}^n$ and
  $\psi : \tilde{I} \to \mathbb{R}^n$ be two smooth curves in $\mathbb{R}^n$ such
  that both the open intervals $I$ and $\tilde{I}$ contain $0$. We say that
  $\phi$ is \hlnoteb{equivalent at $p$ to} $\psi$, and denote this as
  \begin{equation*}
    \phi \sim_{p} \psi,
  \end{equation*}
  iff
  \begin{itemize}
    \item $\phi(0) = \psi(0) = p$, and
    \item $\phi'(0) = \psi'(0)$.
  \end{itemize}
\end{defn}

\begin{note}
  In other words, $\phi \sim_p \psi$ iff both $\phi$ and $\psi$ passes through
  $p$ at $t = 0$, and have the same velocity at this point.
\end{note}

\begin{eg}\label{eg:simple_equivalent_curves}
  Consider the two curves
  \begin{equation*}
    \phi(t) = (\cos t, \sin t) \text{ and } \psi(t) = (1, t),
  \end{equation*}
  where $t \in \mathbb{R}$.
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \draw[-latex] (-2, 0) -- (2, 0);
      \draw[-latex] (0, -2) -- (0, 2);
      \draw[
        decoration={markings, mark=at position 0.625 with {\arrow{latex}}},
        postaction={decorate}
        ] (0, 0) circle (1);
      \draw[midarrow={latex}{0.7}] (1, -2) -- (1, 2);
      \node[draw,circle,fill,inner sep=1pt,label={315:{$p = (1, 0)$}}] at (1, 0) {};
    \end{tikzpicture}
    \caption{Simple example of equivalent curves in \cref{eg:simple_equivalent_curves}}\label{fig:simple_example_of_equivalent_curves}
  \end{marginfigure}
  Notice that at $p = (1, 0)$, i.e. $t = 0$, we have
  \begin{equation*}
    \phi'(0) = (0, 1) \text{ and } \psi'(0) = (0, 1).
  \end{equation*}
  Thus
  \begin{equation*}
    \phi \sim_p \psi.
  \end{equation*}
\end{eg}

\begin{propo}[Equivalent Curves as an Equivalence Relation]\label{propo:equivalent_curves_as_an_equivalence_relation}
  $\sim_p$ is an equivalence relation.
\end{propo}

\begin{ex}
  Proof of \cref{propo:equivalent_curves_as_an_equivalence_relation} is really
  straightforward so try it yourself.
\end{ex}

\begin{defn}[Tangent Vector]\index{Tangent Vector}\label{defn:tangent_vector}
  A \hlnoteb{tangent vector} to $\mathbb{R}^n$ at $p$ is a vector $v \in \mathbb{R}^n$,
  thought of as `\hlnotec{emanating}' from $p$, is in a one-to-one correspondence
  with an equivalence class
  \begin{equation*}
    [\phi]_p := \left\{ \psi : I \to \mathbb{R}^n \mmid \psi \sim_p \phi \right\}.
  \end{equation*}
\end{defn}

\begin{defn}[Tangent Space]\index{Tangent Space}\label{defn:tangent_space}
  The \hlnoteb{tangent space} to $\mathbb{R}^n$ at $p$, denoted
  $T_p \left(\mathbb{R}^n\right)$ is the set of all equivalence classes $[\phi]_p$
  wrt $\sim_p$.
\end{defn}

Now if $\phi : I \to \mathbb{R}^n$ is a smooth curve in $\mathbb{R}^n$ with $0 \in I$,
and $\phi'(0) = v \in \mathbb{R}^n$, then we write $v_p$ to denote the element in
$T_p \left( \mathbb{R}^n \right)$ that it represents.

\begin{propo}[Canonical Bijection from $T_p (\mathbb{R}^n)$ to $\mathbb{R}^n$]\label{propo:canonical_bijection_from_t_p_r_n_to_r_n_}
  There exists a canonical bijection from $T_p(\mathbb{R}^n)$ to $\mathbb{R}^n$.
  Using this bijection, we can equip the tangent space $T_p(\mathbb{R}^n)$ with the
  structure of a real $n$-dimensional real vector space.
\end{propo}

\begin{proof}
  Let $v_p = [\phi]_p \in T_p(\mathbb{R}^n)$, where $v = \phi'(0) \in \mathbb{R}^n$,
  for any $\phi \in [\phi]_p$. Let $\gamma_{v_p} : \mathbb{R} \to \mathbb{R}^n$ by
  \begin{equation*}
    \gamma_{v_p} (t) = (p + tv) = (p^1 + tv^1, p^2 + tv^2, \ldots, p^n + tv^n).
  \end{equation*}
  It follows by construction that $\gamma_{v_p}$ is smooth, $\gamma_{v_p}(0) = p$,
  and $\gamma_{v_p}'(0) = v$. Thus $\gamma_{v_p} \sim_p \phi$. In particular, we have
  $[\gamma_{v_p}]_p = [\phi]_p = v_p \in T_p (\mathbb{R}^n)$. In fact, notice that
  $\gamma_{v_p}$ is the straight line through $p$ in the direction of $v$.

  Now consider the map $T_p : \mathbb{R}^n \to T_p (\mathbb{R}^n)$, given by
  \begin{equation*}
    T_p(v) = [\gamma_{v_p}]_p.
  \end{equation*}
  In other words, we defined the map $T_p$ to send a vector $v \in \mathbb{R}^n$ to
  the \hlnotea{equivalence class of all smooth curves passing through $p$ with
  velocity $v$ at $p$}. Note that since $\gamma_{v_p}$ has a `dependency' on $v$,
  it follows that $T_p$ is indeed a bijection.

  We now get a vector space structure on $T_p (\mathbb{R}^n)$ from that of $\mathbb{R}^n$
  by letting $T_p$ be a linear isomorphism, i.e. we set
  \begin{equation*}
    a[\phi]_p + b[\psi]_p = T_p \left( a T_p^{-1} ([\phi]_p) + bT_p^{-1} ([\psi]_p) \right)
  \end{equation*}
  for all $a, b \in \mathbb{R}$ and all $[\phi]_p, [\psi]_p \in T_p(\mathbb{R}^n)$.\qed
\end{proof}

\begin{note}
  Another way we can say the last line in the proof above is as follows: if
  $v_p, w_p \in T_p(\mathbb{R}^n)$ and $a, b \in \mathbb{R}$, then we define
  $a v_p + b w_p = (av + bw)_p$.

  In other words, looking at the tangent vectors at $p$ is similar to looking
  at the tangents vectors at the origin $0$.
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \draw[-latex] (0, 0, 0) -- (2, 0, 0) node[right] {$y$};
      \draw[-latex] (0, 0, 0) -- (0, 4, 0) node[above] {$z$};
      \draw[-latex] (0, 0, 0) -- (0, 0, 2) node[below,left] {$x$};

      \draw[-latex] (0, 0, 0) -- (1, 1, -1) node[above] {$w$};
      \draw[-latex] (0, 0, 0) -- (-1, 1, 1) node[above] {$v$};
      \node[draw,circle,fill,inner sep=1pt,label={315:{$0$}}] at (0, 0, 0) {};

      \draw[-latex] (1, 2, 1) -- (2, 3, 0) node[above] {$w_p$};
      \draw[-latex] (1, 2, 1) -- (0, 3, 2) node[above] {$v_p$};
      \node[draw,circle,fill,inner sep=1pt,label={315:{$p$}}] at (1, 2, 1) {};
    \end{tikzpicture}
    \caption{Canonical bijection from $T_p(\mathbb{R}^n)$ to $\mathbb{R}^n$}\label{fig:canonical_bijection_from_t_p_r_n_to_r_n_}
  \end{marginfigure}
\end{note}

\begin{note}
  The fact that there is a canonical isomorphism between $\mathbb{R}^n$ and the
  equivalence classes wrt $\sim_p$ is a pheonomenon that is particular to $\mathbb{R}^n$.

  For a $k$-dimensional \hlnoteb{submanifold} $M$ of $\mathbb{R}^n$, or more generally,
  for an abstract smooth $k$-dimensional manifold $M$, and a point $p \in M$, it is
  true that we can still define $T_p(M)$ to be the set of equivalence classes of
  curves wrt to some `natural' equivalence relation. However, there is no canonical
  representation of each equivalence class, and so $T_p(M) \simeq \mathbb{R}^k$, but
  not canonically so.
\end{note}

% section smooth_curves_in_r_n_and_tangent_vectors_continued (end)

% chapter lecture_8_jan_23rd (end)

\chapter{Lecture 9 Jan 25th}%
\label{chp:lecture_9_jan_25th}
% chapter lecture_9_jan_25th

\section{Derivations and Tangent Vectors}%
\label{sec:derivations_and_tangent_vectors}
% section derivations_and_tangent_vectors

Recall the notion of a \hlnotea{directional derivative}.

\begin{defn}[Directional Derivative]\index{Directional Derivative}\label{defn:directional_derivative}
  Let $p, v \in \mathbb{R}^n$. Let $f : U \subseteq \mathbb{R}^n \to \mathbb{R}$ be
  smooth, where $U$ is an open set that contains $p$ (i.e. an open nbd of $p$). The
  \hlnoteb{directional derivative} of $f$ at $p$ in the direction of $v$, denoted
  $v_p f$, is defined as
  \begin{equation}\label{eq:directional_derivative_defn}
    v_p f = \lim_{t \to 0} \frac{f(p + tv) - f(p)}{t}.
  \end{equation}
\end{defn}

\begin{remark}
  The above limit may or may not exist given an arbitrary $f, p$ and $v$. However,
  since we're working exclusively with smooth functions, this limit will always
  exist for us.
\end{remark}

\begin{note}
  By definition, we may think of $v_p f \in \mathbb{R}$ as the instantaneous rate of
  change of $f$ at the point $p$ as we `move in the direction of' the vector $v$.
\end{note}

\begin{remark}
  In multivariable calculus, one may have seen this definition with the additional
  condition that $v$ is a unit vector. We do not have that restriction here.

  Also, note that we have deliberately used the same notation $v_p$ that we used
  for elements of $T_p(\mathbb{R}^n)$, which seems awkward, but it shall be clarified
  in \cref{crly:justification_for_the_notation_v_p_f_}.
\end{remark}

\begin{eg}\label{eg:directional_derivative_on_basis_vectors}
  In the special case of when $v = \hat{e}_i$, where $\hat{e}_i$ is the $i$th
  standard basis vector. Then we have
  \begin{equation*}
    (\hat{e}_i)_p f = \lim_{t \to 0} \frac{f(p + t\hat{e}_i) - f(p)}{t}
      = \frac{\partial f}{\partial x^i}(p) = (f \circ \gamma_{v_p})'(p)
  \end{equation*}
  for the directional derivative of $f$ at $p$ in the $\hat{e}_i$ direction. This
  is precisely the partial derivative of $f$ in the $x^i$ direction at the point
  $p \in \mathbb{R}^n$.
\end{eg}

\begin{thm}[Linearity and Leibniz Rule for Directional Derivatives]\index{Linearity of Directional Derivatives}\index{Leibniz Rule for Directional Derivatives}\label{thm:linearity_and_leibniz_rule_for_directional_derivatives}
  Let $p \in \mathbb{R}^n$, and let $f, g$ be smooth real-valued functions defined
  on open neighbourhoods of $p$. Let $a, b \in \mathbb{R}$. Then
  \begin{enumerate}
    \item (\hlnotea{Linearity}) $v_p(af + bg) = av_p f + bv_p g$;
    \item (\hlnotea{Leibniz Rule} / \hlnotea{Product Rule})
      $v_p(fg) = f(p) v_p g + g(p) v_p f$.
  \end{enumerate}
\end{thm}

\begin{proof}
  Proven on A2Q2.
\end{proof}

\newthought{Recall} that given $p, v \in \mathbb{R}^n$, we denote $\gamma_{v_p}$ as
the curve $\gamma_{v_p}(t) = p + tv$, which is the straight line passing through $p$
with constant velocity $v$. Thus we mmay rewrite \cref{eq:directional_derivative_defn}
as
\begin{equation}\label{eq:directional_derivative_defn_ver2}
  v_p f = \lim_{t \to 0} \frac{f(\gamma_{v_p}(t)) - f(\gamma_{v_p}(0))}{t}
    = (f \circ \gamma_{v_p})'(0),
\end{equation}
where $f \circ \gamma_{v_p} : \mathbb{R} \to \mathbb{R}$ is smooth as it is a
composition of smooth functions.

\begin{thm}[Canonical Directional Derivative, Free From the Curve]\label{thm:canonical_directional_derivative_free_from_the_curve}
  Suppose that $\phi \sim_p \psi$ are two curves on $\mathbb{R}^n$. Let $f : U \to
  \mathbb{R}$ where $U$ is an open neighbourhood of $p$. Then
  \begin{equation*}
    (f \circ \phi)'(0) = (f \circ \psi)'(0).
  \end{equation*}
\end{thm}

\begin{proof}
  By the chain rule,
  \begin{equation*}
    (f \circ \phi)'(0) = (\D (f \circ \phi))_0 = (\D f)_{\phi(0)}(\D \phi)_0
      = (\D f)_{\phi(0)} \phi'(0),
  \end{equation*}
  and a similar expression holds for $\psi$. Our desired result follows from the
  definition of $\sim_p$.
\end{proof}

\begin{crly}[Justification for the Notation $v_p f$]\label{crly:justification_for_the_notation_v_p_f_}
  Let $[\phi]_p \in T_p \mathbb{R}^n$. It follows that
  \begin{equation*}
    v_p f = (f \circ \gamma_{v_p})'(0) = (f \circ \phi)'(0)
  \end{equation*}
  by \cref{eq:directional_derivative_defn_ver2}.
\end{crly}

\begin{remark}
  With that, we have established that tangent vectors give us directional derivatives
  in a way compatible with the characterization of $T_p \mathbb{R}^n$ as equivalence
  classes wrt $\sim_p$.
\end{remark}

\newthought{Now the} fact that \cref{eq:directional_derivative_defn} depends only on
the values of $f$ in some open neighbourhood of $p$ motivates us towards the following
definition.

\begin{defn}[$f \sim_p g$]\index{$f \sim_p g$}\label{defn:_f_sim_p_g_}
  Let $p \in \mathbb{R}^n$. Let $f : U \subseteq \mathbb{R}^n \to \mathbb{R}$ and
  $g : V \subseteq \mathbb{R}^n \to \mathbb{R}$ be smooth where $U$ and $V$ are
  both open neighbourhoods of $p$. We say that \hlnoteb{$f \sim_p g$} if $\exists W
  \subseteq U \cap V$ such that $f \restriction_W = g \restriction_W$. That is,
  $f \sim_p g$ iff $f$ and $g$ agree at all points sufficiently closde to $p$.
\end{defn}

\begin{note}
  It is clear from \cref{eq:directional_derivative_defn} that if $f \sim_p g$, then
  $f(p) = g(p)$ and $v_p f = v_p g$, i.e. $f$ and $g$ agree at $p$ and all possible
  directional derivatives at $p$ of $f$ and $g$ also agree with each other.
\end{note}

\begin{propo}[$\sim_p$ for Smooth Functions is an Equivalence Relation]\label{propo:_sim_p_for_smooth_functions_is_an_equivalence_relation}
  The relation $\sim_p$ on the set of smooth real-valued functions defined on some
  open neighbourhood of $p$ is an equivalence relation.
\end{propo}

\begin{ex}
  Prove \cref{propo:_sim_p_for_smooth_functions_is_an_equivalence_relation}.
\end{ex}

Of course, what else is there to talk about an equivalence relation if not for its
equivalence class?

\begin{defn}[Germ of Functions]\index{Germ of Functions}\label{defn:germ_of_functions}
  An equivalence class of $\sim_p$ is called a \hlnoteb{germ of functions} at $p$.
  The set of all such equivalence classes is dentoed $C_p^{\infty}$, called the
  \hldefn{space of germs} at $p$.
\end{defn}

\begin{note}
  Suppose $f : U \to \mathbb{R}$, where $U$ is an open neighbourhood of $p$. Then it
  is clear that $[f]_p = [f \restriction_V]_p$ for any open neighbourhood $V$ of $p$
  if $V \subseteq U$.
\end{note}

We can define the structure of a real vector space on $C_p^{\infty}$ as follows. Let
$[f]_p, [g]_p \in C_p^{\infty}$, where the functions
\begin{equation*}
  f : U \to \mathbb{R} \text{ and } g : V \to \mathbb{R}
\end{equation*}
represent $[f]_p$ and $[g]_p$, respectively. Also, let $a, b \in \mathbb{R}$. Then we
define
\begin{equation}\label{eq:structure_of_c_p_infty}
  a[f]_p + b[g]_p = [af + bg]_p,
\end{equation}
where $af + bg$ is restricted to the open neighbourhood $U \cap V$ of $p$ on which
both $f$ and $g$ are defined.

We need to show that \cref{eq:structure_of_c_p_infty} is well-defined. Well suppose
$f \sim_p \tilde{f}$ and $g \sim_p \tilde{g}$. Then what we need to show is
\begin{equation*}
  (af + bg) \sim_p (a\tilde{f} + b\tilde{g}).
\end{equation*}
Since $f \sim_p \tilde{f}$ and $g \sim_p \tilde{g}$, we have that
\begin{equation*}
  \tilde{f} : \tilde{U} \to \mathbb{R} \text{ and } \tilde{g} : \tilde{V} \to \mathbb{R}.
\end{equation*}
Then, in particular, there exists $W \subseteq U \cap \tilde{U}$ and $Y \subseteq V
\cap \tilde{V}$ such that
\begin{equation*}
  f \restriction_W = \tilde{f} \restriction_W \text{ and }
  g \restriction_Y = \tilde{g} \restriction_Y.
\end{equation*}
Then $Z = W \cap Y$ is an open neighbourhood of $p$ and thus we must have
\begin{equation*}
  af + bg = a\tilde{f} + b \tilde{g}
\end{equation*}
on $Z$. Thus \cref{eq:structure_of_c_p_infty} is true and $C_p^{\infty}$ is indeed
a vector space.

Further, we can even define a \hlnotea{multiplication} on $C_p^{\infty}$ by setting
\begin{equation}\label{eq:structure_of_c_p_infty_multiplication}
  [f]_p [g]_p = [fg]_p.
\end{equation}

\begin{eg}
  Check that \cref{eq:structure_of_c_p_infty_multiplication} is well-defined.
\end{eg}

\begin{propo}[Linearity of the Directional Derivative over the Germs of Functions]\label{propo:linearity_of_the_directional_derivative_over_the_germs_of_functions}
  Let $v_p \in T_p \mathbb{R}^n$. Then the map $v_p : C_p^{\infty} \to \mathbb{R}$
  defined by $[f]_p \mapsto v_p [f]_p = v_p f$ is well-defined. This map is also
  linear in the sense that
  \begin{equation*}
    v_p(a[f]_p + b[g]_p) = a v_p [f]_p + b v_p [g]_p.
  \end{equation*}
  Moreover, this map satisfies Leibniz's rule:
  \begin{equation*}
    v_p([f]_p [g]_p) = f(p) v-p [g]_p + g(p) v_p [f]_p.
  \end{equation*}
\end{propo}

\begin{proof}
  Our desired result follows almost immedaitely from \cref{defn:_f_sim_p_g_} and
  \cref{thm:linearity_and_leibniz_rule_for_directional_derivatives}.
\end{proof}

% section derivations_and_tangent_vectors (end)

% chapter lecture_9_jan_25th (end)

\chapter{Lecture 10 Jan 28th}%
\label{chp:lecture_10_jan_28th}
% chapter lecture_10_jan_28th

\section{Derivations and Tangent Vectors (Continued)}%
\label{sec:derivations_and_tangent_vectors_continued}
% section derivations_and_tangent_vectors_continued

Recall \cref{crly:justification_for_the_notation_v_p_f_}.

\begin{defn}[Derivation]\index{Derivation}\label{defn:derivation}
  A \hlnoteb{derivation} at $p$ is a linear map $\mathcal{D} : C_p^\infty \to
  \mathbb{R}$ satisfying the additional property that
  \begin{equation*}
    \mathcal{D}([f]_p [g]_p) = f(p) \mathcal{D}[g]_p + g(p) \mathcal{D}[f]_p.
  \end{equation*}
\end{defn}

\begin{remark}
  \cref{propo:linearity_of_the_directional_derivative_over_the_germs_of_functions}
  tells us that any tangent vector $v_p \in T_p \mathbb{R}^n$ is a derivation, so
  the set of derivations is not trivial.
\end{remark}

\begin{propo}[Set of Derivations as a Space]\label{propo:set_of_derivations_as_a_space}
  Let $\Der_p$ be the set of all derivations at $p$. Then this is a subset of the
  vector space $L(C_p^\infty, \mathbb{R})$. In fact, $\Der_p$ is a linear subspace.
\end{propo}

\begin{proof}
  We shall prove this in A2Q3.
\end{proof}

\newthought{This} is likely surprising seeing that we just introduced yet another
definition but there are actually no other derivations at $p$ aside from the tangent
vectors at $p$. In fact, any derivation must be a directional differentiation wrt to
some tangent vector $v_p \in T_p \mathbb{R}^n$. Before we can show this, observe the
following.

\hlbnotea{First} Let us describe a tangent vector $v_p$ as a derivation at $p$ in
terms of the standard basis. Let $\mathcal{B} = \{ \hat{e}_1, \ldots, \hat{e}_n \}$
be the standard basis of $\mathbb{R}^n$. Then
\begin{equation*}
  \{ (\hat{e}_1)_p, \ldots, (\hat{e}_n)_p \}
\end{equation*}
is a basis of $T_p \mathbb{R}^n$, which is called the standard basis of
$T_p \mathbb{R}^n$.  It is the image of $\mathcal{B}$ under the
\hyperref[propo:canonical_bijection_from_t_p_r_n_to_r_n_]{canonical isomorphism}
\begin{equation*}
  T_p : \mathbb{R}^n \to T_p \mathbb{R}^n.
\end{equation*}
Recall from \cref{eg:directional_derivative_on_basis_vectors} that
\begin{equation*}
  (\hat{e}_k)_p f = \frac{\partial f}{\partial x^k}(p).
\end{equation*}
As a linear map, we can write
\begin{equation}\label{eq:directional_derivative_on_basis_vectors_linear}
  (\hat{e}_k)_p = \frac{\partial}{\partial x^k} \at{p}{}.
\end{equation}

Let $v \in \mathbb{R}^n$ be expressed as $v = v^i \hat{e}_i$, in terms of the
standard basis. By the chain rule, we have
\begin{align*}
  v_p f &= (f \circ \gamma_{v_p})'(0) = (\D f)_{\gamma_{v_p}(0)} (\D v_p)_0 \\
    &= (\dif{f})_p v = \frac{\partial f}{\partial x^i}(p) v^i
    = v^i \frac{\partial}{\partial x^i}\at{p}{} f.
\end{align*}
From \cref{eq:directional_derivative_on_basis_vectors_linear}, we can write the
above as
\begin{equation*}
  v_p = v^i (\hat{e}_i)_p,
\end{equation*}
which we see is indeed the image of $v = v^i \hat{e}_i$ under the linear isomorphism
$T_p$.
Henceforth, we will often express tangent vectors at $p$ in the above form, using
linear combinations of the operators $(\hat{e}_i)_p = \frac{\partial}{\partial x^i} \at{p}{}$.

\hlbnotea{Second} Consider the smooth function $x^j : \mathbb{R}^n \to \mathbb{R}$
given by
\begin{equation*}
  x^j(q) = q^j,
\end{equation*}
for all $q = (q^1, \ldots, q^n) \in \mathbb{R}^n$. So as a function of $x^1, \ldots, x^n$ we have
\begin{equation}\label{eq:coefficients_as_smooth_functions}
  x^j(x^1, \ldots, x^n) = x^j,
\end{equation}
which is smooth. Let $v_p = v^i \frac{\partial}{\partial x^i} \at{p}{}$. Then
\begin{equation*}
  v_p x^j = v^i \frac{\partial}{\partial x^i} \at{p}{} x^j = v^i \delta_i^j = v^j.
\end{equation*}
Thus, we deduced that
\begin{equation}\label{eq:tangent_vector_as_a_partial_derivative_operator}
  v_p = v^i \frac{\partial}{\partial x^i} \at{p}{}, \text{ where } v^i = v_p x^i.
\end{equation}

\begin{remark}
  Compare \cref{eq:tangent_vector_as_a_partial_derivative_operator} and
  \cref{eq:v_wrt_basis} and notice the similarity of their $v^i$'s. We shall look into why this
  is the case later on. % TODO : provide anchor to where this is explained
\end{remark}

\begin{lemma}[Derivations Annihilates Constant Functions]\label{lemma:derivations_annihilates_constant_functions}
  Let $\mathcal{D}_p$ be a derivation at $p$. Then $\mathcal{D}$ annihilates constant functions,
  i.e. if $f(q) = c \in \mathbb{R}$ for all $q \in \mathbb{R}^n$, then $\mathcal{D}_p f = 0$.
\end{lemma}

\begin{proof}
  First, consider the constant function $1 : \mathbb{R}^n \to \mathbb{R}$ given by $q \mapsto 1$.
  Note that $1 \cdot 1 = 1$. By Leibniz's Rule, we have
  \begin{equation*}
    \mathcal{D}_p(1) = \mathcal{D}_p(1 \cdot 1) = 1(p) \mathcal{D}_p 1 + 1(p) \mathcal{D}_p 1
      = 2 \mathcal{D}_p (1).
  \end{equation*}
  It follows that $\mathcal{D}_p (1) = 0$.

  Now let $f$ be a constant function. Then $f = c1$ for some $c \in \mathbb{R}$. It follows by
  linearity that
  \begin{equation*}
    \mathcal{D}_p f = \mathcal{D}_p (c1) = c \mathcal{D}_p 1 = 0.
  \end{equation*}
\end{proof}

\begin{thm}[Derivations are Tangent Vectors]\label{thm:derivations_are_tangent_vectors}
  Let $\mathcal{D}_p$ be a derivation at $p$. Then $\mathcal{D}_p = v_p$ for some
  $v_p \in T_p \mathbb{R}^n$. Consequently, $\Der_p = T_p \mathbb{R}^n$.
\end{thm}

\begin{proof}
  Note that if there exists a $v_p$ such that $\mathcal{D}_p = v_p$, then we must have
  $v_p = v^i \frac{\partial}{\partial x^i} \at{p}{}$ with coefficients
  \begin{equation*}
    v^i = v_p x^j = \mathcal{D}_p x^j.
  \end{equation*}
  In particular, we can show that
  \begin{equation*}
    \mathcal{D}_p = (\mathcal{D}_p x^i) \frac{\partial}{\partial x^i} \at{p}{}.
  \end{equation*}

  Let $f$ be a smooth function defined in an open neighbourhood of $p$. By the \hlnotea{
  integral form of Taylor's Theorem}, for $x = (x^1, \ldots, x^n)$ sufficiently close to $p$,
  we can write
  \begin{equation*}
    f(x) = f(p) + \frac{\partial f}{\partial x^i} \at{p} (x^i - p^i) + g_i(x) (x^i - p^i),
  \end{equation*}
  where the functions $g_i(x)$ satisfy $g_i(p) = 0$. More succinctly,
  \begin{equation}\label{eq:derivations_are_tangent_vectors_eq1}
    f = f(p) + \frac{\partial f}{\partial x^i} \at{p}{} (x^i - p^i) + g_i \cdot (x^i - p^i),
  \end{equation}
  where $x^i$ is the function $x^i(x) = x^i$ as in \cref{eq:coefficients_as_smooth_functions},
  and $p^i$ and $f(p)$ are constant functions. Apply $\mathcal{D}_p$ to
  \cref{eq:derivations_are_tangent_vectors_eq1}. By the linearity and Leibniz's rule, both of
  which are satisfied by $\mathcal{D}_p$, and
  \cref{lemma:derivations_annihilates_constant_functions}, we get
  \begin{align*}
    \mathcal{D}_p f
      &= \mathcal{D}_p \left( f(p) + \frac{\partial f}{\partial x^i} \at{p}{} (x^i - p^i) + g_i \cdot (x^i - p^i) \right) \\
      &= 0 + \frac{\partial f}{\partial x^i} \at{p}{} \mathcal{D}_p(x^i - p^i)
        + \mathcal{D}_p (g_i \cdot (x^i - p^i)) \\
      &= \frac{\partial f}{\partial x^i} \at{p}{} (\mathcal{D}_p x^i + 0)
        + g_i(p) \mathcal{D}_p (x^i - p^i) + (x^i - p^i)(p) \mathcal{D}_p (g_i) \\
      &= (\mathcal{D}_p x^i) \frac{\partial}{\partial x^i} \at{p}{} f + 0 + 0
      = \left( (\mathcal{D}_p x^i) \frac{\partial}{\partial x^i} \at{p}{} \right) f.
  \end{align*}
  Since $f$ was arbitrary, it follows that
  $\mathcal{D}_p = (\mathcal{D}_p x^i) \frac{\partial}{\partial x^i} \at{p}{}$,
  which is what we desired.
\end{proof}

\begin{remark}
  From \cref{sec:smooth_curves_in_r_n_and_tangent_vectors} and
  \cref{sec:derivations_and_tangent_vectors}, a tangent vector $v_p \in T_p \mathbb{R}^n$ can
  be considered in any one of the following three ways:
  \begin{enumerate}
    \item as a vector $v \in \mathbb{R}^n$, enamating from the point $p \in \mathbb{R}^n$;
    \item as a unique equivalence class of curves through $p$;
    \item as a unique derivation at $p$.
  \end{enumerate}
  The three different viewpoints are useful in their own ways, and we will be alternating
  between these ideas as we go forward.
\end{remark}

% section derivations_and_tangent_vectors_continued (end)

\section{Smooth Vector Fields}%
\label{sec:smooth_vector_fields}
% section smooth_vector_fields

The idea of a vector field on $\mathbb{R}^n$ is the assignment of a tangent vector at $p$ for
every $p \in \mathbb{R}^n$. A smooth vector field is where we attach these tangent vectors to
every point in a \hlnotea{smoothly varying way}.

\begin{defn}[Tangent Bundle]\index{Tangent Bundle}\label{defn:tangent_bundle}
  The \hlnoteb{tangent bundle} of $\mathbb{R}^n$ is defined as
  \begin{equation*}
    T \mathbb{R}^n = \bigcup_{p \in \mathbb{R}^n} T_p \mathbb{R}^n.
  \end{equation*}
\end{defn}

\begin{remark}
  For us, the tangent bundle is just a set, but it is a very important mathematical object
  which shall be studied in later courses (PMATH 465).
\end{remark}

\begin{defn}[Vector Field]\index{Vector Field}\label{defn:vector_field}
  A \hlnoteb{vector field} on $\mathbb{R}^n$ is a map $X : \mathbb{R}^n \to T \mathbb{R}^n$
  such that $X(p) \in T_p \mathbb{R}^n$ for all $p \in \mathbb{R}^n$. We shall always denote
  $X(p)$ by $X_p$.
\end{defn}

\newthought{Let} $\{ \hat{e}_1, \ldots, \hat{e}_n \}$ be the standard basis of $\mathbb{R}^n$.
We have seen that $\{ (\hat{e}_1)_p, \ldots, (\hat{e}_n)_p \}$ is a basis of $T_p \mathbb{R}^n$.
We can think of each $\hat{e}_i$ as a vector field, where $\hat{e}_i(p) = (\hat{e}_i)_p$. We
call these the \hldefn{standard vector fields} on $\mathbb{R}^n$. Recall that we wrote that
\begin{equation}\label{eq:standard_basis_for_standard_vector_fields}
  (\hat{e}_k) = \frac{\partial}{\partial x^k},
\end{equation}
which means that $(\hat{e}_k)_p = \frac{\partial}{\partial x^k} \at{p}{}$. Henceforth, we shall
write the standard vector fields on $\mathbb{R}^n$ as
$\left\{ \frac{\partial}{\partial x^1}, \ldots, \frac{\partial}{\partial x^n} \right\}$.

Now it follows that for any vector field $X$ on $\mathbb{R}^n$, since $X_p \in T_p \mathbb{R}^n$,
we can write
\begin{equation*}
  X_p = X^i (p) \frac{\partial}{\partial x^i} \at{p}{},
\end{equation*}
where each $X^i : \mathbb{R}^n \to \mathbb{R}$. More succinctly,
\begin{equation*}
  X = X^i \frac{\partial}{\partial x^i}.
\end{equation*}
The functions $X^i : \mathbb{R}^n \to \mathbb{R}$ are called the \hldefn{component functions of
the vector field} $X$ wrt the standard vector fields.

\newthought{We are} now ready to define smoothness of a vector field.

\begin{defn}[Smooth Vector Fields]\index{Smooth Vector Fields}\label{defn:smooth_vector_fields}
  Let $X$ be a vector field on $\mathbb{R}^n$. Then $X = X^i \frac{\partial}{\partial x^i}$ for
  some uniquely determined function $X^i : \mathbb{R}^n \to \mathbb{R}$. We say that $X$ is
  \hlnoteb{smooth} if $X^i$ is smooth for every $i$. We write $X^i \in C^\infty(\mathbb{R}^n)$.
\end{defn}

\begin{remark}
  In multivariable calculus, a smooth field on $\mathbb{R}^n$ is a smooth map $X : \mathbb{R}^n
  \to \mathbb{R}^n$ given by
  \begin{equation*}
    X(p) = (X^1(p), \ldots, X^n(p)),
  \end{equation*}
  i.e. we could say that $X = (X^1, \ldots, X^n)$ is an $n$-tuple of smooth functions on
  $\mathbb{R}^n$.

  Note that this view is particular to $\mathbb{R}^n$ due to the canonical isomorphism between
  $T_p \mathbb{R}^n$ and $\mathbb{R}^n$ for all $p \in \mathbb{R}^n$.
\end{remark}

% section smooth_vector_fields (end)

% chapter lecture_10_jan_28th (end)

\chapter{Lecture 11 Jan 30th}%
\label{chp:lecture_11_jan_30th}
% chapter lecture_11_jan_30th

\section{Smooth Vector Fields (Continued)}%
\label{sec:smooth_vector_fields_continued}
% section smooth_vector_fields_continued

Let $X$ be a vector field on $\mathbb{R}^n$, not necessarily smooth. For any $p \in \mathbb{R}^n$,
we have that $X_p$ is a derivation on smooth functions defined on an open neighbourhood of $p$.
In particular, for any $f \in C^\infty(\mathbb{R}^n)$, $X_p f \in \mathbb{R}$ is a scalar. Then
we can define a function $X f : \mathbb{R}^n \to \mathbb{R}$ by
\begin{equation*}
  (X f)(p) = X_p f.
\end{equation*}

\begin{propo}[Equivalent Definition of a Smooth Vector Field]\label{propo:equivalent_definition_of_a_smooth_vector_field}
  The vector field $X$ on $\mathbb{R}^n$ is smooth iff $X f \in C^\infty(\mathbb{R}^n)$ for all
  $f \in C^\infty(\mathbb{R}^n)$.
\end{propo}

\begin{proof}
  Let $X = X^i \frac{\partial}{\partial x^i}$. Then
  \begin{equation*}
    (Xf)(p) = X_p f = X^i(p) = X^i (p) \frac{\partial f}{\partial x^i} \at{p}{}.
  \end{equation*}
  It follows that $Xf : \mathbb{R}^n \to \mathbb{R}$ is $X^i \frac{\partial f}{\partial x^i}$.
  Now if $X$ is smooth, then each of the $X^j$'s is smooth, and in particular
  $X^i \frac{\partial f}{\partial x^i}$ is smooth for any smooth $f$. On the other hand, suppose
  $Xf$ is smooth for any smooth function $f$. Then, consider $f = x^j$, which is smooth. Then
  \begin{equation*}
    Xf = X^i \frac{\partial x^j}{\partial x^i} = X^i \delta_i^j = X^j,
  \end{equation*}
  is a smooth function.
\end{proof}

\begin{note}
  This equivalent characterization of smoothness of vector fields is independent of any choice
  of basis of $\mathbb{R}^n$. Due to this, it is the \hlnotea{natural definition} of smoothness
  of vector fields on abstract smooth manifolds, where we cannot obtain a canonical basis for
  each tangent space.
\end{note}

\newthought{Let} $U \subseteq \mathbb{R}^n$ is open\sidenote{\hlwarn{Why do we need} $U$ to be
open? % TODO: Talk to prof
}. We can define a smooth vector field on $U$ to be an element $X =
X^i \frac{\partial}{\partial x^i}$ where each $X^i \in C^\infty(U)$ is smooth. From
\cref{propo:equivalent_definition_of_a_smooth_vector_field}, $U$ is smooth iff $Xf \in
C^\infty(U)$ for all $f \in C^\infty(U)$.

Hereafter, we shall assume that all our vector fields, regardless if it is on $\mathbb{R}^n$ or
some open subset $U \subset \mathbb{R}^n$, are smooth, even if we do not explicitly say that
they are.

\begin{note}[Notation]
  We write $\Gamma(T \mathbb{R}^n)$ for the set of smooth vector fields on $\mathbb{R}^n$. More
  generally, we write $\Gamma(T U)$ for $U \subseteq \mathbb{R}^n$ open.
\end{note}

The set $\Gamma(TU)$ is a real vector space, where the structure is given by
\begin{equation*}
  (aX + bY)_p = aX_p + bY_p
\end{equation*}
for all $X, Y \in \Gamma(TU)$ and $a, b \in \mathbb{R}$. This is an \hlnotea{infinite-dimensional}
\sidenote{\hlwarn{Why?} % TODO talk to the prof
} real vector space.

Further, $\forall X \in \Gamma(TU)$ and $h \in C^\infty(U)$, $hX$ is another
smooth vector field on $U$: Let $X = X^i \frac{\partial}{\partial x^i}$. Then $hX = (hX^i)
\frac{\partial}{\partial x^i}$, where $h X^i$ is the product of elements of $C^\infty(U)$.
Equivalently so,
\begin{equation*}
  (hX)_p = h(p) X_p.
\end{equation*}
We say that $\Gamma(TU)$ is a \hldefn{module} over the \hlnotea{ring} \sidenote{Whatever this
means here in Ring Theory.} $C^\infty(U)$.

\newthought{Let} $X$ be a smooth vector field on $U$. Since $X_p$ is a derivation on
$C_p^\infty$ for all $p \in U$, it motivates us to the following definition.

\begin{defn}[Derivation on $C^\infty_p$]\index{Derivation on $C^\infty_p$}\label{defn:derivation_on_c_infty_p_}
  Let $U \subseteq \mathbb{R}^n$ be open. A \hlnoteb{derivation} on $C^\infty(U)$ is a linear
  map $\mathcal{D} : C^\infty (U) \to C^\infty (U)$ that satisfies Leibniz's rule:
  \begin{equation*}
    \mathcal{D}( f \cdot g ) = f \cdot (\mathcal{D}g) + g \cdot (\mathcal{D}f),
  \end{equation*}
  where $f \cdot g$ denotes the multiplication of functions in $C^\infty(U)$.
\end{defn}

Clearly, given $X \in \Gamma(TU)$, $X$ is a derivation on $C^\infty(U)$ since for each $p \in U$,
we have \hlnotea{linearity}
\begin{equation*}
  (X(af + bg))(p) = X_p(af + bg) = aX_p f + bX_p g = a(Xf)(p) + b(Xg)(p),
\end{equation*}
and \hlnotea{Leibniz's rule}
\begin{align*}
  (X(fg))(p) &= X_p(fg) = f(p) X_p g + g(p) X_p f \\
             &= (f X)_p g + (g X)_p f = (f (Xg) + g (Xf))(p).
\end{align*}

Furthermore, if $\mathcal{D}$ is a derivation on $C^\infty(U)$, then we get that $\mathcal{D}
: U \to \mathbb{R}$ by $p \to \mathcal{D}_p f = (\mathcal{D}f)(p)$, which is a derivative at $p$.
It follows that $\mathcal{D}_p \in T_p \mathbb{R}^n$. Thus $\mathcal{D}$ is a vector field, and
since $\mathcal{D}f \in C^infty(U)$ for all $f \in C^\infty(U)$, from
\cref{propo:equivalent_definition_of_a_smooth_vector_field}, we have that $\mathcal{D}$ is smooth.
Hence the derivations on $C^\infty(U)$ are exactly the smooth vector fields on $U$.

% section smooth_vector_fields_continued (end)

\section{Smooth \texorpdfstring{$1$}{1}-Forms}%
\label{sec:smooth_1_forms}
% section smooth_1_forms

\begin{defn}[Cotangent Spaces and Cotangent Vectors]\index{Cotangent Spaces}\index{Cotangent Vectors}\label{defn:cotangent_spaces_and_cotangent_vectors}
  Let $p \in \mathbb{R}^n$. The \hlnoteb{cotangent space} to $\mathbb{R}^n$ at $p$ is defined
  to be the dual space $(T_p \mathbb{R}^n)^*$ of $T_p \mathbb{R}^n$, which is denoted as
  $T_p^* \mathbb{R}^n$. An element $\alpha_p \in T_p^* \mathbb{R}^n$, which is a linear map
  $\alpha_p : T_p \mathbb{R}^n \to \mathbb{R}$, is called a \hlnoteb{cotangent vector} at $p$.
\end{defn}

\marginnote{
\begin{mnote}
  This entire part is similar to our construction of smooth vector fields plus the stuff that
  we learned in \hyperref[chp:lecture_3_jan_11th]{Lecture 3} on $k$-forms.
\end{mnote}}
\begin{remark}
  The idea of a smooth $1$-form is that we want to attach a cotangent vector $\alpha_p \in
  T_p^* \mathbb{R}^n$ at every point $p \in \mathbb{R}^n$ in a smoothly varying manner.
\end{remark}

Let
\begin{equation*}
  T^* \mathbb{R}^n = \bigcup_{p \in \mathbb{R}^n} T_p^* \mathbb{R}^n
\end{equation*}
be the union of all the cotangent spaces to $\mathbb{R}^n$. This is called the \hldefn{cotangent
bundle} of $\mathbb{R}^n$ \sidenote{Again, for us, this is just a set. We shall see this again
in PMATH 465.}.

\begin{defn}[$1$-Form on the Cotangent Bundle]\index{$1$-Form}\label{defn:_1_form_on_the_cotangent_bundle}
  A \hlnoteb{$1$-form} $\alpha$ on $\mathbb{R}^n$ is a map $\alpha : \mathbb{R}^n \to
  T^* \mathbb{R}^n$ such that $\alpha(p) \in T_p^* \mathbb{R}^n$ for all $p \in \mathbb{R}^n$.
  We will always define $\alpha(p)$ by $\alpha_p$.
\end{defn}

Let $\{ \hat{e}_1, \ldots, \hat{e}_n \}$ be the standard basis of $\mathbb{R}^n$. Then $\{
(\hat{e}_1)_p, \ldots, (\hat{e}_n)_p \}$ is a basis for $T_p \mathbb{R}^n$. For now, we shall
denote the dual basis of $T_p^* \mathbb{R}^n$ by $\{ (\hat{e}^1)_p, \ldots, (\hat{e}^n)_p \}$.
We may think of each $\hat{e}^i$ as a $1$-form, where $\hat{e}^i (p) = (\hat{e}^i)_p$. We shall
call these the \hldefn{standard $1$-forms} on $\mathbb{R}^n$.

So for any $1$-form $\alpha$ on $\mathbb{R}^n$, since $\alpha_p \in T_p^* \mathbb{R}^n$, we can
write
\begin{equation*}
  \alpha_p = \alpha_i (p) (\hat{e}^i)_p,
\end{equation*}
where each $\alpha_i : \mathbb{R}^n \to \mathbb{R}$ is a function. More succinctly,
\begin{equation}\label{eq:1_forms_wrt_standard_1_forms}
  \alpha = \alpha_i \hat{e}^i,
\end{equation}
for some \hlwarn{uniquely} determined functions $\alpha_i : \mathbb{R}^n \to \mathbb{R}$, where
\cref{eq:1_forms_wrt_standard_1_forms} means that $\alpha_p = \alpha_i(p)(\hat{e}^i)_p$. The
functions $\alpha_i : \mathbb{R}^n \to \mathbb{R}$ are called the \hldefn{component functions} of
the $1$-form $\alpha$ wrt the standard $1$-forms.

With that, we can define smoothness on $1$-forms. Again, we will then find an equivalent definition
that does not depend on a basis.

\begin{defn}[Smooth $1$-Forms]\index{Smooth $1$-Forms}\label{defn:smooth_1_forms}
  We say that a $1$-form $\alpha$ on $\mathbb{R}^n$ is \hlnoteb{smooth} if the component functions
  $\alpha_i : \mathbb{R}^n \to \mathbb{R}$ given in \cref{eq:1_forms_wrt_standard_1_forms} are all
  smooth functions, i.e. each $\alpha_i \in C^\infty(\mathbb{R}^n)$.
\end{defn}

Let $\alpha$ be a $1$-form on $\mathbb{R}^n$, not necessarily smooth. Then for any $p \in
\mathbb{R}^n$, we know that $\alpha_p \in L(T_p \mathbb{R}^n, \mathbb{R})$. Thus for any vector
field $X$ on $\mathbb{R}^n$ not necessarily smooth, $\alpha_p (X_p) \in \mathbb{R}$ is a scalar.
We can then define a function $\alpha X : \mathbb{R}^n \to \mathbb{R}$ by
\begin{equation}\label{eq:1_form_on_vector_fields_get_scalar}
  (\alpha(X))(p) = \alpha_p(X_p).
\end{equation}

\begin{propo}[Equivalent Definition for Smoothness of $1$-Forms]\label{propo:equivalent_definition_for_smoothness_of_1_forms}
  The $1$-form $\alpha$ on $\mathbb{R}^n$ is smooth iff $\alpha(X) \in C^\infty(\mathbb{R}^n)$
  for all $X \in \Gamma(T \mathbb{R}^n)$.
\end{propo}

\begin{proof}
  First, let $X = X^i \frac{\partial}{\partial x^i} = X^i \hat{e}_i$ and $\alpha =
  \alpha_j \hat{e}^j$. Then we have
  \begin{align*}
    (\alpha(X))(p) &= \alpha_p(X_p) = (\alpha_j(p)(\hat{e}^j)_p)(X^i(p)(\hat{e}_i)_p) \\
                   &= \alpha_j(p)X^i(p)(\hat{e}^j)_p(\hat{e}_i)_p \\
                   &= \alpha_j(p)X^i(p) \delta_i^j = \alpha_i(p) X^i(p).
  \end{align*}
  Since $p$ was arbitrary, we have
  \begin{equation}\label{eq:1_form_on_vector_fields_get_scalar_as_a_sum}
    \alpha(X) = \alpha_i X^i.
  \end{equation}

  Suppose that $\alpha$ is smooth, i.e. $\alpha_i$ is smooth. Then for any smooth vector field
  $X$, $\alpha_i X^i$ is smooth.

  Conversely, if $\alpha(X)$ is smooth for any smooth $X$. Then in particular, if $X
  = \frac{\partial}{\partial x^j}$, It follows that $X^i = \delta_j^i$ since $X = X^i
  \frac{\partial}{\partial x^i}$. Then $\alpha(X) = \alpha_i X^i = \alpha_i \delta_j^i =
  \alpha_j$ is smooth.
\end{proof}

\begin{remark}
  Again, we see that this characterization is independent of the choice of basis.
\end{remark}

\begin{note}
  In the last step of the proof for \cref{propo:equivalent_definition_for_smoothness_of_1_forms},
  we observe that if $X = \hat{e}_i$ is the $i$\textsuperscript{th} standard vector field on
  $\mathbb{R}^n$. Then
  \begin{equation*}
    X = X^j \hat{e}_j = X^j \frac{\partial}{\partial x^j}
  \end{equation*}
  where $X^j = \delta^{i}_j$. Then if $\alpha = \alpha_k \hat{e}^k$ is a $1$-form, we have that
  $\alpha(X) = \alpha(\hat{e}_i) = \alpha_i$, i.e.
  \begin{equation}\label{eq:1_forms_with_coefficients_as_differentials}
    \alpha = \alpha_i \hat{e}^j, \text{ where } \alpha_i = \alpha(\hat{e}_i)
      = \alpha \left( \frac{\partial}{\partial x^i} \right)
  \end{equation}
  Note that the above is a `parameterized version' of \cref{eq:v_wrt_basis}, where the coefficients
  are smooth functions on $\mathbb{R}^n$.
\end{note}

\newthought{If} $U \subseteq \mathbb{R}^n$ is open, we can define a smooth $1$-form on $U$ to
be an element $\alpha = \alpha_i \hat{e}^i$ where $\alpha_i \in C^\infty(U)$ is smooth. We require
$U$ to be open to be able to define smoothness\sidenote{Probably a similar question, but
\hlwarn{why}? % TODO : talk to prof
} at all points of $U$. \cref{propo:equivalent_definition_for_smoothness_of_1_forms} generalizes
to say that a $1$-form on $U$ is smooth iff $\alpha(X) \in C^\infty(U)$ for all $X \in \Gamma(TU)$.

We shall write $\Gamma(T^* \mathbb{R}^n)$ for the set of smooth $1$-forms on $\mathbb{R}^n$ and
more generally $\Gamma(T^* U)$ for te set of smooth $1$-forms on $U$. The set $\Gamma(T^* U)$ is a
real vector space, where the vector space structure is given by
\begin{equation*}
  (a \alpha + b \beta)_p = a \alpha_p + b \beta_p
\end{equation*}
for all $\alpha, \beta \in \Gamma(T^* U)$ and $a, b \in \mathbb{R}$. Again, this is an
\hlnotea{infinite-dimensional} real vector space. Moreover, for $\alpha \in \Gamma(T^* U)$ and
$h \in C^\infty(U)$, $h\alpha$ is another smooth $1$-form on $U$, given as follows:

Let $\alpha = \alpha_i \hat{e}^i$. Then $h\alpha = (h\alpha_i)\hat{e}^i$, where $h \alpha_i$ is
the product of elements of $C^\infty(U)$. Equivalently so
\begin{equation*}
  (h\alpha)_p = h(p) \alpha_p.
\end{equation*}
We say that $\Gamma(T^* U)$ is a \hldefn{module} over the ring $C^\infty(U)$.

% section smooth_1_forms (end)

% chapter lecture_11_jan_30th (end)

\chapter{Lecture 12 Feb 01st}%
\label{chp:lecture_12_feb_01st}
% chapter lecture_12_feb_01st

\section{Smooth \texorpdfstring{$1$}{1}-Forms (Continued)}%
\label{sec:smooth_1_forms_continued}
% section smooth_1_forms_continued

Given a smooth function $f$ on $U$, there is a way for us to obtain a $1$-form on $U$:

\begin{defn}[Exterior Derivative of $f$ ($1$-form)]\index{Exterior Derivative}\label{defn:exterior_derivative_of_f_1_form_}
  Let $f \in C^\infty(U)$. We define $\dif{f} \in \Gamma(T^* U)$ by
  \begin{equation*}
    (\dif{f})(X) = Xf \in C^\infty(U)
  \end{equation*}
  for all $X \in \Gamma(TU)$. That is, for all $p \in U$, we have $(\dif{f})_p(X_p) = (Xf)_p
  = X_p f$. This one form is called the \hlnoteb{exterior derivative of $f$}.
\end{defn}

\begin{note}
  It is clear that $(\dif{f})_p : T_p \mathbb{R}^n \to \mathbb{R}$ is linear, since
  \begin{align*}
    (\dif{f})_p(a X_p + b Y_p) &= (aX_p + bY_p) f = aX_p f + bY_p f \\
                               &= a(\dif{f})_p(X_p) + b(\dif{f})_p(Y_p).
  \end{align*}

  Also, $\dif{f}$ is smooth since $(\dif{f})(X) = Xf$ is smooth for all smooth $X$.
\end{note}

If $f \in C^\infty(U)$, then $f : U \subseteq \mathbb{R}^n \to \mathbb{R}$ is smooth, so its
\hldefn{Jacobian} (or \hldefn{differential}) at  $p \in U$ has already been defined and was
denoted $(\dif{f})_p$. It is linear from $\mathbb{R}^n$ to $\mathbb{R}$, which is representative
by a $1 \times n$ matrix. Of course, we need to clarify why we claimed that $\dif{f}$ is a
Jacobian.

\begin{propo}[Exterior Derivative as the Jacobian]\label{propo:exterior_derivative_as_the_jacobian}
  Under the \hyperref[propo:canonical_bijection_from_t_p_r_n_to_r_n_]{canonical isomorphism}
  between $T_p \mathbb{R}^n$ and $\mathbb{R}^n$, the exterior derivative $(\dif{f})_p :
  T_p \mathbb{R}^n \to \mathbb{R}$ of $f$ at $p$ and the differential $(\D f)_p : \mathbb{R}^n
  \to \mathbb{R}$ coincide. Moreover, wrt the standard $1$-forms on $\mathbb{R}^n$, we have
  \begin{equation}\label{eq:exterior_derivative_as_a_differential}
    \dif{f} = \frac{\partial f}{\partial x^i} \hat{e}^i.
  \end{equation}
\end{propo}

\begin{proof}
  For the $1$-form $\dif{f}$, we have
  \begin{equation*}
    (\dif{f})_p(\hat{e}_i)_p = (\hat{e}_i)_p f = \frac{\partial f}{\partial x^i} \at{p}{},
  \end{equation*}
  so by \cref{eq:1_forms_with_coefficients_as_differentials}, we have
  \begin{equation*}
    \dif{f} = \frac{\partial f}{\partial x^i} \hat{e}^i,
  \end{equation*}
  which is \cref{eq:exterior_derivative_as_a_differential}.

  Now the differential $(\D f)_p : \mathbb{R}^n \to \mathbb{R}$ is the $1 \times n$ matrix
  \begin{equation*}
    (\D f)_p = \begin{pmatrix}
      \frac{\partial f}{\partial x^1} \at{p}{} & \hdots & \frac{\partial f}{\partial x^n} \at{p}{}
    \end{pmatrix}.
  \end{equation*}
  Thus $(\D f)_p (\hat{e}_i)_p = \frac{\partial f}{\partial x^i} \at{p}{}$, so as an element of
  $(\mathbb{R}^n)^*$, we can write $(\D f)_p = \frac{\partial f}{\partial x^i}\at{p}{}
  (\hat{e}^i)_p$. Since $T_p$ is an isomorphism from $\mathbb{R}^n$ to $T_p \mathbb{R}^n$ taking
  $\hat{e}_i$ to $(\hat{e}_i)_p$, the dual map $(T_p)^*$ is an isomorphism from $T_p^* \mathbb{R}^n
  \to ( \mathbb{R}^n )^*$, taking $(\hat{e}^i)_p$ to $\hat{e}_i$. Thus we observe that
  \begin{equation*}
    (\dif{f})_p : T_p^* \mathbb{R}^n \to \mathbb{R} \text{ at } p
  \end{equation*}
  is brought to the same basis as
  \begin{equation*}
    (\D f)_p : \mathbb{R}^n \to \mathbb{R} \text{ at } p,
  \end{equation*}
  which is what we needed to show.
\end{proof}

\newthought{Now} consider the smooth functions $x^j$ on $\mathbb{R}^n$. We obtain a $1$-form
$\dif{x^j}$, which is expressible as $\dif{x^j} = \alpha_i \hat{e}^i$ for some smooth functions
$\alpha_i$ on $\mathbb{R}^n$. By \cref{eq:1_forms_with_coefficients_as_differentials}, we have
$\alpha_i = (\dif{x}^j)(\frac{\partial}{\partial x^i}) = \frac{\partial x^j}{\partial x^i} =
\delta_i^j$. So $\dif{x^j} = \delta_i^j \hat{e}^i = \hat{e}^j$. We have thus showed that
\begin{equation}\label{eq:standard_1_forms_as_exterior_derivatives}
  \dif{x^j} = \hat{e}^j \text{ for all } j \in \{ 1, \ldots, n \}.
\end{equation}

\cref{eq:standard_1_forms_as_exterior_derivatives} tells us that the standard $1$-forms $\hat{e}^j$
on $\mathbb{R}^n$ are given by the exterior derivatives of the standard coordinate functions $x^j$,
and consequently the action of $\hat{e}^j = \dif{x^j}$ on a vector field $X$ is by $\hat{e}^j(X)
= (\dif{x^j})(X) = X x^j$. Thus from hereon, we shall always write the standard $1$-forms on
$\mathbb{R}^n$ as $\{ \dif{x^1}, \ldots, \dif{x^n} \}$.

So by putting \cref{eq:exterior_derivative_as_a_differential} and
\cref{eq:standard_1_forms_as_exterior_derivatives} together, we obtain the familiar
\begin{equation}\label{eq:differential_of_f_from_multivar_calc}
  \dif{f} = \frac{\partial f}{\partial x^i} \dif{x^i},
\end{equation}
which is the `\hlnoteb{differential}' of $f$ from multivariable calculus that is usually not
as rigourously defined in earlier courses.

\newthought{We are now} equipped with nice interpretations of the standard vector fields
and standard $1$-forms on $\mathbb{R}^n$. From \cref{eq:standard_basis_for_standard_vector_fields},
we know that standard vector fields are also partial differential operators 
$\frac{\partial}{\partial x^i}$ on $C^\infty(\mathbb{R}^n)$, where
\begin{equation*}
  \hat{e}_i f = \frac{\partial f}{\partial x^i},
\end{equation*}
and \cref{eq:standard_1_forms_as_exterior_derivatives} tells us the standard $1$-forms should be
regarded as $1$-forms $\dif{x^j}$, whose action on a vector field $X$ is the derivation of $X$
on the function $x^j$. In other words,
\begin{equation*}
  \hat{e}^j(X) = (\dif{x^j})(X) = X x^j.
\end{equation*}
Notice that if $X = \frac{\partial}{\partial x^i}$,
\begin{equation*}
  (\dif{x^j})\left( \frac{\partial}{\partial x^j} \right) = \frac{\partial x^j}{\partial x^i}
    = \delta_i^j,
\end{equation*}
which gives us that at every point $p \in \mathbb{R}^n$, the basis $\{ (\hat{e}^1)_p, \ldots,
  (\hat{e}^n)_p \}$ of $T_p^* \mathbb{R}^n$ is the \hldefn{dual basis} of the basis $\{
(\hat{e}_1)_p, \ldots, (\hat{e}_n)_p \}$ of $T_p \mathbb{R}^n$. 

% section smooth_1_forms_continued (end)

\section{Smooth Forms on \texorpdfstring{$\mathbb{R}^n$}{Rn}}%
\label{sec:smooth_forms_on_r_n_}
% section smooth_forms_on_r_n_

We shall continue the same game and define a smooth $k$-forms.

\begin{defn}[Space of $k$-Forms on $\mathbb{R}^n$]\index{Space of $k$-Forms on $\mathbb{R}^n$}\label{defn:space_of_k_forms_on_r_n_}
  Let $p \in \mathbb{R}^n$ and $1 \leq k \leq n$. The space $\Lambda^k(T_p^* \mathbb{R}^n)$ is
  defined as the \hlnoteb{space of $k$-forms} on $\mathbb{R}^n$ at $p$.
\end{defn}

\begin{remark}
  If $k = 0$, we before, we define $\Lambda^0 (T_p^* \mathbb{R}^n) = \mathbb{R}$.
\end{remark}

\begin{note}
  For any element $\eta_p \in \Lambda(T_p^* \mathbb{R}^n)$, $\eta_p$ is $k$-linear
  and skew-symmetric, i.e.
  \begin{equation*}
    \eta_p : \underbrace{(T_p \mathbb{R}^n) \times \hdots \times (T_p \mathbb{R}^n)
      }_{k \text{ copies }} \to \mathbb{R}.
  \end{equation*}
\end{note}

\begin{defn}[$k$-Forms at $p$]\index{$k$-Forms at $p$}\label{defn:_k_forms_at_p_}
  Elements of $\Lambda^k(T_p^* \mathbb{R}^n)$ are called \hlnoteb{$k$-forms at $p$}.
\end{defn}

Again, we want to attach an element $\eta_p \in \Lambda^k(T_p^* \mathbb{R}^n)$ at every $p \in
\mathbb{R}^n$, in a smoothly varying way. Since $\Lambda^0(T_p^* \mathbb{R}^n) = \mathbb{R}$,
a $0$-form on $\mathbb{R}^n$ is a smoothly varying assignment of a \hlnotea{real number} to every
$p \in \mathbb{R}^n$, i.e. a $0$-form on $\mathbb{R}^n$ is a very familiar object: they are just
\hlnotea{smooth functions} on $\mathbb{R}^n$.

For $1 \leq k \leq n$, let $\Lambda^k(T^* \mathbb{R}^n) = \bigcup_{p \in \mathbb{R}^n} 
\Lambda^k(T_p^* \mathbb{R}^n)$, which is caled the \hldefn{bundle of $k$-forms} on $\mathbb{R}^n$.
For us, this is just a set.

\begin{defn}[$k$-Form on $\mathbb{R}^n$]\index{$k$-Form on $\mathbb{R}^n$}\label{defn:_k_form_on_r_n_}
  Let $1 \leq k \leq n$. A \hlnoteb{$k$-form} $\eta$ on $\mathbb{R}^n$ is a map $\eta : 
  \mathbb{R}^n \to \Lambda^k(T^* \mathbb{R}^n)$ such that $\eta(p) \in \Lambda^k(T_p^* 
  \mathbb{R}^n)$ for all $p \in \mathbb{R}^n$. We will always denote $\eta(p)$ by $\eta_p$.
\end{defn}

Recall from our discussions in \cref{sec:smooth_vector_fields} and \cref{sec:smooth_1_forms},
\begin{equation*}
  \left\{ \frac{\partial}{\partial x^1} \at{p}{}, \ldots, \frac{\partial}{\partial x^n} \at{p}{}  \right\}
\end{equation*}
is the standard basis of $T_p \mathbb{R}^n$, with dual basis
\begin{equation*}
  \left\{ \dif{x^1}\at{p}{}, \ldots, \dif{x^n}\at{p}{} \right\}
\end{equation*}
if $T_p^* \mathbb{R}^n$. Then by \cref{thm:basis_of_lambda_k_v_}, the set
\begin{equation*}
  \left\{ \dif{x^{i_1}}\at{p}{} \land \hdots \land \dif{x^{i_k}}\at{p}{} : 1 \leq i_1 < \hdots < i_k\leq n \right\}
\end{equation*}
is a basis for $\Lambda^k(T_p^* \mathbb{R}^n)$. We can then define $k$-forms $\dif{x^{i_1}} \land
\hdots \land \dif{x^{i_k}}$ on $\mathbb{R}^n$ by
\begin{equation*}
  (\dif{x^{i_1}} \land \hdots \land if  \dif{x^{i_k}})_p = \dif{x^{i_1}}_p \land \hdots \land \dif{x^{i_k}}_p.
\end{equation*}
We shall call these the \hldefn{standard $k$-forms} on $\mathbb{R}^n$.

Then for any $k$-form $\eta$ on $\mathbb{R}^n$, since $\eta_p \in \Lambda^k(T_p^* \mathbb{R}^n)$,
we can write
\begin{align}
  \eta_p 
  &= \sum_{j_1 < \hdots < j_k} \eta_{j_1, \ldots, j_k}(p)
  \dif{x^{j_1}}\at{p}{} \land \hdots \land \dif{x^{j_k}} \at{p}{} \nonumber \\
  &= \frac{1}{k!} \eta_{j_1, \ldots, j_k}(p) \dif{x^{j_1}} \at{p}{} \land
  \hdots \land \dif{x^{j_k}} \at{p}{} \label{eq:k_forms_using_differentials_eval_at_p}
\end{align}
where each $\eta_{j_1, \ldots, j_k} : \mathbb{R}^n \to \mathbb{R}$ is a function. More succinctly,
\begin{equation}\label{eq:k_forms_using_differentials}
  \eta 
    = \sum_{j_1 < \ldots < j_k} \eta_{j_1, \ldots, j_k} 
      \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}}
    = \frac{1}{k!} \eta_{j_1, \ldots, j_k} \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}},
\end{equation}
for some uniquely determined functions $\eta_{j_1, \ldots, j_k} : \mathbb{R}^n \to \mathbb{R}$,
which are skew-symmetric in their $k$ indices $j_1, \ldots, j_k$. The functions $\eta_{j_1,
\ldots, j_k} : \mathbb{R}^n \to \mathbb{R}$ are called the \hldefn{component functions} of the
$k$-form $\eta$ with respect to the standard $k$-forms. We can now give our first definition
of smoothness.

\begin{defn}[Smooth $k$-Forms on $\mathbb{R}^n$]\index{Smooth $k$-Forms on $\mathbb{R}^n$}\label{defn:smooth_k_forms_on_r_n_}
  We say that a \hlnoteb{$k$-form} $\eta$ on $\mathbb{R}^n$ is \hlnoteb{smooth} if the component
  functions $\eta_{j_1, \ldots, j_k} : \mathbb{R}^n \to \mathbb{R}$ as defined in 
  \cref{eq:k_forms_using_differentials} are all smooth funtions. In other words, each
  $\eta_{j_1, \ldots, j_k} \in C^\infty(\mathbb{R}^n)$.
\end{defn}

\begin{note}
  A smooth $k$-form is also called a \hlnotea{differential $k$-form}, but we will not be using
  this terminology in this course.
\end{note}

Let $\eta$ be a $k$-form that is not necessarily smooth. Then for any $p \in \mathbb{R}^n$, we know
\begin{equation*}
  \eta_p : \underbrace{(T_p \mathbb{R}^n) \times \hdots \times (T_p \mathbb{R}^n)}_{k \text{ copies }} \to \mathbb{R}.
\end{equation*}
So if $X_1, \ldots, X_k$ are arbitrary vector fields on $\mathbb{R}^n$ that are not necessarily
smooth, we get a scalar
\begin{equation*}
  \eta_p((X_1)_p, \ldots, (X_k)_p) \in \mathbb{R}.
\end{equation*}
Thus we can define a function $\eta(X_1, \ldots, X_k) : \mathbb{R}^n \to \mathbb{R}$ by
\begin{equation}\label{eq:k_forms_without_basis}
  (\eta(X_1, \ldots, X_k))(p) = \eta_p((X_1)_p, \ldots, (X_k)_p).
\end{equation}

\begin{propo}[Equivalent Definition of Smothness of $k$-Forms]\label{propo:equivalent_definition_of_smothness_of_k_forms}
  The $k$-form $\eta$ on $\mathbb{R}^n$ is smooth iff $\eta(X_1, \ldots, X_k) \in 
  C^\infty(\mathbb{R}^n)$ for all $X_1, \ldots, X_k \in \Gamma(T \mathbb{R}^n)$.
\end{propo}

\begin{proof}
  For $l = 1, \ldots, k$, write $X_l = X_l^{l_i} \frac{\partial}{\partial x^{l_i}}$, and
  $\eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k} \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}}$.
  Then with \cref{eq:k_forms_using_differentials_eval_at_p} and 
  \cref{eq:another_alt_of_a_k_form_scalar}, we have that
  \begin{align*}
    (\eta(X_1, \ldots, X_k))(p)
      &= \eta_p((X_1)_p, \ldots, (X_k)_p) \\
      &= \eta_p\left(X_1^{l_1}(p) \frac{\partial}{\partial x^{l_1}}\at{p}{}, \ldots, X_k^{l_k}(p) \frac{\partial}{\partial x^{l_k}}\at{p}{}\right) \\
      &= X_l^{l_1}(p) \hdots X_k^{l_k}(p) \eta_p 
      \left( \frac{\partial}{\partial x^{l_1}}\at{p}{}, \ldots, \frac{\partial}{\partial x^{l_k}}\at{p}{} \right) \\
      &= X_1^{l_1}(p) \hdots X_k^{l_k}(p) \eta_{l_1, \ldots, l_k}(p).
  \end{align*}
  Since this holds for an arbitrary $p \in \mathbb{R}^n$, we have that
  \begin{equation}\label{eq:k_forms_without_basis_eq1}
    \eta(X_1, \ldots, X_k) = X_1^{l_1} \hdots X_k^{l_k} \eta_{l_1, \ldots, l_k}.
  \end{equation}
  So the function $\eta(X_1, \ldots, X_k) : \mathbb{R}^n \to \mathbb{R}$ is in fact $X_1^{l_1}
  \hdots X_k^{l_k} \eta_{l_1, \ldots, l_k}$.

  Suppose that $\eta$ is smooth. Then each of the $\eta_{j_1, \ldots, j_k}$ is smooth, and so in
  particular $X_1^{l_1} \hdots X_k^{l_k} \eta_{l_1, \ldots, l_k}$ is smooth for smooth vector
  fields $X_1, \ldots, X_k$.

  Conversely, sps $\eta(X_1, \ldots, X_k)$ is smooth for any smooth $X_1, \ldots, X_k$. Then
  consider $X_l^{l_i} = \delta^{l_i j_i}$. Then
  \begin{equation*}
    \eta(X_1, \ldots, X_k) = \eta_{l_1, \ldots, l_k} \delta^{l_1 j_1} \hdots \delta^{l_k j_k} 
      = \eta_{j_1, \ldots, j_k}
  \end{equation*}
  is smooth.
\end{proof}

\begin{remark}
  The proof above provides us a very useful observation. Let 
  $X_i = \frac{\partial}{\partial x^{j_i}}$ be the $j_i$\textsuperscript{th} standard vector
  field on $\mathbb{R}^n$. Then $X = X_i^{l_i} \frac{\partial}{\partial x^{l_i}}$ where
  $X_i^{l_i} = \delta^{l_i j_i}$. Then if $\eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k}
  \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}}$ is a $k$-form, we have that 
  $\eta(X_1, \ldots, X_k) = \eta_{j_1, \ldots, j_k}$. In other words,
  \begin{equation}\label{eq:k_forms_using_only_the_vector_fields}
    \eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k} \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}}
    \text{ where } \eta_{j_1, \ldots, j_k} 
    = \eta \left( \frac{\partial}{\partial x^{j_1}}, \ldots, \frac{\partial}{\partial x^{j_k}} \right)
  \end{equation}
\end{remark}

Now if $U \subseteq \mathbb{R}^n$ is open, we define a smooth $k$-form on $U$ to be an element
$\eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k} \dif{x^{j_1}} \land \dots \land \dif{x^{j_k}}$, where
$\eta_{j_1, \ldots, j_k} \in C^\infty(U)$ is smooth. We need $U$ to be able to define smoothness
at all points of $U$. Again, it is clear that 
\cref{propo:equivalent_definition_of_smothness_of_k_forms} generalizes to say that $k$-forms on $U$
are smooth iff $\eta(X_1, \ldots, X_k) \in C^\infty(U)$ for all $X_1, \ldots, X_k \in \Gamma(TU)$.

We shall write $\Gamma(\Lambda^k(T^* \mathbb{R}^n))$ for the set of smooth $k$-forms on 
$\mathbb{R}^n$, and more generally $\Gamma(\Lambda^k(T^* U))$ for the set of smooth $k$-forms
on $U$. The set $\Gamma(\Lambda^k(T^* U))$ is a real vector space, where the vector space structure
is given by
\begin{equation*}
  (a \eta + b \zeta)_p = a \eta_p + b \zeta_p
\end{equation*}
for all $\eta, \zeta \in \Gamma(\Lambda^k(T^* U))$ and $a, b \in \mathbb{R}$. Again, this space is
\hlnotea{infinite-dimensional}. Moreover, given $\eta \in \Gamma(\Lambda^k(T^* U))$ and 
$h \in C^\infty(U)$, $h \eta$ is another smooth $k$-form on $U$, defined as follows:

Let
\begin{equation*}
  \eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k} \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}}.
\end{equation*}
Then
\begin{equation*}
  h \eta = \frac{1}{k!} (h\eta_{j_1, \ldots, j_k}) \dif{x^{j_1}} \land \hdots \land \dif{x^{j_k}},
\end{equation*}
where $h\eta_{j_1, \ldots, j_k}$ is the product of elements of $C^{\infty}(U)$. Or equivalently,
we can define
\begin{equation}\label{eq:k_form_with_scalar}
  (h \eta)_p = h(p) \eta_p.
\end{equation}
We say that $\Gamma(\Lambda^k(T^* U))$ is a \hldefn{module} over the ring $C^\infty(U)$. Also,
note that if $k = 0$, we have $\Gamma(\Lambda^0(T^* U)) = C^\infty(U)$.

\begin{note}[Notation]
  To minimize notation, we shall write
  \begin{equation*}
    \Omega^k(U) = \Gamma(\Lambda^k(T*U))
  \end{equation*}
  to be the space of smooth $k$-forms on $U$. Note that $\Omega^0(U) = C^\infty(U)$.
\end{note}

% section smooth_forms_on_r_n_ (end)

% chapter lecture_12_feb_01st (end)

\chapter{Lecture 13 Feb 04th}%
\label{chp:lecture_13_feb_04th}
% chapter lecture_13_feb_04th

\section{Wedge Product of Smooth Forms}%
\label{sec:wedge_product_of_smooth_forms}
% section wedge_product_of_smooth_forms

We can now define \hyperref[sec:wedge_product_of_forms]{wedge products} on these smooth $k$-forms.

\begin{defn}[Wedge Product of $k$-Forms]\index{Wedge Product of $k$-Forms}\label{defn:wedge_product_of_k_forms}
  Let $\eta \in \Omega^k(U)$ and let $\zeta \in \Omega^l(U)$. Then the \hlnoteb{wedge product}
  $\eta \land \zeta$ is an element of $\Omega^{k + l}(U)$ defined by
  \begin{equation*}
    (\eta \land \zeta)_p = \eta_p \land \zeta_p.
  \end{equation*}
\end{defn}

By the properties of wedge products on forms at $p$ for any $p \in U$, we may generalize the
properties that were shown on page \cref{remark:wedge_product_operation_properties}, which
shall be shown here:

\begin{note}\label{note:wedge_product_operation_properties_generalized}
  Let $\eta, \zeta \in \Omega^k(U)$ and $\rho \in \Omega^l(U)$. Let $f, g \in C^\infty(U)$. Then
  \begin{equation*}
    (f \eta + g \zeta) \land \rho = f \eta \land \rho + g \zeta \land \rho.
  \end{equation*}
  Similarly,
  \begin{equation*}
    \rho \land (f \eta + g \zeta) = f \rho \land \eta + g \rho \land \zeta.
  \end{equation*}
  These show that the wedge product of smooth forms is linear in each argument.

  Further, we have that the wedge product of smooth forms is associative: we have
  \begin{equation*}
    (\zeta \land \eta) \land \rho = \zeta \land (\eta \land \rho),
  \end{equation*}
  for any smooth forms $\eta, \zeta, \rho$ of any degree.

  Finally, wedge product of smooth forms is also \hldefn{skew-commutative}:
  \begin{equation}\label{eq:skew_commutativity_of_wedge_prod_of_smooth_forms}
    \zeta \land \eta = (-1)^{\abs{\eta}\abs{\zeta}} \eta \land \zeta.
  \end{equation}
  In particular, if $\abs{\eta}$ is odd, then 
  \cref{eq:skew_commutativity_of_wedge_prod_of_smooth_forms} says that $\eta \land \eta = 0$.
\end{note}

These properties makes it easier to compute wedge products of smooth forms.

\begin{eg}
  Let $\eta = y \dif{x} + \sin z \dif{y}$ and $\zeta = x^3 \dif{x} \land \dif{z}$. Then we have
  \begin{align*}
    \eta \land \zeta &= (y \dif{x} + \sin z \dif{y}) \land (x^3 \dif{x} \land \dif{z}) \\
                     &= x^3 y \dif{x} \land \dif{x} \land \dif{z} 
                      + x^3 \sin z \dif{y} \land \dif{x} \land \dif{z} \\
                     &= - x^3 \sin z \dif{x} \land \dif{y} \land \dif{z}.
  \end{align*}
\end{eg}

% section wedge_product_of_smooth_forms (end)

\section{Pullback of Smooth Forms}%
\label{sec:pullback_of_smooth_forms}
% section pullback_of_smooth_forms

Recall that following \cref{sec:wedge_product_of_forms} (wedge product of forms), we introduced
pullback of forms (\cref{sec:pullback_of_forms}). We shall be introducing an analogue of pullbacks
for smooth forms.

Let $k \geq 1$. From \cref{sec:pullback_of_forms}, if $S \in L(V< W)$, then $S^* : \Lambda^k(W^*)
\to \Lambda^k(V^*)$ is an induced linear map that we called the pullback, defined by
\begin{equation}\label{eq:normal_pullback_as_guide}
  (S^* \alpha)(v_1, \ldots, v_k) = \alpha(Sv_1, \ldots, Sv_k)
\end{equation}
for all $\alpha \in \Lambda^k(W^*)$. There is, however, some preliminary results that we need to
understand before generalizing the above.

Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be a smooth map, $x = (x^1, \ldots, x^n)$ for coordinates
on the domain $\mathbb{R}^n$ and $y = (y^1, \ldots, y^m)$ for coordinates on the codomain
$\mathbb{R}^m$. Thus for $p \in \mathbb{R}^n$, a basis for $T_p \mathbb{R}^n$ is given by
$\mathcal{B} = \left\{ \frac{\partial}{\partial x^i}\at{p}{}, \ldots,
\frac{\partial}{\partial x^n}\at{p}{} \right\}$ and, for $q \in \mathbb{R}^m$, a basis for $T_q
\mathbb{R}^m$ is given by $\mathcal{C} = \left\{ \frac{\partial}{\partial y^1}\at{q}{},
\ldots, \frac{\partial}{\partial y^m}\at{q}{} \right\}$. We write $y = F(x) = (F^1(x), \ldots,
F^m(x))$.

For any $p \in \mathbb{R}^n$, we have an induced linear map $(\dif{F})_p : T_p \mathbb{R}^n \to
T_{F(p)} \mathbb{R}^m$, which we defined in A2. The definition shall be restated here. If $X_p
= [\phi]_p \in T_p \mathbb{R}^n$, then $(\dif{F})_p X_p = [F \circ \phi]_{F(p)}$. We showed that
the $m \times n$ matrix for $(\dif{F})_p$ wrt the bases $\mathcal{B}$ and $\mathcal{C}$ is
$(\D F)_p$, the \hyperref[defn:differential]{Jacobian} of $F$ at $p$. That is,
\begin{equation}\label{eq:linear_map_across_tangent_spaces_over_f}
  (\dif{F})_p \frac{\partial}{\partial x^i} \at{p}{} 
  = ((\D F)_p)_i^j \frac{\partial}{\partial y^j} \at{F(p)}{}
  = \frac{\partial F^j}{\partial x^i} \at{p}{} \frac{\partial}{\partial y^j} \at{F(p)}{}.
\end{equation}
The element $(\dif{F})_p v_p \in T_{F(p)} \mathbb{R}^m$ is called the \hldefn{pushforward} of the
element $v_p \in T_p \mathbb{R}^n$ by the map $F$.

We can now talk about the pullback of smooth $k$-forms for $k \geq 1$. Given an element
$\eta_{F(p)} \in \Lambda^k(T_{F(p)}^* \mathbb{R}^m)$, we can pull it back by $(\dif{F})_p \in
L(T_p \mathbb{R}^n, T_{F(p)} \mathbb{R}^m)$ to an element $(\dif{F})_p^* \eta_{F(p)} \in
\Lambda^k(T_p^* \mathbb{R}^n)$ as in \cref{eq:normal_pullback_as_guide}, where we let $V = T_p
\mathbb{R}^n$ and $W = T_{F(p)} \mathbb{R}^m$. In other words,
\begin{equation*}
  ((\dif{F})_p^* \eta_{F(p)})((X_1)_p, \ldots, (X_k)_p)
  = \eta_{F(p)} ((\dif{F})_p(X_1)_p, \ldots, (\dif{F})_p(X_k)_p)
\end{equation*}
for all $(X_1)_p, \ldots, (X_k)_p \in T_p \mathbb{R}^n$.

\begin{defn}[Pullback by $F$ of a $k$-Form]\index{Pullback}\label{defn:pullback_by_f_of_a_k_form}
  Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be a smooth map. Let $\eta$ be a $k$-form on
  $\mathbb{R}^m$. The \hlnoteb{pullback by $F$ of $\eta$} is a $k$-form $F^* \eta$ on
  $\mathbb{R}^n$ defined by $(F^* \eta)_p = (\dif{F})_p^* \eta_{F(p)}$. Explicitly so, $F^* \eta$
  is the $k$-form on $\mathbb{R}^n$ defined by
  \begin{equation*}
    (F^* \eta)_p ((X_1)_p, \ldots, (X_k)_p)
    = \eta_{F(p)} ((\dif{F})_p(X_1)_p, \ldots, (\dif{F})_p(X_k)_p).
  \end{equation*}
\end{defn}

\begin{propo}[Pullbacks Preserve Smoothness]\label{propo:pullbacks_preserve_smoothness}
  The pullback by a smooth map $F : \mathbb{R}^n \to \mathbb{R}^m$ takes smooth $k$-forms to
  smooth $k$-forms, i.e. if $\eta \in \Omega^k(\mathbb{R}^m)$, then $F^* \eta \in \Omega^k(
  \mathbb{R}^n)$.
\end{propo}

\begin{proof}
  It suffices to show that the functions
  \begin{equation*}
    (F^* \eta)_{j_1, \ldots, j_k} = (F^* \eta) \left( \frac{\partial}{\partial x^{j_1}}, \ldots,
    \frac{\partial}{\partial x^{j_k}} \right)
  \end{equation*}
  are smooth on $\mathbb{R}^n$. By \cref{eq:linear_map_across_tangent_spaces_over_f}, we have
  \begin{align*}
    &(F^* \eta)_p \left( \frac{\partial}{\partial x^{j_1}} \at{p}{}, \ldots,
      \frac{\partial}{\partial x^{j_k}}\at{p}{} \right) \\
    &= \eta_{F(p)} \left( (\dif{F})_p \frac{\partial}{\partial x^{j_1}}\at{p}{}, \ldots,
      (\dif{F})_p \frac{\partial}{\partial x^{j_k}}\at{p}{} \right)
      \enspace \because \text{ definition }\\
    &= \eta_{F(p)} \left( \frac{\partial F^{l_1}}{\partial x^{j_1}}\at{p}{} 
      \frac{\partial}{\partial y^{l_1}}\at{F(p)}{}, \ldots,
      \frac{\partial F^{l_k}}{\partial x^{j_k}}\at{p}{}
    \frac{\partial}{\partial y^{l_k}}\at{F(p)}{} \right)
      \enspace \because \text{ \cref{eq:linear_map_across_tangent_spaces_over_f} } \\
    &= \left( \frac{\partial F^{l_1}}{\partial x^{j_1}}\at{p}{} \hdots
      \frac{\partial F^{l_k}}{\partial x^{j_k}}\at{p}{} \right) \eta_{F(p)}
      \left( \frac{\partial}{\partial y^{l_1}}\at{F(p)}{}, \ldots,
      \frac{\partial}{\partial y^{l_k}}\at{F(p)}{} \right)
      \because \text{ linearity } \\
    &= \left( \frac{\partial F^{l_1}}{\partial x^{j_1}} \hdots 
      \frac{\partial F^{l_k}}{\partial x^{j_k}} \right)(p) \cdot \eta \left( 
      \frac{\partial}{\partial y^{l_1}}, \ldots, \frac{\partial}{\partial xy^{l_k}}
      \right)(F(p)) \because \text{ rewrite } \\
    &= \left( \frac{\partial F^{l_1}}{\partial x^{j_1}} \hdots
      \frac{\partial F^{l_k}}{\partial x^{j_k}} ( \eta_{l_1, \ldots, l_k} \circ F )
      \right)(p) \enspace \because \text{ product of functions }
  \end{align*}
  Since $p \in \mathbb{R}^n$ was arbitrary, we have
  \begin{equation*}
    (F^* \eta)_{j_1, \ldots, j_k} = \frac{\partial F^{l_1}}{\partial x^{j_1}} \hdots
      \frac{\partial F^{l_k}}{\partial x^{j_k}} (\eta_{l_1, \ldots, l_k} \circ F).
  \end{equation*}
  By assumption, we have that $\eta$ is smooth, and so since $F$ is always assumed to be smooth,
  we have that $(F^* \eta)_{j_1, \ldots, j_k}$ is smooth, as required.
\end{proof}

\begin{propo}[Different Linearities of The Pullback]\label{propo:different_linearities_of_the_pullback}
  Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be smooth. Let $k, l \geq 1$. Let $\eta, \zeta \in
  \Omega^k(\mathbb{R}^m)$, $\rho \in \Omega^l(\mathbb{R}^m)$, and let $a, b \in \mathbb{R}$. Then
  \begin{equation}\label{eq:different_linearities_of_the_pullback}
    F^* (a\eta + b\zeta) = aF^* \eta + bF^* \zeta, \quad
    F^*(\eta \land \rho) = (F^* \eta) \land (F^* \rho).
  \end{equation}
\end{propo}

\begin{proof}
  The proof for this follows almost immediately from \cref{propo:properties_of_the_pullback}.
  (See A1Q8)
\end{proof}

% section pullback_of_smooth_forms (end)

% chapter lecture_13_feb_04th (end)

\chapter{Lecture 14 Feb 08th}%
\label{chp:lecture_14_feb_08th}
% chapter lecture_14_feb_08th

\section{Pullback of Smooth Forms (Continued)}%
\label{sec:pullback_of_smooth_forms_continued}
% section pullback_of_smooth_forms_continued

Up to this point, notice that our discussions have mostly been about $k \geq 1$. Notice
that for $k = 0$, the \hlnotea{smooth $0$-forms} are just smooth functions. It follows
that if the pullback by a smooth map $F : \mathbb{R}^n \to \mathbb{R}^m$ will map from
$\Omega^0(\mathbb{R}^m)$ to $\Omega^0(\mathbb{R}^n)$, it is sensible that the definition
of $F^* h = h \circ F$ for any $h \in \Omega^0(\mathbb{R}^m) = C^\infty(\mathbb{R}^m)$.

It goes without saying that $F^* h \in \Omega^0(\mathbb{R}^n) = C^\infty(\mathbb{R}^n)$.

\begin{defn}[Pullback of $0$-forms]\index{Pullback of $0$-forms}\label{defn:pullback_of_0_forms}
  Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be smooth. Let $h \in \Omega^0(\mathbb{R}^m)$.
  Then we define
  \begin{equation}\label{eq:pullback_of_0_forms}
    F^* h = h \circ F \in \Omega^0(\mathbb{R}^n).
  \end{equation}
\end{defn}

\begin{lemma}[Linearity of the Pullback over the $0$-form that is a Scalar]\label{lemma:linearity_of_the_pullback_over_the_0_form_that_is_a_scalar}
  Let $k \geq 1$. Let $h \in \Omega^0(\mathbb{R}^m)$ and $\eta \in
  \Omega^k(\mathbb{R}^m)$. Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be smooth. The
  \begin{equation*}
    F^* (h \eta) = (F^* h)(F^* \eta).
  \end{equation*}
\end{lemma}

\begin{proof}
  Recall from \cref{eq:k_form_with_scalar}, we had $(h \eta)_q = h(q) \eta_q$ for any $q
  \in \mathbb{R}^m$. It follows that
  \begin{align*}
    (F^* (h\eta))_p &= (\dif{F})_p^*(h\eta)_{F(p)} = (\dif{F})_p^*(h(F(p))\eta_{F(p)}) \\
                    &= h(F(p)) (\dif{F})_p^* (\eta_{F(p)}) \\
                    &= (h \circ F)(p) (F^* \eta)_p \\
                    &= ((F^* h)(F^* \eta))(p).
  \end{align*}
  Thus we have $F^* (h \eta) = (F^* h)(F^* \eta)$.
\end{proof}

This motivates the following definition.

\begin{defn}[Wedge Product of a $0$-form and $k$-form]\label{defn:wedge_product_of_a_0_form_and_k_form}
  Let $h \in \Omega^(\mathbb{R}^m)$ and $\eta \in \Omega^k(\mathbb{R}^m)$, where $k \geq
  1$. We define
  \begin{equation*}
    h \land \eta = h \eta.
  \end{equation*}
\end{defn}

\begin{note}
  This definition is consistent with the identity $\alpha \land \beta = (-1)^{\abs{\alpha}
  \abs{\beta}} \beta \land \alpha$, since the degree of $h$ is $0$, and so it commutes
  with all forms.
\end{note}

\begin{crly}[General Linearity of the Pullback]\label{crly:general_linearity_of_the_pullback}
  Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be smooth. Let $k, l \geq 0$. Let $\eta, \xi \in
  \Omega^k(\mathbb{R}^m)$, $\rho \in \Omega^l(\mathbb{R}^m)$, and let $a, b \in
  \mathbb{R}$. Then
  \begin{equation*}
    F^* (a \eta + b \xi) = a F^* \eta + b F^* \xi \quad F^* (\eta \land \rho) = (F^* \eta)
    \land (F^* \rho).
  \end{equation*}
\end{crly}

\begin{proof}
  If $k, l > 0$, the statement is simply
  \cref{propo:different_linearities_of_the_pullback}. If either one or both of $k, l$ are
  $0$, then the wedge product case follows from
  \cref{lemma:linearity_of_the_pullback_over_the_0_form_that_is_a_scalar}, while the other
  follows from the properties
  \begin{equation*}
    (a h + b g) \circ F = a(h \circ F) + b(g \circ F)
  \end{equation*}
  and
  \begin{equation*}
    (hg) \circ F = (h \circ F)(g \circ F),
  \end{equation*}
  for any $g, h \in C^\infty(\mathbb{R}^m)$.
\end{proof}

\newthought{Before we} begin considering examples, let us derive an explicit formula for
the pullback.

\begin{note}
  Consider the pullback of the standard $1$-forms $\dif{y^1}, \ldots, \dif{y^m}$ on
  $\mathbb{R}^m$. Then for $F : \mathbb{R}^n \to \mathbb{R}^m$, $F^* \dif{y^j}$ is a smooth
  $1$-form on $\mathbb{R}^n$, and it can hence be written as
  \begin{equation*}
    F^* \dif{y^j} = A_i^j \dif{x^i}
  \end{equation*}
  for some smooth function $A_i^j$ on $\mathbb{R}^n$. Observe that
  \begin{equation*}
    (F^* \dif{y^j})_p \left( \frac{\partial}{\partial x^l} \at{p}{} \right)
    = A_i^j (p) \dif{x^i} \at{p}{} \left( \frac{\partial}{\partial x^l} \at{p}{} \right)
    = A_i^j(p) \delta_l^i = A_l^j(p).
  \end{equation*}
  By the definition of the pullback, we also have that
  \begin{align*}
    (F^* \dif{y^j})_p \left( \frac{\partial}{\partial x^l} \at{p}{} \right)
    &= \dif{y^l} \at{F(p)}{} \left( (\dif{F})_p \frac{\partial}{\partial x^l} \at{p}{}
      \right) \\
    &= \dif{y^j} \at{F(p)}{} \left( \frac{\partial F^i}{\partial x^l}\at{p}{}
      \frac{\partial}{\partial y^i} \at{F(p)}{} \right) \\
    &= \frac{\partial F^i}{\partial x^l} \at{p}{} \dif{y^j} \at{F(p)}{} \left(\partial y^i
      \frac{\partial}{\partial y^i} \at{F(p)}{} \right) \\
    &= \frac{\partial F^i}{\partial x^l} \at{p}{} \delta_i^j = \frac{\partial F^j}{\partial
      x^l} \at{p}{}.
  \end{align*}
  It follows that $A_l^j(p) = \frac{\partial F^j}{\partial x^l} \at{p}{}$ for all $p \in
  \mathbb{R}^n$, which implies $A_l^j = \frac{\partial F^j}{\partial x^l}$. Therefore, we
  have that
  \begin{equation}\label{eq:explicit_formula_of_standard_1_forms}
    F^* \dif{y^j} = \frac{\partial F^j}{\partial x^i} \dif{x^i}.
  \end{equation}
\end{note}

Following \cref{crly:general_linearity_of_the_pullback} and
\cref{eq:explicit_formula_of_standard_1_forms}, we have the following proposition.

\begin{propo}[Explicit Formula for the Pullback of Smooth $1$-forms]\label{propo:explicit_formula_for_the_pullback_of_smooth_1_forms}
  Let $\alpha = \alpha_j \dif{y^j}$ be a smooth $1$-form on $\mathbb{R}^m$, and let $F :
  \mathbb{R}^n \to \mathbb{R}^m$ be smooth. Then $F^* \alpha$ is the smooth $1$-form
  \begin{equation*}
    F^* \alpha = (\alpha_j \circ F) \frac{\partial F^j}{\partial x^i} \dif{x^i}.
  \end{equation*}
\end{propo}

\begin{crly}[Commutativity of the Pullback and the Exterior Derivative on Smooth $0$-forms]\label{crly:commutativity_of_the_pullback_and_the_exterior_derivative_on_smooth_0_forms}
  Let $F : \mathbb{R}^n \to \mathbb{R}^m$ be smooth. Let $h \in C^\infty(\mathbb{R}^m)$.
  Then $\dif{h} \in \Omega^1(\mathbb{R}^m)$ and $F^*(\dif{h}) \in \Omega^1(\mathbb{R}^n)$,
  In fact,
  \begin{equation*}
    F^* (\dif{h}) = \dif{(h \circ F)} = \dif{F^* h}.
  \end{equation*}
\end{crly}

\begin{proof}
  By \cref{eq:differential_of_f_from_multivar_calc} with $f = h \circ F$, we get
  \begin{equation*}
    \dif{(h \circ F)} = \left( \frac{\partial}{\partial x^i} (h \circ F) \right)
    \dif{x^i}.
  \end{equation*}
  Using \cref{eq:explicit_formula_of_standard_1_forms} and the chain rule, we have
  \begin{equation*}
    \dif{(h \circ F)} = \left( \frac{\partial h}{\partial y^j} \circ F \right)
    \frac{\partial F^j}{\partial x^i} \dif{x^i} = \left( \frac{\partial h}{\partial y^j}
    \circ F \right) F^* \dif{y^j}.
  \end{equation*}
  Also, we have $\dif{h} = \frac{\partial h}{\partial y^j} \dif{y^j}$. Then
  \begin{equation*}
    F^*(\dif{h}) = F^* \left(\frac{\partial h}{\partial d^j} \dif{y^j}\right) = \left(
    \frac{\partial h}{\partial y^j} \circ F \right) F^* \dif{y^j}
  \end{equation*}
  by \cref{propo:explicit_formula_for_the_pullback_of_smooth_1_forms}. It follows that
  $\dif{F^* h} = F^* \dif{h}$, as claimed.
\end{proof}

We will make explicit the operation $d$ on $k$-forms for any $k$ in the coming section. We
will see that
\cref{crly:commutativity_of_the_pullback_and_the_exterior_derivative_on_smooth_0_forms}
works even in the general case (see proposition later).

\begin{note}[More abuses of notation]
  Let $y = F(x)$. Let us employ the usual abuse of notation and identify a function with
  its output. In particular, since we write $y^j = F^j(x^1, \ldots, x^n)$, let us write
  $\frac{\partial y^j}{\partial x^l}$ for $\frac{\partial F^j}{\partial x^l}$. Then
  \cref{eq:explicit_formula_of_standard_1_forms} becomes
  \begin{equation}\label{eq:abused_explicit_formula_of_standard_1_forms}
    F^* \dif{y^j} = \frac{\partial y^j}{\partial x^l} \dif{x^l}.
  \end{equation}

  \noindent
  \hlbnoteb{Method to remember \cref{eq:abused_explicit_formula_of_standard_1_forms}} The
  smooth map $F : \mathbb{R}^n \to \mathbb{R}^m$ allows us to think of the $y^j$'s as
  smooth functions of the $x^i$'s, and
  \cref{eq:abused_explicit_formula_of_standard_1_forms} expresses the differential in the
  same sense as \cref{eq:differential_of_f_from_multivar_calc} for the smooth functions
  $y^j = y^j(x^1, \ldots, x^n)$ in terms of the $\dif{x^i}$'s.
\end{note}

We will use this abuse of notation frequently in this course. For instance, it allows us
to express the general formula for the pullback as follows: for 
\begin{equation*}
  \eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k} (y) \dif{y^{j_1}} \land \hdots \land
  \dif{y^{j_k}},
\end{equation*}
we have
\begin{equation*}
  F^* \eta = \frac{1}{k!} \eta_{j_1, \ldots, j_k}(y(x)) \frac{\partial y^{j_1}}{\partial
    x^{l_1}} \hdots \frac{\partial y^{j_k}}{\partial x^{l_k}} \dif{x^{l_1}} \land \hdots
    \land \dif{x^{l_k}}.
\end{equation*}

\begin{eg}
  Consider the map $F : \mathbb{R}^3 \to \mathbb{R}^3$, given by $(\rho, \phi, \theta)
  \mapsto (x, y, z)$, where
  \begin{equation*}
    x = \rho \sin \phi \cos \theta, \enspace y = \rho \sin \phi \sin \theta, \text{ and }
    z = \rho \cos \phi.
  \end{equation*}
  Then
  \begin{align*}
    F^*(\dif{x}) = \dif{(F^* x)} &= \left( \frac{\partial x}{\partial \rho} \dif{\rho} +
      \frac{\partial x}{\partial \phi} \dif{\phi} + \frac{\partial x}{\partial \theta}
      \dif{\theta} \right) \\
                                 &= \sin \phi \cos \theta \dif{\rho} + \rho \cos \phi \cos
      \theta \dif{\phi} - \rho \sin \phi \sin \theta \dif{\theta}.
  \end{align*}
  Similarly, we have
  \begin{align*}
    F^*(\dif{y}) = \dif{(F^* y)} &= \left( \frac{\partial y}{\partial \rho} \dif{\rho} +
      \frac{\partial y}{\partial \phi} \dif{\phi} + \frac{\partial y}{\partial \theta}
      \dif{\theta} \right) \\
                                 &= \sin \phi \sin \theta \dif{\rho} + \rho \cos \phi \sin
      \theta \dif{\phi} + \rho \sin \phi \sin \theta \dif{\theta}
  \end{align*}
  and
  \begin{align*}
    F^*(\dif{z}) = \dif{(F^* z)} &= \left( \frac{\partial z}{\partial \rho} \dif{\rho} +
      \frac{\partial z}{\partial \phi} \dif{\phi} + \frac{\partial z}{\partial \theta}
      \dif{\theta} \right) \\
                                 &= \cos \phi \dif{\rho} - \rho \sin \phi \dif{\phi}.
  \end{align*}
  It follows that
  \begin{align*}
    F^* (\dif{x} \land \dif{y} \land \dif{z})
    &= (F^* \dif{x}) \land (F* \dif{y}) \land (F^* \dif{z}) \\
    &= (\sin \phi \cos \theta \dif{\rho} + \rho \cos \phi \cos \theta \dif{\phi} - \rho
      \sin \phi \sin \theta \dif{\theta}) \land \\
    &\qquad (\sin \phi \sin \theta \dif{\rho} + \rho \cos \phi \sin \theta \dif{\phi} +
      \rho \sin \phi \cos \theta \dif{\theta}) \land \\
    &\qquad (\cos \phi \dif{\rho} - \rho \sin \phi \dif{\phi}) \\
    &= (\dif{\rho} \land \dif{\phi} \land \dif{\theta})(\rho^2 \sin^3 \phi \cos^2 \theta +
      \rho^2 \sin^3 \phi \sin^2 \theta) \\
    &\qquad + (\dif{\rho} \land \dif{\phi} \land \dif{\theta})(\rho^2 \sin \phi \cos^2
      \phi \cos^2 \theta + \\
    &\qquad \qquad \rho^2 \sin \phi \cos^2 \phi \sin^2 \theta) \\
    &= (\rho^2 \sin \phi)(\dif{\rho} \land \dif{\phi} \land \dif{\theta}).
  \end{align*}
  Recall that this formula relates the `volume form' $\dif{x} \land \dif{y} \land \dif{z}$
  of $\mathbb{R}^3$ in Cartesian coordinates to the `volume form' $\rho^2 \sin \phi
  \dif{\rho} \land \dif{\phi} \land \dif{\theta}$ in spherical coordinates. We will see
  this again much later in the couse.
\end{eg}

% section pullback_of_smooth_forms_continued (end)

% chapter lecture_14_feb_08th (end)

\appendix

\chapter{Review of Earlier Contents}%
\label{chp:review_of_earlier_contents}
% chapter review_of_earlier_contents

\section{Rank-Nullity Theorem}%
\label{sec:rank_nullity_theorem}
% section rank_nullity_theorem

\nocite{stephen2002}

\begin{defn}[Kernel and Image]\index{Kernel}\index{Image}\label{defn:kernel_and_image}
  Let $V$ and $W$ be vector spaces, and let $T \in L(V, W)$.
  The \hlnoteb{kernel} (or \hldefn{null space}) of $T$ is defined as
  \begin{equation*}
    \ker(T) := \left\{ v \in V \mmid Tv = 0 \right\},
  \end{equation*}
  i.e. the set of vectors in $V$ such that they are mapped to $0$ under $T$.

  The \hlnoteb{image} (or \hldefn{range}) of $T$ is defined as
  \begin{equation*}
    \Img(T) = \left\{ Tv \mmid v \in V \right\},
  \end{equation*}
  that is the set of all images of vectors of $V$ under $T$.
\end{defn}

It can be shown that for a linear map $T \in L(V, W)$,
$\ker (T)$ and $\Img(T)$ are subspaces of $V$ and $W$, respectively.
As such, we can define the following:

\begin{defn}[Rank and Nullity]\index{Rank}\index{Nullity}\label{defn:rank_and_nullity}
  Let $V, W$ be vector spaces, and let $T \in L(V, W)$.
  If $\ker(T)$ and $\Img(T)$ are finite-dimensional
  \sidenote{In this course, this is always the case, since we are only dealing with finite dimensional real vector spaces.},
  then we define the \hlnoteb{nullity} of $T$ as
  \begin{equation*}
    \nullity(T) := \dim \ker (T),
  \end{equation*}
  and the \hlnoteb{rank} of $T$ as
  \begin{equation*}
    \rank(T) := \dim \Img(T).
  \end{equation*}
\end{defn}

\begin{note}
  From the action of a linear transformation,
  we observe that the \hlnotec{larger the nullity, the smaller the rank}.
  Put in another way, the more vectors are sent to $0$ by the linear transformation,
  the smaller the range.

  Similarly, the larger the rank, the smaller the nullity.
\end{note}

This observation gives us the Rank-Nullity Theorem.

\begin{thm}[Rank-Nullity Theorem]\index{Rank-Nullity Theorem}\label{thm:rank_nullity_theorem}
  Let $V$ and $W$ be vector spaces, and $T \in L(V, W)$. If $V$ is finie-dimensional, then
  \begin{equation*}
    \nullity(T) + \rank(T) = \dim (V).
  \end{equation*}
\end{thm}

From the Rank-Nullity Theorem,
we can make the following observations about the relationships
between injection and surjection, and the nullity and rank.

\begin{propo}[Nullity of Only $0$ and Injectivity]\label{propo:nullity_of_only_0_and_injectivity}
  Let $V$ and $W$ be vector spaces, and $T \in L(V, W)$.
  Then $T$ is injective iff $\nullity(T) = \left\{ 0 \right\}$.
\end{propo}

Surjection and injectivity come hand-in-hand when
we have the following special case.

\begin{propo}[When Rank Equals The Dimension of the Space]\label{propo:when_rank_equals_the_dimension_of_the_space}
  Let $V$ and $W$ be vector spaces of equal (finite) dimension,
  and let $T \in L(V, W)$. TFAE
  \begin{enumerate}
    \item $T$ is injective;
    \item $T$ is surjective;
    \item $\rank(T) = \dim(V)$.
  \end{enumerate}
\end{propo}

Note that the proof for \cref{propo:when_rank_equals_the_dimension_of_the_space}
requires the understanding that $\ker(T) = \{ 0 \}$ implies that $\nullity(T) = 0$.
See \href{https://math.stackexchange.com/questions/664594/why-mathbf0-has-dimension-zero}{this explanation on Math SE}.

% section rank_nullity_theorem (end)

% chapter review_of_earlier_contents (end)

\backmatter

\pagestyle{plain}

\nobibliography*
\bibliography{references}

\printindex

\end{document}

