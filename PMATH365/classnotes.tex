% !TEX TS-program = pdflatex
\documentclass[notoc,notitlepage]{tufte-book}
% \nonstopmode % uncomment to enable nonstopmode

\input{latex-classnotes-preamble.tex}

\usepackage{classnotetitle}
\usepackage{tikz-3dplot}

\title{PMATH365 --- Differential Geometry}
\author{Johnson Ng}
\subtitle{Classnotes for Winter 2019}
\credentials{BMath (Hons), Pure Mathematics major, Actuarial Science Minor}
\institution{University of Waterloo}

\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\id}{id}

\begin{document}
\input{latex-classnotes-header.tex}

\chapter*{Preface}%
\addcontentsline{toc}{chapter}{Preface}
\label{chp:preface}
% chapter preface

\nocite{karigiannis2019}

This course is a post-requisite of MATH 235/245 (\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-MATH.html\#MATH235}{Linear Algebra II}) and AMATH 231 (\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-AMATH.html#AMATH231}{Calculus IV}) or MATH 247 (\href{http://www.ucalendar.uwaterloo.ca/1819/COURSE/course-MATH.html#MATH247}{Advanced Calculus III}). In other words, familiarity with vector spaces and calculus is expected.

The course is spiritually separated into two parts. The first part shall be called \textbf{Exterior Differential Calculus}, which allows for a natural, metric-independent generalization of \hlnotea{Stokes' Theorem}, \hlnotea{Gauss's Theorem}, and \hlnotea{Green's Theorem}. Our end goal of this part is to arrive at Stokes' Theorem, that renders the \hlnotea{Fundamental Theorem of Calculus} as a special case of the theorem.

The second part of the course shall be called in the name of the course: \textbf{Differential Geometry}. This part is dedicated to studying geometry using techniques from differential calculus, integral calculus, linear algebra, and multilinear algebra.

% chapter preface (end)

\newgeometry{letterpaper}
\part{Exterior Differential Calculus}
\restoregeometry

\chapter{Lecture 1 Jan 07th}%
\label{chp:lecture_1_jan_07th}
% chapter lecture_1_jan_07th

\section{Linear Algebra Review}%
\label{sec:linear_algebra_review}
% section linear_algebra_review

\begin{defn}[Linear Map]\index{Linear Map}\label{defn:linear_map}
  Let $V, W$ be finite dimensional real vector spaces. A map $T : V \to W$ is called \hlnoteb{linear} if $\forall a, b \in \mathbb{R}$, $\forall v \in V$ and $\forall w \in W$,
  \begin{equation*}
    T(av + bw) = aT(v) + bT(w).
  \end{equation*}
  We define L(U, W) to be the set of all linear maps from $V$ to $W$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item Note that $L(U, W)$ is itself a finite dimensional real vector space.
    \item The structure of the vector space $L(V, W)$ is such that $\forall T, S \in L(V, W)$, and $\forall a, b \in \mathbb{R}$, we have
      \begin{equation*}
        aT + bS : V \to W
      \end{equation*}
      and
      \begin{equation*}
        (aT + bS)(v) = aT(v) + bS(v).
      \end{equation*}
    \item A special case: when $W = V$, we usually write
      \begin{equation*}
        L(V, W) = L(V),
      \end{equation*}
      and we call this the \hldefn{space of linear operators on $V$}.
  \end{itemize}
\end{note}

Now suppose $\dim(V) = n$ for some $n \in \mathbb{N}$. This means that there exists a basis $\{ e_1 , \ldots, e_n \}$ of $V$ with $n$ elements.

\begin{defn}[Basis]\index{Basis}\label{defn:basis}
  A basis $\mathcal{B} = \{ e_1, \ldots, e_n \}$ of an $n$-dimensional vector space $V$ is a subset of $V$ where
  \begin{enumerate}
    \item $\mathcal{B}$ \hlnotea{spans} $V$, i.e. $\forall v \in V$
      \begin{equation*}
        v = \sum_{i=1}^{n} v^i e_i. \quad \footnotemark
      \end{equation*}
      \footnotetext{We shall use a different convention when we write a linear combination. In particular, we use $v^i$ to represent the $i$\textsuperscript{th} coefficient of the linear combination instead of $v_i$. Note that this should not be confused with taking powers, and should be clear from the context of the discussion.}
    \item $e_1, \ldots, e_n$ are linearly independent, i.e.
      \begin{equation*}
        v^i e_i = 0 \implies v^i = 0 \text{ for every } i.
      \end{equation*}
  \end{enumerate}
\end{defn}

\begin{note}
  We shall abusively write
  \begin{equation*}
    v^i e_i = \sum_{i} v^i e_i.
  \end{equation*}
  Again, this should be clear from the context of the discussion.
\end{note}

The two conditions that define a basis implies that any $v \in V$ can be expressed as $v^i e_i$, where $v^i \in \mathbb{R}$.

\begin{defn}[Coordinate Vector]\index{Coordinate Vector}\label{defn:coordinate_vector}
  The $n$-tuple $(v^1, \ldots, v^n) \in \mathbb{R}^n$ is called the \hlnoteb{coordinate vector} $[v]_{\mathcal{B}} \in \mathbb{R}^n$ of $v$ with respect to the basis $\mathcal{B} = \{ e_1, \ldots, e_n \}$.
\end{defn}

\begin{note}
  It is clear that the coordinate vector $[v]_{\mathcal{B}}$ is dependent on the basis $\mathcal{B}$. Note that we shall also assume that the basis is ``ordered'', which is somewhat important since the same basis (set-wise) with a different ``ordering'' may give us a completely different coordinate vector.
\end{note}

\begin{eg}
  Let $V = \mathbb{R}^n$, and $\hat{e}_i = ( 0, \ldots, 0, 1, 0, \ldots, 0 )$, where $1$ is the $i$\textsuperscript{th} compoenent of $\hat{e}_1$. Then
  \begin{equation*}
    \mathcal{B}_{\std} = \left\{ \hat{e}_1, \ldots, \hat{e}_n \right\}
  \end{equation*}
  is called the \hldefn{standard basis} of $\mathbb{R}^n$.
\end{eg}

\begin{note}
  Let $v = (v^1, \ldots, v^n) \in \mathbb{R}^n$. Then
  \begin{equation*}
    v = v^1 \hat{e}_1 + \hdots v^n \hat{e}_n.
  \end{equation*}
  So $\mathbb{R}^n \ni [v]_{\mathcal{B}_{\std}} = v \in V = \mathbb{R}^n$.

  This is a privilege enjoyed by the $n$-dimensional vector space $\mathbb{R}^n$.
\end{note}

Now if we choose a \hldefn{non-standard basis} of $\mathbb{R}^n$, say $\tilde{\mathcal{B}}$, then $[v]_{\tilde{\mathcal{B}}} \neq v$.

\begin{note}
  It does not make sense to ask if a standard basis exists for an arbitrary space, as we have seen above. A geometrical way of wrestling with this notion is as follows:

  \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      \draw[->] (0, 0, 0) -- (2, 0, 0) node[right] {$x$};
      \draw[->] (0, 0, 0) -- (0, 2, 0) node[above] {$z$};
      \draw[->] (0, 0, 0) -- (0, 0, 2) node[below,left] {$y$};

      \draw[-,fill=be-blue,fill opacity=0.5] (1.5, 0, 0.5) -- (0.5, 0, 1.5) -- (-1.5, 1, -0.5) -- (-0.5, 1, -1.5) -- (1.5, 0, 0.5);
      \node[above,right] at (1.5, 1.5, 0) {a $2$-dimensional subspace in $\mathbb{R}^3$};
    \end{tikzpicture}
    \caption{An arbitrary $2$-dimensional subspace in a $3$-dimensional space}
    \label{fig:an_arbitrary_2-dimensional_subspace_in_a_3-dimensional_space}
  \end{figure}

  While the subspace is embedding in a vector space of which has a standard basis, we cannot establish a ``standard'' basis for this $2$-dimensional subspace. In laymen terms, we cannot tell which direction is up or down, positive or negative for the subspace, without making assumptions.
\end{note}

However, since we are still in a finite-dimensional vector space, we can still make a connection to a Euclidean space of the same dimension.

\begin{defn}[Linear Isomorphism]\index{Linear Isomorphism}\label{defn:linear_isomorphism}
  Let $V$ be $n$-dimensional, and $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ be some basis of $V$. The map
  \begin{equation*}
    v = v^i e_i \mapsto [v]_{\mathcal{B}}
  \end{equation*}
  from $V$ to $\mathbb{R}^n$ is a \hlnoteb{linear isomorphism} of vector spaces.
\end{defn}

\begin{ex}
  Prove that the said linear isomorphism is indeed linear and bijective\sidenote{i.e. we are right in calling it linear and being an isomorphism}.
\end{ex}

\begin{note}
  Any $n$-dimensional real vecotr space is isomorphic to $\mathbb{R}^n$, but not \hlnotea{canonically} so, as it requires the knowledge of the basis that is arbitrarily chosen. In other words, a different set of basis would give us a different isomorphism.
\end{note}

% section linear_algebra_review (end)

\section{Orientation}%
\label{sec:orientation}
% section orientation

Consider an $n$-dimensional vector space $V$. Recall that for any linear operator $T \in L(V)$, we may associate a real number $\det(T)$, called the \hldefn{determinant} of $T$, such that $T$ is said to be \hldefn{invertible} iff $\det(T) \neq 0$.

\begin{defn}[Same and Opposite Orientations]\index{Same orientation}\index{Opposite orientation}\label{defn:same_and_opposite_orientations}
  Let
  \begin{equation*}
    \mathcal{B} = \left\{ e_1, \ldots, e_n \right\} \quad \text{ and } \quad \tilde{\mathcal{B}} = \left\{ \tilde{e}_1, \ldots, \tilde{e}_n \right\}
  \end{equation*}
  be two ordered bases of $V$. Let $T \in L(V)$ be the linear operator defined by
  \begin{equation*}
    T(e_i) = \tilde{e}_i
  \end{equation*}
  for each $i = 1, 2, \ldots, n$. This mapping is clearly invertible, and so $\det(T) \neq 0$, and $T^{-1}$ is also linear, such that $T^{-1} \left( \tilde{e}_i \right) = e_i$, for each $i$.

  We say that $\mathcal{B}$ and $\tilde{\mathcal{B}}$ determine the \hlnoteb{same orientation} if $\det(T) > 0$, and we say that they determine the \hlnoteb{opposite orientations} if $\det(T) < 0$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item This notion of orientation only works in real vector spaces,
      as, for instance, in a complex vector space,
      there is no sense of ``positivity'' or ``negativity''.
    \item Whenever we talk about same and opposite orientation(s),
      we are usually talking about 2 sets of bases.
      It makes sense to make a comparison to the standard basis in a Euclidean space,
      and determine that the compared (non-)standard basis is
      ``positive'' (same direction) or ``negative'' (opposite),
      but, again, in an arbitrary space, we do not have this convenience.
  \end{itemize}
\end{note}

\begin{ex}[A1Q1]
  Show that any $n$-dimensional real vector space $V$ admits exactly 2 orientations.
\end{ex}

\begin{eg}
  On $\mathbb{R}^n$, consider the standard basis
  \begin{equation*}
    \mathcal{B}_{\std} = \{ \hat{e}_1, \ldots, \hat{e}_n \}.
  \end{equation*}
  The orientation determined by $\mathcal{B}_{\std}$ is called the \hldefn{standard orientation} of $\mathbb{R}^n$.
\end{eg}

% section orientation (end)

\section{Dual Space}%
\label{sec:dual_space}
% section dual_space

\begin{defn}[Dual Space]\index{Dual Space}\label{defn:dual_space}
  Let $V$ be an $n$-dimensional vector space.
  Then $\mathbb{R}$ is a $1$-dimensional real vector space.
  Thus we have that $L(V, \mathbb{R})$ is also a real vector space
  \sidenote{Note that $L(V, \mathbb{R})$ is also finite dimensional since both the domain and codomain are finite dimensional.}.
  The \hlnoteb{dual space} $V^*$ of $V$ is defined to be
  \begin{equation*}
    V^* := L(V, \mathbb{R}).
  \end{equation*}
\end{defn}

Let $\mathcal{B}$ be a basis of $V$. For all $i = 1, 2, \ldots, n$, let $e^i \in V^*$ such that
\begin{equation*}
  e^i(e_j) = \delta^i_j = \begin{cases}
    1 & i = j \\
    0 & i \neq j
  \end{cases}.
\end{equation*}
This $\delta^i_j$ is known as the \hldefn{Kronecker Delta}.

In general, we have that for every $v = v^j e_j \in V$, where $v^i \in \mathbb{R}$,
by the linearity of $e^i$, we have
\begin{equation*}
  e^i(v) = e^i(v^j e_j) = v^j e^i(e_j) = v_j \delta^i_j = v^i.
\end{equation*}
So each of the $e^i$, when applied on $v$,
gives us the $i$\textsuperscript{th} component of $[v]_{\mathcal{B}}$,
where $\mathcal{B}$ is a basis of $V$.

% section dual_space (end)

% chapter lecture_1_jan_07th (end)

\chapter{Lecture 2 Jan 09th}%
\label{chp:lecture_2_jan_09th}
% chapter lecture_2_jan_09th

\section{Dual Space (Continued)}%
\label{sec:dual_space_continued}
% section dual_space_continued

\begin{propo}[Dual Basis]\index{Dual Basis}\label{propo:dual_basis}
  The set
  \begin{equation*}
    \mathcal{B}^* := \left\{ e^1, \ldots, e^n \right\}
  \end{equation*}
  \sidenote{Note that the $e^i$'s are defined as in the last part of the last lecture.}
  is a basis of $V^*$, and is called the \hlnoteb{dual basis} of $\mathcal{B}$,
  where $\mathcal{B}$ is a basis of $V$.
  In particular, $\dim V^* = n = \dim V$.
\end{propo}

\begin{proof}
  \hlbnoted{$\mathcal{B}^*$ spans $V^*$}
  Let $\alpha \in V^*$. Let $v = v^j e_j \in V$,
  where we note that
  \begin{equation*}
    \mathcal{B} = \left\{ e_i \right\}_{i = 1}^{n}.
  \end{equation*}
  We have that
  \begin{equation*}
    \alpha(v) = \alpha(v^j e_j) = v^j \alpha( e_j ).
  \end{equation*}
  Now for all $j = 1, 2, \ldots, n$, define $\alpha_j = \alpha(e_j)$.
  Then
  \begin{equation*}
    \alpha(v) = \alpha_j v^j = \alpha_j e^j(v),
  \end{equation*}
  which holds for all $v \in V$. This implies that $\alpha = \alpha_j e^j$,
  and so $\mathcal{B}^*$ spans $V^*$.

  \noindent
  \hlbnoted{$\mathcal{B}^*$ is linearly independent} 
  Suppose $\alpha_j e^j = 0 \in V^*$.
  Applying $\alpha_j e^j$ to each of the vectors $e_k$ in $\mathcal{B}$, we have
  \begin{equation*}
    \alpha_j e^j(e_k) = 0(e_k) = 0 \in \mathbb{R}
  \end{equation*}
  and
  \begin{equation*}
    \alpha_j e^j(e_k) = \alpha_j \delta_k^j = \alpha_k.
  \end{equation*}
  By A1Q2, we have that $a_k = 0$ for all $k = 1, 2, \ldots, n$,
  and so $\mathcal{B}^*$ is linearly independent.\qed\
\end{proof}

\begin{remark}
  Let $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ be a basis of $V$,
  with dual space $\mathcal{B}^* = \left\{ e^1, \ldots, e^n \right\}$.
  Then the map $T : V \to V^*$ such that
  \begin{equation*}
    T(e_i) = e^i
  \end{equation*}
  is a vector space \hyperref[defn:linear_isomorphism]{isomorphism}.
  And so we have that $V \simeq V^*$, but not \hlnotea{cannonically} so
  since we needed to know what the basis is in the first place.
\end{remark}

We will see later that if we impose an \hlnotea{inner product} on $V$,
then it will induce a canonical isomorphism from $V$ to $V^*$.

\begin{defn}[Natural Pairing]\index{Natural Pairing}\label{defn:natural_pairing}
  The function
  \begin{equation*}
    \langle \cdot, \cdot \rangle : V^* \times V \to \mathbb{R}
  \end{equation*}
  given by
  \begin{equation*}
    \langle \alpha, v \rangle \mapsto \alpha (v)
  \end{equation*}
  is called a \hlnoteb{natural pairing} of $V^*$ and $V$.
\end{defn}

\begin{note}
  A natural pairing is bilinear,
  i.e. it is linear in $\alpha$ and linear in $v$,
  which means that
  \begin{equation*}
    \langle \alpha, t_1 v_1 + t_2 v_2 \rangle 
    = t_1 \langle \alpha, v_1 \rangle + t_2 \langle \alpha, v_2 \rangle
  \end{equation*}
  and
  \begin{equation*}
    \langle t_1 \alpha_1 + t_2 \alpha_2, v \rangle
    = t_1 \langle \alpha_1, v \rangle + t_2 \langle \alpha_2, v \rangle,
  \end{equation*}
  respectively.
\end{note}

\begin{propo}[Natural Pairings are Nondegenerate]\label{propo:natural_pairings_are_nondegenerate}
  For a finite dimensional real vector space $V$,
  a natural pairing is said to be nondegenerate if
  \marginnote{This is A1Q2.}
  \begin{equation*}
    \forall v \in V \enspace \langle \alpha, v \rangle = 0 \iff \alpha = 0
  \end{equation*}
  and
  \begin{equation*}
    \forall \alpha \in V^* \enspace \langle \alpha, v \rangle = 0 \iff v = 0.
  \end{equation*}
\end{propo}

\begin{eg}
  Fix a basis $\mathcal{B} = \left\{ e_1, \ldots, e_n \right\}$ of $V$.
  Given $T \in L(V)$,
  there is an associated $n \times n$ matrix $A = [T]_{\mathcal{B}}$ defined by
  \begin{equation*}
    T(e_i) = A_{\underarrow{i}{\text{row index}}}^{\overarrow{j}{\text{column index}}} e_j.
  \end{equation*}
  In particular,
  \begin{equation*}
    A = \overarrow{
      \begin{bmatrix}
        [ T(e_1) ]_{\mathcal{B}} & \hdots & [ T(e_n) ]_{\mathcal{B}}
      \end{bmatrix}
    }{\text{block matrix}}
  \end{equation*}
  and
  \begin{equation*}
    A_i^k = e^k( T(e_i) ) = \langle e^k, T(e_i) \rangle.
  \end{equation*}
\end{eg}

\begin{defn}[Double Dual Space]\index{Double Dual Space}\label{defn:double_dual_space}
  The set
  \begin{equation*}
    V^{**} = L(V^*, \mathbb{R})
  \end{equation*}
  is called the \hlnoteb{double dual space}.
\end{defn}

\begin{propo}[The Space and Its Double Dual Space]\label{propo:the_space_and_its_double_dual_space}
  Let $V$ be a finite dimensional real vector space and $V^{**}$ be its double dual space. There exists a linear map $\xi$ such that
  \begin{equation*}
    \xi : V \to V^{**}
  \end{equation*}
\end{propo}

\begin{proof}
\marginnote{As messy as this may seem, this is really a follow your nose kind of proof.
    
  Since we are proving that a map exists, we need to construct it.
  Since $\xi : V \to V^{**} = L(V^*, \mathbb{R})$, for any $v \in V$,
  we must have $\xi(v)$ as some linear map from $V^*$ to $\mathbb{R}$.}

  Let $v \in V$. Then $\xi(v) \in V^{**} = L(V^*, \mathbb{R})$, 
  i.e. $\xi(v) : V^* \to \mathbb{R}$. Then for any $\alpha \in V^*$,
  \begin{equation*}
    \left( \xi(v) \right)(\alpha) \in \mathbb{R}.
  \end{equation*}
  Since $\alpha \in V^*$, i.e. $\alpha : V \to \mathbb{R}$,
  and $\alpha$ is linear, let us define
  \begin{equation*}
    \xi(v)(\alpha) = \alpha(v).
  \end{equation*}
  To verify that $\xi(v)$ is indeed linear, notice that 
  for any $t, s \in \mathbb{R}$, and for any $\alpha, \beta \in V^*$,
  we have
  \begin{align*}
    \xi(v)(t \alpha + s \beta)
      &= (t \alpha + s \beta) (v) \\
      &= t \alpha(v) + s \beta(v) \\
      &= t \xi(v)(\alpha) + s \xi(v)(\beta).
  \end{align*}
  It remains to show that $\xi$ itself is linear:
  for any $t, s \in \mathbb{R}$, any $v, w \in V$,
  and any $\alpha \in V^*$, we have
  \begin{align*}
    \xi(tv + sw)(\alpha)
      &= \alpha (tv + sw) = t \alpha(v) + s \alpha(w) \\
      &= t \xi(v)(\alpha) + s \xi(v)(\alpha) \\
      &= [ t \xi(v) + s \xi(w) ](\alpha)
  \end{align*}
  by addition of functions.\qed\
\end{proof}

\begin{propo}[Isomorphism Between The Space and Its Dual Space]\label{propo:isomorphism_between_the_space_and_its_dual_space}
  The linear map in \cref{propo:the_space_and_its_double_dual_space} is an isomorphism.
\end{propo}

\begin{proof}
  From \cref{propo:the_space_and_its_double_dual_space}, $\xi$ is linear.
  Let $v \in V$ such that $\xi(v) = 0$, i.e. $v \in \ker(\xi)$.
  Then by the same definition of $\xi$ as above, we have
  \begin{equation*}
    0 = (\xi(v))(\alpha) = \alpha(v)
  \end{equation*}
  for any $\alpha \in V^*$.
  By \cref{propo:natural_pairings_are_nondegenerate}, we must have that $v = 0$,
  i.e. $\ker(\xi) = \{ 0 \}$.
  Thus by \cref{propo:nullity_of_only_0_and_injectivity}, $\xi$ is injective.

  Now, since
  \begin{equation*}
    V^{**} = L(V^*, \mathbb{R}) = L ( L ( V, \mathbb{R} ), \mathbb{R} ),
  \end{equation*}
  we have that
  \begin{equation*}
    \dim(V^{**}) = \dim(V^*) = \dim(V).
  \end{equation*}
  Thus, by the Rank-Nullity Theorem
  \sidenote{See \cref{sec:rank_nullity_theorem}, and especially \cref{propo:when_rank_equals_the_dimension_of_the_space}.},
  we have that $\xi$ is surjective.
  
  \qed\
\end{proof}

The above two proposition shows to use that we may
identify $V$ with $V^{**}$ using $\xi$, and we can
gleefully assume that $V = V^{**}$.

Consequently, if $v \in V = V^{**}$ and $\alpha \in V^*$,
we have
\begin{equation}\label{eq:commutativity_of_the_vector_and_its_linear_operator}
  \alpha(v) = v(\alpha) = \langle \alpha, v \rangle.
\end{equation}

% section dual_space_continued (end)

\section{Dual Map}%
\label{sec:dual_map}
% section dual_map

\begin{defn}[Dual Map]\index{Dual Map}\label{defn:dual_map}
  Let $T \in L(V, W)$, where $V, W$ are finite dimensional real vector spaces. Let
  \begin{equation*}
    T^* : W^* \to V^*
  \end{equation*}
  be defined as follows: for $\beta \in W^*$, we have $T^*(\beta) \in V^*$.
  Let $v \in V$, and so $(T^*(\beta))(v) \in \mathbb{R}$
  \sidenote{It shall be verified here that $T^*(\beta)$ is indeed linear:
  let $v_1, v_2 \in V$ and $c_1, c_2 \in \mathbb{R}$.
  Indeed
  \begin{align*}
    &T^*(\beta)(c_1 v_1 + c_2 v_2) \\
      &= c_1 T^*(\beta)(v_1) + c_2 T^*(\beta)(v_2)
  \end{align*}}.
  From here, we may define
  \begin{equation*}
    (T^*(\beta))(v) = \beta(T(v)).
  \end{equation*}
  The map $T^*$ is called \hlnoteb{the dual map}.
\end{defn}

\begin{ex}
  Prove that $T^* \in L(W^*, V^*)$, i.e. that $T^*$ is linear.
\end{ex}

\begin{proof}
  Let $\beta_1, \beta_2 \in W^*$, $t_1, t_2 \in \mathbb{R}$, and $v \in V$.
  Then
  \begin{align*}
    T^*(t_1 \beta_1 + t_2 \beta_2)(v) 
      &= (t_1 \beta_1 + t_2 \beta_2)(Tv) \\
      &= t_1 \beta_1 (Tv) + t_2 \beta_2 (Tv) \\
      &= t_1 T^*(\beta_1)(v) + t_2 T^*(\beta_2)(v).
  \end{align*}\qed\
\end{proof}

\begin{note}
  Note that in \cref{defn:dual_map}, our construction of $T^*$ is canonical,
  i.e. its construction is independent of the choice of a basis.

  Also, notice that in the language of pairings, we have
  \begin{equation*}
    \langle T^*\beta, v \rangle = (T^*(\beta))(v) = \beta(T(v)) = \langle \beta, T(v) \rangle,
  \end{equation*}
  where we note that
  \begin{gather*}
    T^*(\beta) \in V^* \quad v \in V \\
    \beta \in W^* \quad T(v) \in W.
  \end{gather*}
\end{note}

% section dual_map (end)

% chapter lecture_2_jan_09th (end)

\chapter{Lecture 3 Jan 11th}%
\label{chp:lecture_3_jan_11th}
% chapter lecture_3_jan_11th

\section{Dual Map (Continued)}%
\label{sec:dual_map_continued}
% section dual_map_continued

\begin{note}
  Elements in $V^*$ are also called \hldefn{co-vectors}.
\end{note}

Recall from last lecture that if $T \in L(V, W)$, then it
induces a dual map $T^* \in L(W^*, V^*)$ such that
\begin{equation*}
  (T^* \beta)(v) = \beta (T(v)).
\end{equation*}

\begin{propo}[Identity and Composition of the Dual Map]\label{propo:identity_and_composition_of_the_dual_map}
  Let $V$ and $W$ be finite dimensional real vector spaces.
  \begin{enumerate}
    \item Supppose $V = W$ and $T = I_V \in L(V)$, then
      \begin{equation*}
        (I_V)^* = I_{V^*} \in L(V^*).
      \end{equation*}

    \item Let $T \in L(V, W)$, $S \in L(W, U)$. Then
      $S \circ T \in L(V, U)$. Moreover,
      \begin{equation*}
        L \left( U^*, V^* \right) \ni ( S \circ T )^* = T^* \circ S^*.
      \end{equation*}
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item Observe that for any $\beta \in V^*$, and any $v \in V$,
      we have
      \begin{equation*}
        ((I_V)^*(\beta))(v) = \beta((I_V)(v)) = \beta(v).
      \end{equation*}
      Therefore $(I_V)^* = I_{V^*}$.

    \item Observe that for $\gamma \in U^*$ and $v \in V$, we have
      \begin{align*}
        ((S \circ T)^*(\gamma))(v)
          &= \gamma ( ( S \circ T ) (v) ) \\
          &= \gamma ( S ( T ( v ) ) ) \\
          &= S^* ( \gamma T(v) ) \\
          &= ( T^* \circ S^* )(\gamma)(v),
      \end{align*}
      and so $( S \circ T )^* = T^* \circ S^*$ as required.
  \end{enumerate}\qed\
\end{proof}

Let $T \in L(V)$, and the dual map $T^* \in L(V^*)$.
Let $\mathcal{B}$ be a basis of $V$, with the dual basis
$\mathcal{B}^*$. We may write
\begin{equation*}
  A = [T]_{\mathcal{B}} \text{ and } A^* = [T^*]_{\mathcal{B}^*}.
\end{equation*}
Note that
\begin{equation*}
  T(e_i) = A_i^j e_j \text{ and } T^*(e^i) = (A^*)_j^i e^j.
\end{equation*}
Consequently, we have
\begin{equation*}
  \langle e^k, T( e_i ) \rangle = A_i^k \text{ and } 
  \langle T^*(e^i), e_k \rangle = (A^*)_k^i.
\end{equation*}

From here, notice that by applying $e_k \in V = V^{**}$ to both sides,
we have % TODO: I don't get this
\begin{equation*}
  (A^*)_k^i = e_k ( T^*(e^i) ) = \langle T^* (e^i), e_k \rangle 
  \overset{(*)}{=} \langle e^i, T(e_k) \rangle = A_k^i.
\end{equation*}
Thus $A^*$ is the transpose of $A$, and
\begin{equation}\label{eq:coordinate_of_t_star_in_v_star_equals_transpose_of_coordinate_of_t_in_v}
  [ T^* ]_{\mathcal{B}^*} = [ T ]_{\mathcal{B}}^t
\end{equation}
where $M^t$ is the transpose of the matrix $M$.

\subsection{Application to Orientations}%
\label{sub:application_to_orientations}
% subsection application_to_orientations

Let $\mathcal{B}$ be a basis of $V$. Then $\mathcal{B}$ determines an
orientation of $V$. Let $\mathcal{B}^*$ be the dual basis of $V^*$.
So $\mathcal{B}^*$ determines an orientation for $V^*$.

\begin{eg}
  Suppose $\mathcal{B}$ and $\tilde{\mathcal{B}}$ determines the same
  orientation of $V$. Does it follow that the dual bases $\mathcal{B}^*$
  and $\tilde{\mathcal{B}}^*$ determine the same orientation of $V^*$?
\end{eg}

\begin{proof}
  Let
  \begin{align*}
    \mathcal{B} &= \left\{ e_1, \ldots, e_n \right\} &
    \tilde{\mathcal{B}} &= \left\{ \tilde{e}_1, \ldots, \tilde{e}_n \right\} \\
    \mathcal{B}^* &= \left\{ e^1, \ldots, e^n \right\} &
    \tilde{\mathcal{B}}^* &= \left\{ \tilde{e}^1, \ldots, \tilde{e}^n \right\}
  \end{align*}
  Let $T \in L(V)$ such that $T(e_i) = \tilde{e}_i$. By assumption, $\det T > 0$.
  Notice that
  \begin{equation*}
    \delta_j^i = \tilde{e}^i (\tilde{e}_j) = \tilde{e}^i ( Te_j ) = (T^*(\tilde{e}^i))(e_j),
  \end{equation*}
  and so we must have $T^*(\tilde{e}^i) = e^i$. By
  \cref{eq:coordinate_of_t_star_in_v_star_equals_transpose_of_coordinate_of_t_in_v},
  we have that
  \begin{equation*}
    \det T^* = \det T > 0
  \end{equation*}
  as well. This shows that $\mathcal{B}^*$ and $\tilde{\mathcal{B}}^*$ determines
  the same orientation.
\end{proof}

% subsection application_to_orientations (end)

% section dual_map_continued (end)

\section{The Space of $k$-forms on $V$}%
\label{sec:the_space_of_k_forms_on_v_}
% section the_space_of_k_forms_on_v_

\begin{defn}[$k$-Form]\index{$k$-Form}\label{defn:_k_form}
  Let $V$ be an $n$dimensional vector space. Let $k \geq 1$. A \hlnoteb{$k$-form} on
  $V$ is a map
  \begin{equation*}
    \alpha : \underbrace{V \times V \times \hdots \times V}_{k \text{ times }} \to \mathbb{R}
  \end{equation*}
  such that
  \begin{enumerate}
    \item (\hlnotea{$k$-linearity / multi-linearity}) if we fix all but one of 
      the arguments of $\alpha$, then it is a linear map
      from $V$ to $\mathbb{R}$; i.e. if we fix
      \begin{equation*}
        v_1, \ldots, v_{j - 1}, v_{j + 1}, \ldots, v_k \in V,
      \end{equation*}
      then the map
      \begin{equation*}
        u \mapsto \alpha(v_1, \ldots, v_{j - 1}, u, v_{j + 1}, \ldots, v_k)
      \end{equation*}
       is linear in $u$.

    \item (\hlnotea{alternating property}) $\alpha$ is \hlnotea{alternating}
      (aka \hlnotea{totally skewed-symmetric}) in its $k$ arguments; i.e.
      \begin{equation*}
        \alpha ( v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k ) 
          = \alpha ( v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k ).
      \end{equation*}
  \end{enumerate}
\end{defn}

\begin{eg}
  The following is an example of the second condition: if $k = 2$, then
  $\alpha : V \times V \to \mathbb{R}$. Then $\alpha(v, w) = - \alpha(w, v)$.

  If $k = 3$, then $\alpha : V \times V \times V \to \mathbb{R}$. Then we
  have
  \begin{align*}
    \alpha(u, v, w) &= - \alpha(v, u, w) = - \alpha(w, v, u) = - \alpha(u, w, v) \\
                    &= \alpha(v, w, u) = \alpha(w, u, v).
  \end{align*}
\end{eg}

\begin{note}
  Note that if $k = 1$, then condition 2 is vacuous. Therefore, a $1$-form of
  $V$ is just an element of $V^* = L(W, \mathbb{R})$.
\end{note}

\begin{remark}[Permutations]
  From the last example, we notice that the `sign' of the value changes
  as we permute more times. To be precise, we are performing \hlnotea{transpositions}
  on the arguments
  \sidenote{See \href{https://tex.japorized.ink/PMATH347S18/classnotes.pdf\#defn.13}{PMATH 347}.},
  i.e. we only swap two of the arguments in a single move.
  Here are several remarks about permutations from group theory:
  \begin{itemize}
    \item A permutation $\sigma$ of $\left\{ 1, 2, \ldots, k \right\}$ is a
      bijective map.
    \item Compositions of permutations results in a permutation.
    \item The set $S_k$ of permutations on the set $\left\{ 1, 2, \ldots, k \right\}$
      is called a \hlnotea{group}.
    \item There are $k!$ such permutations.
    \item For each transposition, we may assign a \hlnotea{parity} of either $-1$ or $1$,
      and the parity is determined by the number of times we need to perform a transposition
      to get from $(1, 2, \ldots, k)$ to $(\sigma(1), \sigma(2), \ldots, \sigma(k))$. We
      usually denote a parity by $\sgn(\sigma)$.
  \end{itemize}

  The following is a fact proven in group theory: let $\sigma, \tau \in S_k$. Then
  \begin{align*}
    \sgn(\sigma \circ \tau) &= \sgn(\sigma) \cdot \sgn(\tau) \\
    \sgn(\id) &= 1 \\
    \sgn(\tau) &= \sgn(\tau^{-1}).
  \end{align*}
\end{remark}

Using the above remark, we can rewrite condition 2 as follows:

\begin{note}[Rewrite of condition 2 for \cref{defn:_k_form}]
  $\alpha$ is alternating, i.e.
  \begin{equation*}
    \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)}) = \sgn(\sigma) \cdot \alpha(v_1, \ldots, v_k),
  \end{equation*}
  where $\sigma \in S_k$.
\end{note}

\begin{remark}
  If $\alpha$ is a $k$-form on $V$, notice that
  \begin{equation*}
    \alpha(v_1, \ldots, v_k) = 0
  \end{equation*}
  if \hlimpo{any $2$ of the arguments are equal}. 
\end{remark}

% section the_space_of_k_forms_on_v_ (end)

% chapter lecture_3_jan_11th (end)

\appendix

\chapter{Review of Earlier Contents}%
\label{chp:review_of_earlier_contents}
% chapter review_of_earlier_contents

\section{Rank-Nullity Theorem}%
\label{sec:rank_nullity_theorem}
% section rank_nullity_theorem

\nocite{stephen2002}

\begin{defn}[Kernel and Image]\index{Kernel}\index{Image}\label{defn:kernel_and_image}
  Let $V$ and $W$ be vector spaces, and let $T \in L(V, W)$.
  The \hlnoteb{kernel} (or \hldefn{null space}) of $T$ is defined as
  \begin{equation*}
    \ker(T) := \left\{ v \in V \mmid Tv = 0 \right\},
  \end{equation*}
  i.e. the set of vectors in $V$ such that they are mapped to $0$ under $T$.

  The \hlnoteb{image} (or \hldefn{range}) of $T$ is defined as
  \begin{equation*}
    \Img(T) = \left\{ Tv \mmid v \in V \right\},
  \end{equation*}
  that is the set of all images of vectors of $V$ under $T$.
\end{defn}

It can be shown that for a linear map $T \in L(V, W)$,
$\ker (T)$ and $\Img(T)$ are subspaces of $V$ and $W$, respectively.
As such, we can define the following:

\begin{defn}[Rank and Nullity]\index{Rank}\index{Nullity}\label{defn:rank_and_nullity}
  Let $V, W$ be vector spaces, and let $T \in L(V, W)$.
  If $\ker(T)$ and $\Img(T)$ are finite-dimensional
  \sidenote{In this course, this is always the case, since we are only dealing with finite dimensional real vector spaces.},
  then we define the \hlnoteb{nullity} of $T$ as
  \begin{equation*}
    \nullity(T) := \dim \ker (T),
  \end{equation*}
  and the \hlnoteb{rank} of $T$ as
  \begin{equation*}
    \rank(T) := \dim \Img(T).
  \end{equation*}
\end{defn}

\begin{note}
  From the action of a linear transformation,
  we observe that the \hlnotec{larger the nullity, the smaller the rank}.
  Put in another way, the more vectors are sent to $0$ by the linear transformation,
  the smaller the range.

  Similarly, the larger the rank, the smaller the nullity.
\end{note}

This observation gives us the Rank-Nullity Theorem.

\begin{thm}[Rank-Nullity Theorem]\index{Rank-Nullity Theorem}\label{thm:rank_nullity_theorem}
  Let $V$ and $W$ be vector spaces, and $T \in L(V, W)$. If $V$ is finie-dimensional, then
  \begin{equation*}
    \nullity(T) + \rank(T) = \dim (V).
  \end{equation*}
\end{thm}

From the Rank-Nullity Theorem,
we can make the following observations about the relationships
between injection and surjection, and the nullity and rank.

\begin{propo}[Nullity of Only $0$ and Injectivity]\label{propo:nullity_of_only_0_and_injectivity}
  Let $V$ and $W$ be vector spaces, and $T \in L(V, W)$.
  Then $T$ is injective iff $\nullity(T) = \left\{ 0 \right\}$.
\end{propo}

Surjection and injectivity come hand-in-hand when
we have the following special case.

\begin{propo}[When Rank Equals The Dimension of the Space]\label{propo:when_rank_equals_the_dimension_of_the_space}
  Let $V$ and $W$ be vector spaces of equal (finite) dimension,
  and let $T \in L(V, W)$. TFAE
  \begin{enumerate}
    \item $T$ is injective;
    \item $T$ is surjective;
    \item $\rank(T) = \dim(V)$.
  \end{enumerate}
\end{propo}

Note that the proof for \cref{propo:when_rank_equals_the_dimension_of_the_space}
requires the understanding that $\ker(T) = \{ 0 \}$ implies that $\nullity(T) = 0$.
See \href{https://math.stackexchange.com/questions/664594/why-mathbf0-has-dimension-zero}{this explanation on Math SE}.

% section rank_nullity_theorem (end)

% chapter review_of_earlier_contents (end)

\backmatter

\pagestyle{plain}

\nobibliography*
\bibliography{references}

\printindex

\end{document}

