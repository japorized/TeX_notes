\documentclass[notoc,notitlepage]{tufte-book}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\renewcommand{\baselinestretch}{1.1}

\usepackage{tikz-cd}
\input{latex-classnotes-preamble.tex}

\title{PMATH347S18 - Groups \& Rings}
\author{Johnson Ng}

% Header formatting
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\makeatletter
\pagestyle{fancy}
\fancyhead{}
\fancyhead[RO]{\textsl{\@title} \enspace \thepage}
\fancyhead[LE]{\thepage \enspace \textsl{\leftmark \enspace - \enspace \rightmark}}
\makeatother

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\tableofcontents

\chapter*{List of Definitions}
\theoremlisttype{all}
\listtheorems{defn}

\chapter*{List of Theorems}
\theoremlisttype{allname}
\listtheorems{axiom,lemma,thm,crly,propo}

\chapter{Lecture 1 May 02nd 2018}
  \label{chapter:lecture_1_may_02nd_2018}

\section{Introduction} % (fold)
\label{sec:introduction}

\subsection{Numbers} % (fold)
\label{sub:numbers}

The following are some of the number sets that we are already familiar with:
\begin{gather*}
  \mathbb{N} = \{1, 2, 3, ...\} \qquad \mathbb{Z} = \{.., -2, -1, 0, 1, 2, ...\} \\
  \mathbb{Q} = \left\{\frac{a}{b} : a \in \mathbb{Z}, b \in \mathbb{N} \right\} \qquad \mathbb{R} = \text{ set of real numbers} \\
  \mathbb{C} = \{a + bi : a, b \in \mathbb{R}, i = \sqrt{-1} \} = \text{ set of complex numbers} 
\end{gather*}
For $n \in \mathbb{Z}$, let $\mathbb{Z}_n$ denote the set of integers modulo $n$, i.e.
\begin{equation*}
  \mathbb{Z}_n = \{ [0], [1], ..., [n - 1] \}
\end{equation*}
where the $[r]$, $0 \leq r \leq n - 1$, are the congruence classes, i.e.
\begin{equation*}
  [r] = \{z \in \mathbb{Z} : z \equiv r \mod n\}
\end{equation*}

These sets share some common properties, e.g. $+$ and $\times$. Let's try to break that down to make further observation.

\newthought{Note that} for $R = \mathbb{N}, \, \mathbb{Z}, \, \mathbb{Q}, \, \mathbb{R}, \, \mathbb{C},$ or $\mathbb{Z}_n$, $R$ has 2 operations, i.e. addition and multiplication.

\paragraph{Addition} If $r_1, r_2, r_3 \in R$, then
\begin{itemize}
  \item (\hldefn{closure}) $r_1 + r_2 \in R$
  \item (\hldefn{associativity}) $r_1 + (r_2 + r_3) = (r_1 + r_2) + r_3$
\end{itemize}
Also, if $R \neq \mathbb{N}$, then $\exists 0 \in R$ (the \hldefn{additive identity}) such that
\begin{equation*}
  \forall r \in R \quad r + 0 = r = 0 + r.
\end{equation*}
Also, $\forall r \in R$, $\exists (-r) \in R$ such that
\begin{equation*}
  r + (-r) = 0 = (-r) + r.
\end{equation*}

\paragraph{Multiplication} For $r_1, r_2, r_3 \in R$, we have
\begin{itemize}
  \item (\hlnoteb{closure}) $r_1 r_2 \in R$
  \item (\hlnoteb{associativity}) $r_1 (r_2 r_3) = (r_1 r_2) r_3$
\end{itemize}
Also, $\exists 1 \in R$ (a.k.a the \hldefn{mutiplicative identity}), such that
\begin{equation*}
  \forall r \in R \quad r \cdot 1 = r = 1 \cdot r.
\end{equation*}
Finally, for $R = \mathbb{Q}, \, \mathbb{R},$ or $\mathbb{C}$, $\forall r \in R, \, \exists r^{-1} \in R$ such that
\begin{equation*}
  r \cdot r^{-1} = 1 = r^{-1} \cdot r.
\end{equation*}
Note that for $R = \mathbb{Z}_n$, where $n \in \mathbb{Z}$, not all $[r] \in \mathbb{Z}_n$ have a multiplicative inverse. For example, for $[2] \in \mathbb{Z}_4$, there is no $[x] \in \mathbb{Z}_4$ such that $[2][x] = [1]$.\sidenote{This is best proven using techniques introduced in MATH135/145.}

% subsection numbers (end)

\subsection{Matrices}
  \label{sub:matrices}

For $n \in \mathbb{N} \setminus \{1\}$, an $n \times n$ matrix over $\mathbb{R}$ \sidenote{$\mathbb{R}$ can be replaced by $\mathbb{Q}$ or $\mathbb{C}$.} is an $n \times n$ array that can be expressed as follows:
\begin{equation*}
  A = [a_{ij}] = \begin{bmatrix}
    a_{11} & a_{12} & \hdots & a_{1n} \\
    a_{21} & a_{22} & \hdots & a_{2n} \\
    \vdots & \vdots &        & \vdots \\
    a_{n1} & a_{n2} & \hdots & a_{nn}
  \end{bmatrix}
\end{equation*}
where for $1 \leq i, j \leq n$, $a_{ij} \in \mathbb{R}$. We denote $M_n(\mathbb{R})$ as the set of all $n \times n$ matrices over $\mathbb{R}$.

As in \cref{sub:numbers}, we can perform \hlnotea{addition and multiplication} on $M_n(\mathbb{R})$.

\paragraph{Matrix Addition} Given $A = [a_{ij}], B = [b_{ij}], C = [c_{ij}] \in M_n(\mathbb{R})$, we define matrix addition as
\begin{equation*}
  A + B = [a_{ij} + b_{ij}],
\end{equation*}
which immediately gives the \hlnoteb{closure property}, since $a_{ij} + b_{ij} \in \mathbb{R}$ and hence $A + B \in M_n(\mathbb{R})$. Also, by this definition, we also immediately obtain the \hlnoteb{associativity property}, i.e.
\begin{equation*}
  A + (B + C) = (A + B) + C.
\end{equation*}
We define the zero matrix as
\begin{equation*}
  0 = \begin{bmatrix}
    0      &   0    & \hdots &   0 \\
    0      &   0    & \hdots &   0 \\
    \vdots & \vdots &        & \vdots \\
    0      &   0    & \hdots &   0
  \end{bmatrix}.
\end{equation*}
Then we have that $0$ is the \hlnoteb{additive identity}, i.e.
\begin{equation*}
  A + 0 = A = 0 + A.
\end{equation*}
Finally, $\forall A \in M_n(\mathbb{R})$, $\exists (-A) \in M_n(\mathbb{R})$ (the \hlnoteb{additive inverse}) such that
\begin{equation*}
  A + (-A) = 0 - (-A) + A.
\end{equation*}

Note that in this case, we also have that that the operation is \hlnoteb{commutative}, i.e.
\begin{equation*}
  A + B = B + A.
\end{equation*}

\paragraph{Matrix Multiplication} Given $A = [a_{ij}], B = [b_{ij}], C = [c_{ij}] \in M_n(\mathbb{R})$, we define the matrix multiplication as
\begin{equation*}
  AB = [d_{ij}] \text{ where } c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj} \in \mathbb{R}.
\end{equation*}
Clearly, $AB \in M_n(\mathbb{R})$, i.e. it is \hlnoteb{closed under matrix multiplication}. Also, we have that, under such a defintion, matrix multiplication is \hlnoteb{associative}, i.e.
\begin{equation*}
  A(BC) = (AB)C.
\end{equation*}
Define the identity matrix, $I \in M_n(\mathbb{R})$, as follows:
\begin{equation*}
  I = \begin{bmatrix}
    1      &   0    & \hdots & 0 \\
    0      &   1    & \hdots & 0 \\
    \vdots & \vdots &        & \vdots \\
    0      &   0    & \hdots & 1
  \end{bmatrix}.
\end{equation*}
Then we have that $I$ is the \hlnoteb{multiplicative identity}, since
\begin{equation*}
  AI = A = IA.
\end{equation*}
However, contrary to matrix addition, $\forall A \in M_n(\mathbb{R})$, it is not always true that $\exists A^{-1} \in M_n(\mathbb{R})$ such that\marginnote{This is especially true if the \hlnotea{determinant} of $A$ is $0$.}
\begin{equation*}
  AA^{-1} = I = A^{-1} A.
\end{equation*}

Also, we can always find some $A, B \in M_n(\mathbb{R})$ such that
\begin{equation*}
  AB \neq BA,
\end{equation*}
i.e. matrix multiplication is not always commutative.

\newthought{The common properties} of the operations from above: \hlimpo{closure, associativity, and existence of an inverse}, are not unique to just addition and multiplication. We shall see in the next lecture that there are other operations where these properties will continue to hold, e.g. \hlnoteb{permutations}.

% subsection matrices (end)

% section introduction (end)

% chapter lecture_1_may_02nd_2018 (end)

\chapter{Lecture 2 May 04th 2018}
  \label{chapter:lecture_2_may_04th_2018}

\section{Introduction (Continued)} % (fold)
\label{sec:introduction_continued}

\subsection{Permutations} % (fold)
\label{sub:permutations}

\begin{defn}[Injectivity]\label{defn:injectivity}
\index{Injectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{injective} (or \hldefn{one-to-one}) if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
\end{defn}

\begin{defn}[Surjectivity]\label{defn:surjectivity}
\index{Surjectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{surjective} (or \hldefn{onto}) if $\forall y \in Y \enspace \exists x \in X \enspace f(x) = y$.
\end{defn}

\begin{defn}[Bijectivity]\label{defn:bijectivity}
\index{Bijectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{bijective} if it is both \hlnoteb{injective} and \hlnoteb{surjective}.
\end{defn}

\begin{defn}[Permutations]\label{defn:permutations}
\index{Permutations}
  Given a non-empty set $L$, a permutation of $L$ is a bijection from $L$ to $L$. The set of all permutations of $L$ is denoted by $S_L$.
\end{defn}

\begin{eg}
  \label{eg:permutations_first}
  Consider the set $L = \{1, 2, 3\}$, which has the following $6$ different permutations: \marginnote{\begin{note}
    \begin{equation*}
      \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}
    \end{equation*}
    indicates the bijection $\sigma: \{1, 2, 3\} \to \{1, 2, 3\}$ with $\sigma(1) = 1$, $\sigma(2) = 3$ and $\sigma(3) = 2$.
  \end{note}}

  \begin{gather*}
     \begin{pmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{pmatrix} \\
     \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 1 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 3 & 1 & 2 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 3 & 2 & 1 \end{pmatrix}
   \end{gather*} 
\end{eg}

\newthought{For $n \in \mathbb{N}$}, we denote $S_n := S_{\{1, 2, ..., n\}}$, the set of all permutations of $\{1, 2, ..., n\}$. \cref{eg:permutations_first} shows the elements of the set $S_3$.

\begin{defn}[Order]\label{defn:order}
\index{Order}
  The \hlnoteb{order} of a set $A$, denoted by $\abs{A}$, is the cardinality of the set.
\end{defn}

\begin{eg}
  \label{eg:order_of_prev_eg}
  We have seen that the order of $S_3$, $\abs{S_3}$ is $6 = 3!$.
\end{eg}

\begin{propo}\label{propo:order_of_Sn_is_n}
  $\abs{S_n} = n!$
\end{propo}

\begin{proof}
  $\forall \sigma \in S_n$, there are $n$ choices for $\sigma(1)$, $n - 1$ choices for $\sigma(2)$, ..., $2$ choices for $\sigma(n - 1)$, and finally $1$ choice for $\sigma(n)$. \qed
\end{proof}

\paragraph{Do elements of $S_n$ share the same properties as what we've seen in the numbers?} Given $\sigma, \tau \in S_n$, we can \hlnotea{compose} the 2 together to get a third element in $S_n$, namely $\sigma \tau$ (wlog), where $\sigma \tau : \{1, ..., n\} \to \{1, ..., n\}$ is given by $\forall x \in \{1, ..., n\}$, $x \mapsto \sigma( \tau(x) )$.

It is important to note that $\because \sigma, \tau$ are \hlimpo{both bijective}, $\sigma \tau$ is also bijective. Thus, together with the fact that $\sigma \tau : \{1, ..., n\} \to \{1, ..., n\}$, we have that $\sigma \tau \in S_n$ by definition of $S_n$.

$\therefore \forall \sigma, \tau \in S_n, \; \sigma \tau, \tau \sigma \in S_n$, but $\sigma \tau \neq \tau \sigma$ in general. The following is an example of the stated case:

\begin{eg}
  \label{eg:commutativity_of_Sn}
  Let
  \begin{equation*}
    \sigma &= \begin{pmatrix}
      1 & 2 & 3 & 4 \\
      3 & 4 & 1 & 2
    \end{pmatrix}, \text{ and } 
    \tau &= \begin{pmatrix}
      1 & 2 & 3 & 4 \\
      2 & 4 & 3 & 1
    \end{pmatrix}.
  \end{equation*}
  Compute $\sigma \tau$ and $\tau \sigma$ to show that they are not equal.

  \begin{solution}
    \begin{equation*}
      \sigma \tau &= \begin{pmatrix}
        1 & 2 & 3 & 4 \\
        4 & 2 & 1 & 3
      \end{pmatrix} \text{ but } 
      \tau \sigma &= \begin{pmatrix}
        1 & 2 & 3 & 4 \\
        3 & 1 & 2 & 4
      \end{pmatrix}
    \end{equation*}
  \end{solution}
\end{eg}

Perhaps what is interesting is the question of: \textbf{when does commutativity occur?} One such case is when $\sigma$ and $\tau$ have support sets that are disjoint\sidenote{This is proven in A1}.

On the other hand, the associative property holds\sidenote{
  \begin{ex}
    Prove this as an exercise.
  \end{ex}
}, i.e.
\begin{equation*}
  \forall \sigma, \tau, \mu \in S_n \enspace \sigma (\tau \mu) = (\sigma \tau) \mu
\end{equation*}

The set $S_n$ also has an identity element\sidenote{
  \begin{ex}
    Verify that the given identity element is indeed the identity, i.e.
    \begin{equation*}
      \forall \sigma \in S_n \enspace \sigma \epsilon = \sigma = \epsilon \sigma.
    \end{equation*}
  \end{ex}
}, namely
\begin{equation*}
  \epsilon = \begin{pmatrix}
    1 & 2 & \hdots & n \\
    1 & 2 & \hdots & n
  \end{pmatrix}
\end{equation*}

Finally, $\forall \sigma \in S_n$, since $\sigma$ is a bijection, we have that its inverse function, $\sigma^-1$ is also a bijection, and thus satisfies the requirements to be in $S_n$. We call $\sigma^{-1} \in S_n$ to be the \hldefn{inverse permutation} of $\sigma$, such that
\begin{equation*}
  \forall x, y \in \{1, ..., n\} \quad \sigma^{-1}(x) = y \iff \sigma(y) = x.
\end{equation*}
It follows, immediately, that
\begin{equation*}
  \sigma \big( \sigma^{-1}(x) \big) = x \, \land \, \sigma^{-1} \big( \sigma(y) \big) = y.
\end{equation*}
$\therefore$ We have that 
\begin{equation*}
  \sigma \sigma^{-1} = \epsilon = \sigma^{-1} \sigma.
\end{equation*}

\begin{eg}
  \label{eg:inverse_permutation}
  Find the inverse of
  \begin{equation*}
    \sigma = \begin{pmatrix}
      1 & 2 & 3 & 4 & 5 \\
      4 & 5 & 1 & 2 & 3
    \end{pmatrix}
  \end{equation*}

  \begin{solution}
    By rearranging the image in ascending order, using them now as the object and their respective objects as their image, construct
    \begin{equation*}
      \tau = \begin{pmatrix}
        1 & 2 & 3 & 4 & 5 \\
        3 & 4 & 5 & 1 & 2
      \end{pmatrix}.
    \end{equation*}
    It can easily (although perhaps not so prettily) be shown that
    \begin{equation*}
      \sigma \tau = \epsilon = \tau \sigma.
    \end{equation*}
  \end{solution}
\end{eg}

With all the above, we have for ourselves the following proposition:

\begin{propo}[Properties of $S_n$]\label{propo:properties_of_Sn}
  We have
  \begin{enumerate}
    \item $\forall \sigma, \tau \in S_n \enspace \sigma \tau, \tau \sigma \in S_n$.
    \item $\forall \sigma, \tau, \mu \in S_n \enspace \sigma (\tau \mu) = (\sigma \tau) \mu$.
    \item $\exists \epsilon \in S_n \enspace \forall \sigma \in S_n \enspace \sigma \epsilon = \sigma = \epsilon \sigma$.
    \item $\forall \sigma \in S_n \enspace \exists! \sigma^{-1} \in S_n \enspace \sigma \sigma^{-1} = \epsilon = \sigma^{-1} \sigma$.
  \end{enumerate}
\end{propo}

\newthought{Consider}
\begin{equation*}
  \sigma = \begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    3 & 1 & 7 & 6 & 9 & 4 & 2 & 5 & 8 & 10
  \end{pmatrix} \in S_{10}
\end{equation*}
If we represent the action of $\sigma$ geometrically, we get

\begin{tabular}{c c c}
  \begin{tikzcd}
   & 1 \arrow[rd] &  \\
  2 \arrow[ru] &  & 3 \arrow[ld] \\
   & 7 \arrow[lu] & 
  \end{tikzcd}
  &
  \begin{tikzcd}
  4 \arrow[dd, bend left] \\
   \\
  6 \arrow[uu, bend left]
  \end{tikzcd}
  &
  \begin{tikzcd}
   & 5 \arrow[rdd] &  \\
   &  &  \\
  8 \arrow[ruu] &  & 9 \arrow[ll]
  \end{tikzcd}
  \\ &
  \begin{tikzcd}
    10 \arrow[loop below]
  \end{tikzcd}
\end{tabular}
We observe that $\sigma$ can be \hlnotea{decomposed} into one $4$-cycle, $\begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix}$, one $2$-cycle, $\begin{pmatrix} 4 & 6 \end{pmatrix}$, one $3$-cycle, $\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}$, and one $1$-cycle, $\begin{pmatrix} 10 \end{pmatrix}$.

Note that these cycles are (pairwise) \hlnotea{disjoint}, and we can write\sidenote{We generally do not include the $1$-cycle and assume that by excluding them, it is known that any number that is supposed to appear loops back to themselves.}
\begin{equation*}
  \sigma = \begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix}\begin{pmatrix} 4 & 6 \end{pmatrix}\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}
\end{equation*}
Note that we may also write
\begin{align*}
  \sigma &= \begin{pmatrix} 4 & 6 \end{pmatrix}\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}\begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix} \\
    &= \begin{pmatrix} 6 & 4 \end{pmatrix}\begin{pmatrix} 9 & 8 & 5 \end{pmatrix}\begin{pmatrix} 7 & 2 & 1 & 3 \end{pmatrix}
\end{align*}
It is interesting to note that the cycles can rotate their ``elements'' in a \hlnotea{cyclic} manner, i.e.
\begin{gather*}
  \begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix} = \begin{pmatrix} 7 & 2 & 1 & 3 \end{pmatrix} \neq \begin{pmatrix} 1 & 2 & 7 & 3 \end{pmatrix}.
\end{gather*}
Although the decomposition of the cycle notation is not unique (i.e. you may rearrange them), each individual cycle is unique, and is proven below\sidenote{See bonus question of A1. Proof will be included in the notes once the assignment is over.}.

\begin{thm}[Cycle Decomposition Theorem]\label{thm:cycle_decomposition_theorem}
\index{Cycle Decomposition Theorem}
  If $\sigma \in S_n$, $\sigma \neq \epsilon$, then $\sigma$ is a product of (one or more) disjoint cycles of length at least $2$. This factorization is unique up to the order of the factors.
\end{thm}

\begin{note}[Convention]
 Every permutation in $S_n$ can be regarded as a permutation of $S_{n + 1}$ by fixing the permutation of $n + 1$. Therefore, we have that
 \begin{equation*}
   S_1 \subseteq S_2 \subseteq \hdots \subseteq S_n \subseteq S_{n + 1} \subseteq \hdots
 \end{equation*}
\end{note}

% subsection permutations (end)

% section introduction_continued (end)

% chapter lecture_2_may_04th_2018 (end)

\chapter{Lecture 3 May 07th 2018}
  \label{chapter:lecture_3_may_07th_2018}

\section{Groups} % (fold)
\label{sec:groups}

\subsection{Groups} % (fold)
\label{sub:groups}

\begin{defn}[Groups]\label{defn:groups}
\index{Groups}
  Let $G$ be a set and $*$ an operation on $G \times G$. We say that $G = (G, *)$ is a \hlnoteb{group} if it satisfies\sidenote{If you wonder why the uniqueness is not specified for \hlnoteb{Identity} and \hlnoteb{Inverse}, see \cref{propo:uniqueness_of_group_identity_and_group_element_inverse}.}
  \begin{enumerate}
    \item \hlnoteb{Closure}: $\forall a, b \in G \quad a * b \in G$
    \item \hlnoteb{Associativity}: $\forall a, b, c \in G \quad a * (b * c) = (a * b) * c$
    \item \hlnoteb{Identity}: $\exists e \in G \enspace \forall a \in G \quad a * e = a = e * a$
    \item \hlnoteb{Inverse}: $\forall a \in G \enspace \exists b \in G \quad a * b = e = b * a$
  \end{enumerate}
\end{defn}

\begin{defn}[Abelian Group]\label{defn:abelian_group}
\index{Abelian Group}
  A group $G$ is said to be abelian if $\forall a, b \in G$, we have $a * b = b * a$.
\end{defn}

\begin{propo}[Group Identity and Group Element Inverse]\label{propo:uniqueness_of_group_identity_and_group_element_inverse}
  Let $G$ be a group and $a \in G$.
  \begin{enumerate}
    \item The identity of $G$ is unique.
    \item The inverse of $a$ is unique.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item If $e_1, e_2 \in G$ are both identities of $G$, then we have
      \begin{equation*}
        e_1 \overset{(1)}{=} e_1 * e_2 \overset{(2)}{=} e_2
      \end{equation*}
      where $(1)$ is because $e_2$ is an identity and $(2)$ is because $e_1$ is an identity.

    \item Let $a \in G$. If $b_1, b_2 \in G$ are both the inverses of $a$, then we have
      \begin{equation*}
        b_1 = b_1 * e = b_1 * (a * b_2) \overset{(1)}{=} e * b_2 = b_2
      \end{equation*}
      where $(1)$ is by associativity.
  \end{enumerate}
\end{proof}

\begin{eg}
  The sets $(\mathbb{Z}, +), \, (\mathbb{Q}, +), \, (\mathbb{R}, +)$, and $(\mathbb{C}, +)$ are all abelian, wehre the additive identity is $0$, and the additive inverse of an element $r$ is $(-r)$.
\end{eg}

\begin{note}
  $(\mathbb{N}, +)$ is not a group for neither does it have an identity nor an inverse for any of its elements.
\end{note}

\begin{eg}
  The sets $(\mathbb{Q}, \cdot), \, (\mathbb{R}, \cdot)$ and $(\mathbb{C}, \cdot)$ are \hlwarn{not} groups, since $0$ has no multiplicative inverse in $\mathbb{Q}, \mathbb{R}$ or $\mathbb{C}$.
\end{eg}

We may define that for a set $S$, let $S^* \subseteq S$ contain all the elements of $S$ that has a multiplicative inverse. For example, $\mathbb{Q}^* = \mathbb{Q} \setminus \{0\}$. Then, $(\mathbb{Q}, \cdot), (\mathbb{R}, \cdot)$ and $(\mathbb{C}, \cdot)$ are groups and are in fact abelian, where the multiplicative identity is $1$ and the multiplicative of an element $r$ is $\frac{1}{r}$.

\begin{eg}
  The set $\big( M_n(\mathbb{R}), + \big)$ is an abelian group, where the additive identity is the zero matrix, $0 \in M_n(\mathbb{R})$, and the additive inverse of an element $M = [a_{ij}] \in M_n(\mathbb{R})$ is $-M = [-a_{ij}] \in M_n(\mathbb{R})$.
\end{eg}

\newthought{Consider} the set $M_n(\mathbb{R})$ under the matrix mutiplication operation that we have introduced in \nameref{chapter:lecture_1_may_02nd_2018}. We found that the identity matrix is
\begin{equation*}
  I = \begin{bmatrix}
    1 & 0 & \hdots & 0 \\
    0 & 1 & \hdots & 0 \\
    \vdots & \vdots & & \vdots \\
    0 & 0 & \hdots & 1
  \end{bmatrix} \in M_n(\mathbb{R}).
\end{equation*}
But since not all elements of $M_n(\mathbb{R})$ have a multiplicative inverse\sidenote{The multiplicative inverse of a matrix does not exist if its determinant is $0$.}, $(M_n(\mathbb{R}), \cdot)$ is not a group.

But we can try to do something similar as to what we did before: by excluding the elements that do not have an inverse. In this case, we exclude elements whose determinant is $0$. Define the set
\begin{equation*}
  GL_n(\mathbb{R}) := \{ M \in M_n(\mathbb{R}) \, : \, \det M \neq 0 \}
\end{equation*}
Note that $\because \det I = 1 \neq 0$, we have that $I \in GL_n(\mathbb{R})$. \\
Also, $\forall A, B \in GL_n(\mathbb{R} )$, we have that $\because \det A \neq 0 \, \land \, \det B \neq 0$,
\begin{equation*}
  \det AB = \det A \det B \neq 0,
\end{equation*}
and therefore $AB \in GL_n(\mathbb{R} )$. Finally, $\forall M \in GL_n(\mathbb{R})$, $\exists M^{-1} \in GL_n(\mathbb{R})$ such that
\begin{equation*}
  MM^{-1} = I = M^{-1} M
\end{equation*}
since $\det M \neq 0$. $\therefore (GL_n(\mathbb{R}), \cdot)$ is a group, and is in fact called the \hldefn{general linear group} \hlnoteb{of degree $n$ over $\mathbb{R}$}.

\newthought{Since} we have introduced permutations in \nameref{chapter:lecture_2_may_04th_2018}, we shall formalize the purpose of its introduction below.

\begin{eg}
  Consider $S_n$, the set of all permutations on $\{1, 2, ..., n\}$. By \cref{propo:properties_of_Sn}, we know that $S_n$ is a group. We call $S_n$ the \hldefn{symmetry group} \hlnoteb{of degree $n$}. For $n \geq 3$, the group $S_n$ is not abelian\sidenote{Let us make this an exercise.
  \begin{ex}
    For $n \geq 3$, prove that the group $S_n$ is not abelian.
  \end{ex}}.
\end{eg}

\newthought{Now that} we have a fairly good idea of the basic concept of a group, we will now proceed to look into handling multiple groups. One such operation is known as the \hldefn{direct product}.

\begin{eg}
  \label{eg:direct_product}
  Let $G$ and $H$ be groups. Their direct product is the set $G \times H$ with the component-wise operation defined by
  \begin{equation*}
    (g_1, h_1) * (g_2, h_2) = (g_1 *_G g_2, h_1 *_H h_2)
  \end{equation*}
  where $g_1, g_2 \in G$, $h_1, h_2 \in H$, $*_G$ is the operation on $G$, and $*_H$ is the operation on $H$.

  The \hlnoteb{closure} and \hlnoteb{associativity} property follow immediately from the definition of the operation. The identity is $(1_G, \, 1_H)$ where $1_G$ is the identity of $G$ and $1_H$ is the identity of $H$. The inverse of an element $(g_1, \, h_1) \in G \times H$ is $(g_1^{-1}, \, h_1^{-1})$.
\end{eg}

By induction, we can show that if $G_1, G_2, ..., G_n$ are groups, then so is $G_1 \times G_2 \times \hdots \times G_n$.

To facilitate our writing, use shall use the following notations:

\begin{notation}
  Given a group $G$ and $g_1, g_2 \in G$, we often denote its identity by $1$, and write $g_1 * g_2 = g_1 g_2$. Also, we denote the unique inverse of an element $g \in G$ as $g^{-1}$.

  We will write $g^0 = 1$. Also, for $n \in \mathbb{N}$, we define
  \begin{equation*}
     g^n = \underbrace{g * g * \hdots * g}_{n \text{ times}}
  \end{equation*}
  and
  \begin{equation*}
    g^{-n} = (g^{-1})^n
  \end{equation*}
\end{notation}

With the above notations,

\begin{propo}\label{propo:group_notations}
  Let $G$ be a group and $g, h \in G$. We have \marginnote{
    \begin{ex}
      Prove \cref{propo:group_notations} as an exercise.
    \end{ex}
  }
  \begin{enumerate}
    \item $(g^{-1})^{-1} = g$
    \item $(gh)^{-1} = h^{-1} g^{-1}$
    \item $g^n g^m = g^{n + m}$ for all $n, m \in \mathbb{Z}$
    \item $(g^n)^m = g^{nm}$ for all $n, m \in \mathbb{Z}$
  \end{enumerate}
\end{propo}

\begin{warning}
  In general, it is not true that if $g, h \in G$, then $(gh)^n = g^n h^n$. For example,
  \begin{equation*}
    (gh)^2 = ghgh \quad \text{but} \quad g^2 h^2 = gghh.
  \end{equation*}
  The two are only equal if and only if $G$ is abelian.
\end{warning}

% subsection groups (end)

% section groups (end)

% chapter lecture_3_may_07th_2018 (end)

\chapter{Lecture 4 May 09 2018}
  \label{chapter:lecture_4_may_09_2018}

\section{Groups (Continued)} % (fold)
\label{sec:groups_continued}

\subsection{Groups (Continued)} % (fold)
\label{sub:groups_continued}

\begin{propo}[Cancellation Laws]\label{propo:cancellation_laws}
  Let $G$ be a group and $g, h, f \in G$. Then
  \begin{enumerate}
    \item \begin{enumerate}
        \item (\hlnoteb{Right Cancellation}) $gh = gf \implies h = f$
        \item (\hlnoteb{Left Cancellation}) $hg = fg \implies h = f$
      \end{enumerate}
    \item The equation $ax = b$ and $ya = b$ have unique solution for $x, y \in G$.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item \begin{enumerate}
      \item By left multiplication and associativity,
        \begin{equation*}
          gh = gf \iff g^{-1} gh = g^{-1} gf \iff h = f
        \end{equation*}
      \item By right multiplication and associativity,
        \begin{equation*}
          hg = fg \iff hgg^{-1} = fgg^{-1} \iff h = f
        \end{equation*}
    \end{enumerate}

    \item Let $x = a^{-1} b$. Then
      \begin{equation*}
        a x = a (a^{-1} b) = (aa^{-1}) b = b.
      \end{equation*}
      If $\exists u \in G$ that is another solution, then
      \begin{equation*}
        au = b = ax \implies u = x
      \end{equation*}
      by Left Cancellation. The proof for $ya = b$ is similar by letting $y = ba^{-1}$.
  \end{enumerate}\qed
\end{proof}

% subsection groups_continued (end)

\subsection{Cayley Tables} % (fold)
\label{sub:cayley_tables}

For a finite group, defining its operation by means of a table is sometimes convenient.

\begin{defn}[Cayley Table]\label{defn:cayley_table}
\index{Cayley Table}
  Let $G$ be a group. Given $x, y \in G$, let the product $xy$ be an entry of a table in the row corresponding to $x$ and column corresponding to $y$. Such a table is called a \hlnoteb{Cayley Table}.
\end{defn}

\begin{note}
  By \autoref{propo:cancellation_laws}, the entries in each row (and respectively, column) of a Cayley Table are all distinct.
\end{note}

\begin{eg}
  Consider the group $(\mathbb{Z}_2, +)$. Its Cayley Table is
  \begin{center}
    \begin{tabular}{c|c|c}
      $\mathbb{Z}_2$ & $[0]$ & $[1]$ \\
      \hline
      $[0]$     & $[0]$ & $[1]$ \\
      $[1]$     & $[1]$ & $[0]$ 
    \end{tabular}
  \end{center}
  where note that we must have $[1] + [1] = [0]$; otherwise if $[1] + [1] = [1]$ then $[1]$ does not have its additive inverse, which contradicts the fact that it is in the group.
\end{eg}

\marginnote {
  If we replace $1$ by $[0]$ and $-1$ by $[1]$, the Cayley Tables of $\mathbb{Z}_2$ and $\mathbb{Z}^*$ are the same. In thie case, we say that $\mathbb{Z}_2$ and $\mathbb{Z}^*$ are \hlnotea{isomorphic}, which we denote by $\mathbb{Z}_2 \cong \mathbb{Z}^*$.
}

\begin{eg}
  Consider the group $\mathbb{Z}^* = \{1. -1\}$. Its Cayley Table (under multiplication) is
  \begin{center}
    \begin{tabular}{c|c|c}
      $\mathbb{Z}^*$ & $1$    & $-1$ \\
      \hline
      $1$              & $1$  & $-1$ \\
      $-1$             & $-1$ & $1$
    \end{tabular}
  \end{center}
\end{eg}

\begin{eg}\label{eg:cyclic_group_cayley_table}
  Given $n \in \mathbb{N}$, the \hldefn{Cyclic Group} of order $n$ is defined by
  \begin{equation*}
    C_n = \{1, a, a^2, ..., a^{n - 1}\} \quad \text{with } a^n = 1.
  \end{equation*}
  We write $C_n = \langle a : a^n = 1 \rangle$ and $a$ is called a generator of $C_n$. The Cayley Table of $C_n$ is
  \begin{center}
    \begin{tabular}{c | c c c c c c}
      $C_n$     & $1$       & $a$       & $a^2$  & \hdots & $a^{n - 2}$ & $a^{n - 1}$ \\
      \hline
      $1$       & $1$       & $a$       & $a^2$  & \hdots & $a^{n - 2}$ & $a^{n - 1}$ \\
      $a$       & $a$       & $a^2$     & $a^3$  & \hdots & $a^{n - 1}$ & $1$ \\
      $a^2$     & $a^2$     & $a^3$     & $a^4$  & \hdots & $1$         & $a$ \\
      \vdots    & \vdots    & \vdots    & \vdots &        & \vdots      & \vdots \\
      $a^{n-2}$ & $a^{n-2}$ & $a^{n-1}$ & $1$    & \hdots & $a^{n-4}$   & $a^{n-3}$ \\
      $a^{n-1}$ & $a^{n-1}$ & $1$       & $a$    & \hdots & $a^{n-3}$   & $a^{n-2}$
    \end{tabular}
  \end{center}
\end{eg}

\begin{propo}\label{propo:small_groups}
  Let $G$ be a group. Up to isomorphism, we have
  \begin{enumerate}
    \item if $\abs{G} = 1$, then $G \cong \{1\}$.
    \item if $\abs{G} = 2$, then $G \cong C_2$.
    \item if $\abs{G} = 3$, then $G \cong C_3$.
    \item if $\abs{G} = 4$, then either $G \cong C_4$ or $G \cong K_4 \cong C_2 \times C_2$ \marginnote{$K_n$ is known as the \hldefn{Klein n-group}}.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item If $\abs{G} = 1$, then it can only be $G = \{1\}$ where $1$ is the identity element.
    \item $\abs{G} = 2 \implies G = \{1, g\}$ with $g \neq 1$. The Cayley Table of $G$ is thus
      \begin{center}
        \begin{tabular}{c | c c}
        $G$ & $1$ & $g$ \\
        \hline
        $1$ & $1$ & $g$ \\
        $g$ & $g$ & $1$
        \end{tabular}
      \end{center}
      where we note that $g^2 = 1$; otherwise if $g^2 = g$, then we would have $g = 1$ by \autoref{propo:cancellation_laws}, which contradicts the fact that $g \neq 1$. Comparing the above Cayley Table with that of $C_2$, we see that $G = \langle g : g^2 = 1 \rangle \cong C_2$.
    \item $\abs{G} = 3 \implies G = \{1, g, h\}$ with $g \neq 1 \neq h$ and $g \neq h$. We can then start with the following Cayley Table:
      \begin{center}
        \begin{tabular}{c | c c c}
        $G$ & $1$ & $g$ & $h$ \\
        \hline
        $1$ & $1$ & $g$ & $h$ \\
        $g$ & $g$ &     &     \\
        $h$ & $h$ &     &     
        \end{tabular}
      \end{center}
      We know that by \autoref{propo:cancellation_laws}, $gh \neq g$ and $gh \neq h$. Thus $gh = 1$. Similarly, we get that $hg = 1$.

      \underline{Claim:} Entries in a row (or column) must be distinct. Suppose not. Then say $g^2 = 1$. But since $gh = 1$, by \autoref{propo:cancellation_laws}, we have that $h = g$, which is a contradiction.

      With that, we can proceed to fill in the rest of the entries: with $g^2 = h$ and $h^2 = g$. Therefore,
      \begin{center}
        \begin{tabular}{c | c c c}
        $G$ & $1$ & $g$ & $h$ \\
        \hline
        $1$ & $1$ & $g$ & $h$ \\
        $g$ & $g$ & $h$ & $1$ \\
        $h$ & $h$ & $1$ & $g$
        \end{tabular}
      \end{center}

      Recall that the Cayley Table for $C_3$ is:
      \begin{center}
        \begin{tabular}{c | c c c}
        $C_3$ & $1$   & $a$   & $a^2$ \\
        \hline
        $1$   & $1$   & $a$   & $a^2$ \\
        $a$   & $a$   & $a^2$ & $1$ \\
        $a^2$ & $a^2$ & $1$   & $a$
        \end{tabular}
      \end{center}
      $\therefore G \cong C_3$ (by identifying $g = a$ and $h = a^2$).

    \item \hlwarn{Proof will be added once assignment 1 is over}
  \end{enumerate}
\end{proof}

% subsection cayley_tables (end)

% section groups_continued (end)

\section{Subgroups}
\label{sec:subgroups}

\subsection{Subgroups}
\label{sub:subgroups}

\begin{defn}[Subgroup]\label{defn:subgroup}
\index{Subgroup}
  Let $G$ be a group and $H \subseteq G$. If $H$ itself is a group, then we say that $H$ is a subgroup of $G$
\end{defn}

% subsection subgroups (end)

% section subgroups (end)

% chapter lecture_4_may_09_2018 (end)

\chapter{Lecture 5 May 11th 2018}
\label{chp:lecture_5_may_11th_2018}

\section{Subgroups (Continued)}
\label{sec:subgroups_continued}
% section Subgroups (Continued)

\subsection{Subgroups (Continued)}
\label{sub:subgroups_continued}
% subsection Subgroups (Continued)

\begin{note}[Recall: definition of a subgroup]
  Let $G$ be a group and $H \subseteq G$. If $H$ itself is a group, then we say that $H$ is a subgroup of $G$.
\end{note}

\begin{note}
  Since $G$ is a group, $\forall h_1, h_2, h_3 \in H \subseteq G$, we have $h_1 (h_2 h_3) = (h_1 h_2) h_3$. So $H$ is a subgroup of $G$ if it satisfies the following conditions, which we shall hereafter refer to as the Subgroup Test.

\noindent\hldefn{Subgroup Test} \\
  \marginnote{Note that the identity in $H$ must also be the identity in $G$. This is because if $h_1, h_1^{-1} \in H$, then $h_1 h_1^{-1} = 1_H$, but $h_1, h_1^{-1} \in G$ as well, and so $h_1 h_1^{-1} = 1_G$. Thus $1_H = 1_G$.}
  \begin{enumerate}
    \item $h_1 h_2 \in H$
    \item $1_G \in H$
    \item $\exists h_1^{-1} \in H$ such that $h_1 h_1^{-1} = 1_G$
  \end{enumerate}
\end{note}

\begin{eg}
  Given a group $G$, it is clear that $\{1\}$ and $G$ are both subgroups of $G$.
\end{eg}

\begin{eg}
  We have the following chain of groups:
  \begin{equation*}
    (\mathbb{Z}, +) \subseteq (\mathbb{Q}, +) \subseteq (\mathbb{R}, +) \subseteq (\mathbb{C}, +)
  \end{equation*}
\end{eg}

Recall that the general linear group is defined as:
\begin{equation*}
  GL_n(\mathbb{R}) = (GL_n(\mathbb{R}), \cdot) = \{A \in M_n(\mathbb{R}) : \det A \neq 0 \}
\end{equation*}

\begin{defn}[Special Linear Group]\label{defn:special_linear_group}
\index{Special Linear Group}
  The \hlnoteb{special linear group} of order $n$ of $\mathbb{R}$ is defined as
  \begin{equation*}
    SL_n(\mathbb{R}) = (SL_n(\mathbb{R}), \cdot) = \{A \in M_n(\mathbb{R}) : \det A = 1 \}
  \end{equation*}
\end{defn}

\begin{eg}\label{eg:special_linear_group_as_a_subgroup}
  Clearly, $SL_n(\mathbb{R}) \subseteq GL_n(\mathbb{R})$. Note that the identity matrix $I$ must be in $SL_n(\mathbb{R})$ since $\det I = 1$. Also, $\forall A, B \in SL_n(\mathbb{R})$, we have that
  \begin{equation*}
    \det AB = \det A \det B = 1
  \end{equation*}
  $\therefore AB \in SL_n(\mathbb{R})$. Also, since $\det A^{-1} = \frac{1}{\det A} = 1$, we also have that $\A^{-1} \in SL_n(\mathbb{R})$. We see that $SL_n(\mathbb{R})$ satisfies the \hlnoteb{Subgroup Test}, and hence it is a subgroup of $GL_n(\mathbb{R})$.
\end{eg}

\begin{defn}[Center of a Group]\label{defn:center_of_a_group}
\index{Center of a Group}
  Given a group $G$, the \hlnoteb{the center of a group $G$} is defined as
  \begin{equation*}
    Z(G) = \{z \in G \, : \, \forall g \in G \enspace zg = gz \}
  \end{equation*}
\end{defn}

\begin{eg}
  For a group $G$, $Z(G)$ is an abelian subgroup of $G$.

  \begin{proof}
    Clearly, $1_G \in Z(G)$. Let $y, z \in G$. $\forall g \in G$, we have that
    \begin{equation*}
      (yz)g = y(zg) = y(gz) = (yg)z = (gy)z = g(yz)
    \end{equation*}
    Therefore $yz \in Z(G)$ and so $Z(G)$ is closed under its operation. Also, $\forall h 
    in G$, we can write $h = (h^{-1})^{-1} = g^{-1}$. Since $z \in Z(G)$, we have that $\forall g \in G$,
    \begin{align*}
      zg = gz \iff (zg)^{-1} = (gz)^{-1} &\iff g^{-1} z^{-1} = z^{-1} g^{-1} \\
          &\iff hz^{-1} = z^{-1} h
    \end{align*}
    Therefore $z^{-1} \in Z(G)$. By the \hlnoteb{Subgroup Test}, it follows that $Z(G)$ is a subgroup of $G$.

    Finally, since $Z(G) \subseteq G$, by its definition, we have that $\forall x, y \in Z(G)$, $x, y \in G$ as well, and we have that $xy = yx$. Therefore, $Z(G)$ is abelian. \qed
  \end{proof}
\end{eg}

\begin{propo}[Intersection of Subgroups is a Subgroup]\label{propo:intersection_of_subgroups_is_a_subgroup}
  Let $H$ and $K$ be subgroups of a group $G$. Then their intersection
  \begin{equation*}
    H \cap K = \{g \in G : g \in H \, \land \, g \in K\}
  \end{equation*}
  is also a subgroup of $G$.
\end{propo}

\begin{proof}
  Since $H$ and $K$ are subgroups, we have that $1 \in H$ and $1 \in K$ and hence $1 \in H \cap K$. Let $a, b \in H \cap K$. Since $H$ and $K$ are subgroups, we have that $ab \in H$ and $ab \in K$. Therefore, $ab \in H \cap K$. Similarly, since $a^{-1} \in H$ and $a^{-1} \in K$, $a^{-1} \in H \cap K$. By the \hlnoteb{Subgroup Test}, $H \cap K$ is a subgroup of $G$. \qed
\end{proof}

\begin{propo}[Finite Subgroup Test]\label{propo:finite_subgroup_test}
\index{Finite Subgroup Test}
\marginnote{This result says that if $H$ is a finite nonempty subset, then we only need to prove that it is closed under its operation to prove that it is a subgroup. The other two conditions in the \hlnoteb{Subgroup Test} are automatically implied.}
  If $H$ is a finite nonempty subset of a group $G$, then $H$ is a subgroup if and only if $H$ is closed under its operation.
\end{propo}

\begin{proof}
  The forward direction of the proof is trivially true, since $H$ must satisfy the closure property for it to be a subgroup.

  For the converse, since $H \neq \emptyset$, let $h \in H$. Since $H$ is closed under its operation, we have that
  \begin{equation*}
    h, h^2, h^3, ...
  \end{equation*}
  are all in $H$. Since $H$ is finite, not all of the $h^n$'s are distinct. Then, $\forall n \in \mathbb{N}$, there must $\exists m \in \mathbb{N}$ such that $h^n = h^{n + m}$. Then by \autoref{propo:cancellation_laws}, $h^m = 1$ and so $1 \in H$. Also, because $1 = h^{m - 1} h$, we have that $h^{-1} = h^{m - 1}$, and thus the inverse of $h$ is also in $H$. Therefore, $H$ is a subgroup of $G$ as requried. \qed
\end{proof}

% subsection Subgroups (Continued) (end)

% section Subgroups (Continued) (end)

% chapter lecture_5_may_11th_2018 (end)

\chapter{Lecture 6 May 14th 2018}
\label{chp:lecture_6_may_14th_2018}
% chapter Lecture 6 May 14th 2018

\section{Subgroups (Continued 2)}
\label{sec:subgroups_continued_2}
% section Subgroups (Continued 2)

\subsection{Alternating Groups}
\label{sub:alternating_groups}
% subsection Alternating Groups

Recall that $\forall \sigma \in S_n$, with $\sigma \neq \epsilon$, $\sigma$ can be uniquely decomposed (up to the order) as disjoint cycles of length at least $2$. We will now present a related concept.

\begin{defn}[Transposition]\label{defn:transposition}
\index{Transposition}
  A \hlnoteb{transposition} $\sigma \in S_n$ is a cycle of length $2$, i.e. $\sigma = \begin{pmatrix} a & b \end{pmatrix}$, where $a, b \in \{1, ..., n\}$ and $a\ neq b$.
\end{defn}

\begin{eg}
  We have that\sidenote{If we apply the permutations on the right hand side, we have that
    \begin{gather*}
      1 \quad 2 \quad 3 \quad 4 \quad 5 \\
      \downarrow \\
      1 \quad 2 \quad 3 \quad 5 \quad 4 \\
      \downarrow \\
      1 \quad 4 \quad 3 \quad 5 \quad 2 \\
      \downarrow \\
      2 \quad 4 \quad 3 \quad 5 \quad 1
    \end{gather*}
  }
  \begin{equation*}
    \begin{pmatrix} 1 & 2 & 4 & 5 \end{pmatrix} = \begin{pmatrix} 1 & 2 \end{pmatrix} \begin{pmatrix} 2 & 4 \end{pmatrix} \begin{pmatrix} 4 & 5 \end{pmatrix}
  \end{equation*}
  Also, we can show that\sidenote{
  \begin{ex}
    Show that \autoref{eq:transposition_eg} is true.
  \end{ex}

  \begin{ex}
    Play around with the same idea and create a few of your own transpositions. Note that you will only be able to get an odd number of tranpositions (why?).
  \end{ex}
  }
  \begin{equation}\label{eq:transposition_eg}
    \begin{pmatrix} 1 & 2 & 4 & 5 \end{pmatrix} = \begin{pmatrix} 2 & 3 \end{pmatrix} \begin{pmatrix} 1 & 2 \end{pmatrix} \begin{pmatrix} 2 & 5 \end{pmatrix} \begin{pmatrix} 1 & 3 \end{pmatrix} \begin{pmatrix} 2 & 4 \end{pmatrix}
  \end{equation}
\end{eg}

Observe that the factorization into transpositions are \hlimpo{not unique or disjoint}. However, the following property is true.

\begin{thm}[Parity Theorem]\label{thm:parity_theorem}
\index{Parity Theorem}
  If a permutations $\sigma$ has $2$ factorizations
  \begin{equation*}
    \sigma = \gamma_1 \gamma_2 \hdots \gamma_r = \mu_1 \mu_2 \hdots \mu_s,
  \end{equation*}
  where each $\gamma_i$ and $\mu_j$ are transpositions, then $r \equiv s \mod 2$.
\end{thm}

\begin{proof}
  \hlwarn{This is the bonus question in A2. Proof shall be included after the end of the assignment.}
\end{proof}

\begin{defn}[Odd and Even Permutations]\label{defn:odd_and_even_permutations}
\index{Odd Permutations}\index{Even Permutations}
  A permutation $\sigma$ is even (or odd) if it can be written as a product of an even (or odd) number of transpositions. By \autoref{thm:parity_theorem}, a permutation must either be even or odd, but not both.
\end{defn}

\begin{thm}[Alternating Group]\label{thm:alternating_group}
\index{Alternating Group}
  For $n \geq 2$, let $A_n$ denote the set of all even permutations in $S_n$. Then
  \begin{enumerate}
    \item $\epsilon \in A_n$
    \item $\forall \sigma, \tau \in A_n \enspace \sigma \tau \in A_n$ and $\exists \sigma^{-1} \in A_n$ such that $\sigma \sigma^{-1} = \epsilon = \sigma^{-1} \sigma$
    \item $\abs{A_n} = \frac{1}{2} n!$
  \end{enumerate}
\end{thm}

\begin{note}
  From items 1 and 2, we know that $A_n$ si a subgroup of $S_n$. $A_n$ is called the \hlnoteb{alternating subgroup of degree $n$}.
\end{note}

\begin{proof}
  \begin{enumerate}
    \item We have that $\epsilon = \begin{pmatrix} 1 & 2 \end{pmatrix} \begin{pmatrix} 1 & 2 \end{pmatrix}$. Thus $\epsilon$ is even and so $\epsilon \in A_n$.
    \item $\forall \sigma, \tau \in A_n$, we may write
      \begin{align*}
        \sigma &= \sigma_1 \sigma_2 \hdots \sigma_r \quad \text{and} \\
        \tau   &= \tau_1 \tau_2 \hdots \tau_s,
      \end{align*}
      where $\sigma_i, \tau_j$ are transpositions, and $r, s$ are even integers. Then
      \begin{equation*}
        \sigma \tau = \sigma_1 \sigma_2 \hdots \sigma_r \tau_1 \tau_2 \hdots \tau_s
      \end{equation*}
      is a product of $(r + s)$ transpositions, and thus $\sigma \tau$ is even. THus $\sigma \tau \in A_n$.

      For the inverse, note that since $\sigma_i$ is a transposition, we have that $\sigma_i^2 = \epsilon$ and thus $\sigma_i^{-1} = \sigma_i$. It follows that
      \begin{align*}
        \sigma^{-1} &= (\sigma_1 \sigma_2 \hdots \sigma_r)^{-1} \\
          &= \sigma_r^{-1} \sigma_{r - 1}^{-1} \hdots \sigma_2^{-1} \sigma_1^{-1} \\
          &= \sigma_r \sigma_{r - 1} \hdots \sigma_2 \sigma_1
      \end{align*}
      which is an even permutation and
      \begin{equation*}
        \sigma \sigma^{-1} = \sigma_1 \sigma_2 \hdots \sigma_r \sigma_r \hdots \sigma_2 \sigma_1 = \epsilon.
      \end{equation*}
      Thus $\exists \sigma^{-1} \in A_n$ such that it is the inverse of $\sigma$.
    \item Let $O_n$ denote the set of odd permutations in $S_n$.\marginnote{For the proof of 3, we know that $\abs{S_n} = n!$, which is twice of the suggested order of $A_n$. Since we took out the even permutations of $S_n$, we just need to make the rest of the permutations, the odd permutations, into a set and prove that $A_n$ and this new set has the same size. One way to show this is by creating a bijection between the two.
    
        Also, note that the set of all odd permutations of $S_n$ is not a group, since
        \begin{itemize}
          \item there is no identity element in this set; and
          \item this set is not closed under map composition.
        \end{itemize}
    
        We have shown that $\epsilon$ is an even permutation, and so by the \hyperref[thm:parity_theorem]{Parity Theorem}, it cannot be an odd permutation, and there is only one identity in $S_n$. The set is not closed under map composition since if we compose two odd permutations, we would get an even permutation, which does not belong to this set.
    } Then we have $S_n = A_n \cup O_n$, and by the \hyperref[thm:parity_theorem]{Parity Theorem}, we have that $A_n \cap O_n = \emptyset$. Since $\abs{S_n} = n!$, to prove that $\abs{A_n} = \frac{1}{2} n!$, it suffices to show that $\abs{A_n} = \abs{O_n}$.
    
    Let $\gamma = \begin{pmatrix} 1 & 2 \end{pmatrix}$ and $f : A_n \to O_n$ such that $f(\sigma) = \gamma \sigma$. Since $\sigma$ is even, $\gamma \sigma$ is odd, and so $f$ is well-defined.
    
    Also, if $\gamma \sigma_1 = \gamma \sigma_2$, then by \hyperref[propo:cancellation_laws]{Cancellation Laws}, $\sigma_1 = \sigma_2$, and hence $f$ is injective.
    
    Finally, $\forall \tau \in O_n$, we have that $\gamma \tau = \sigma \in A_n$. Note that
  \begin{equation*}
    f(\sigma) = \gamma \sigma = \gamma \gamma \tau = \tau.
  \end{equation*}
  Therefore, $f$ is surjective.

  It follows that $\abs{A_n} = \abs{O_n}$. \qed
  \end{enumerate}
\end{proof}

% subsection Alternating Groups (end)

\subsection{Order of Elements}
\label{sub:order_of_elements}
% subsection Order of Elements

\begin{notation}
  If $G$ is a group and $g \in G$, we denote
  \begin{equation*}
    \lra{g} = \{ g^k : k \in \mathbb{Z} \}.
  \end{equation*}
  Note that $1 = g^0 \in \lra{g}$.

  If $x = g^m, y = g^n \in \lra{g}$ where $m, n \in \mathbb{Z}$, then
  \begin{equation*}
    xy = g^m g^n = g^{m + n} \in \lra{g}
  \end{equation*}
  and we have $\exists x^{-1} = g^{-m} \in \lra{g}$ such that
  \begin{equation*}
    xx^{-1} = g^m g^{-m} = g^0 = 1.
  \end{equation*}
\end{notation}

Along with the \hlnoteb{Subgroup Test}, we have the following proposition:

\begin{propo}[Cyclic Group as A Subgroup]\label{propo:cyclic_group_as_a_subgroup}
  If $G$ is a group and $g \in G$, then $\lra{g}$ is a subgroup of $G$.
\end{propo}

\begin{defn}[Cyclic Groups]\label{defn:cyclic_groups}
\index{Cyclic Group}
  Let $G$ be a group and $g \in G$. Then we call $\lra{g}$ the \hlnoteb{cyclic subgroup} of $G$ generated by $g$. If $G = \lra{g}$ for some $g \in G$, then we say that $G$ is a \hlnoteb{cyclic group}, and $g$ is a \hldefn{generator} of $G$.
\end{defn}

% subsection Order of Elements (end)

% section Subgroups (Continued 2) (end)

% chapter Lecture 6 May 14th 2018 (end)

\chapter{Lecture 7 May 16th 2018}%
\label{chp:lecture_7_may_16th_2018}
% chapter lecture_7_may_16th_2018

\section{Subgroups (Continued 3)}%
\label{sec:subgroups_continued_3}
% section subgroups_continued_3

\subsection{Order of Elements (Continued)}%
\label{sub:order_of_elements_continued}
% subsection order_of_elements_continued

\begin{eg}
  Consider $(\mathbb{Z}, +)$ . Note that $\forall k \in \mathbb{Z}$, we can write $k = k \cdot 1 = \underbrace{1 + 1 + \hdots + 1}_{k \text{times}}$. So we have that $(\mathbb{Z} , +) = \lra{1}$. Similarly, we would have $(\mathbb{Z} , +) = \lra{-1}$.

\noindent However, observe that $\forall n \in \mathbb{Z}$ with $n \neq \pm 1$, there is no $k \in \mathbb{Z} $ such that $k \cdot n = 1$. Therefore, $\pm 1$ are the only \hlnotea{generators} of $\mathbb{Z}$.
\end{eg}

\newthought{Let} $G$ be a group and $g \in G$. Suppose $\exists k \in \mathbb{Z}$ with $k \neq 0$ such that $g^k = 1$. Then $g^{-k} = ( g^k )^{-1} = 1$. Thus wlog, we can assume that $k \geq 1$. By the \hlnotea{Well Ordering Principle}, $\exists n \in \mathbb{N}$ such that $n$ is the smallest, such that $g^n = 1$.

With that, we may have the following definition:

\begin{defn}[Order of an Element]\index{Order of an Element}
\label{defn:order_of_an_element}
  Let $G$ be a group and $g \in G$. If $n$ is the smallest positive integer such that $g^n = 1$, we say that the order of $g$ is $n$, denoted by $o(g) = n$.

  \noindent If no such $n$ exists, then we say that $g$ has infinite order and write $o(g) = \infty$.
\end{defn}

\begin{propo}[Properties of Elements of Finite Order]
\label{propo:properties_of_elements_of_finite_order}
  Let $G$ be a group with $g \in G$ where $o(g) = n \in \mathbb{N}$. Then
  \begin{enumerate}
    \item $g^k = 1 \iff n | k$;
    \item $g^k = g^m \iff k \equiv m \mod n$; and
    \item $\lra{g} = \{1, g, g^2, ..., g^{n - 1} \}$ where each $g^i$ is distinct from others.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item $(\impliedby)$ If $n | k$, then $k = nq$ for some $q \in \mathbb{Z}$. Then
      \begin{equation*}
        g^k = g^{nq} = (g^n)^q = 1^q = 1
      \end{equation*}

      $(\implies)$ Suppose $g^k = 1$. Since $k \in \mathbb{Z}$, the \hlnotea{Division Algorithm}, we can write $k = nq + r$ with $q, r \in \mathbb{Z}$ and $0 \leq r < n$. Note $g^n = 1$. Thus
      \begin{equation*}
        g^r = g^{k - nq}  = g^k (g^n)^{-q} = 1 \cdot 1 = 1.
      \end{equation*}
      Since $0 \leq r < n$, we must have that $r = 0$. Thus $n | k$.

    \item $(\implies)$ $g^k = g^m \implies g^{k - m} = 1 \overset{\text{by } 1}{\implies} n | ( k - m ) \iff k \equiv m \mod n$
    
      $(\impliedby)$ $k \equiv m \mod n \implies \exists q \in \mathbb{Z} \enspace k = qnm$. The result follows from 1.

    \item $(\supseteq)$ is clear by definition of $\lra{g} = \{g^k : k \in \mathbb{Z}\}$.

      To prove $(\subseteq)$, let $x = g^k \in \lra{g}$ for some $k \in \mathbb{Z}$. By the \hlnotea{Division Algorithm}, $k = nq + r$ for some $q, r \in \mathbb{Z}$ and $0 \leq r < n$. Then
      \begin{equation*}
        x = g^k = g^{nq + r} = g^{nq} g^r \overset{\text{by } 1}{=} g^r.
      \end{equation*}
      Since $0 \leq r < n$, we have that $x \in \{1, g, g^2, ..., g^{n - 1} \}$. Thus $\lra{g} = \{1, g, g^2, ..., g^{n - 1} \}$.

      It remains to show that all the elements in $\lra{g}$ are distinct. Suppose $g^k = g^m$ for some $k, m \in \mathbb{Z}$ with $0 \leq k, m < n$. By 2, we have that $k \equiv m \mod 2$. Therefore, $k = m$.

      We can also use 1 by the fact that $g^{k - m} = 1$ from assumption to complete the uniqueness proof.
  \end{enumerate} \qed
\end{proof}

\begin{propo}[Property of Elements of Infinite Order]
\label{propo:property_of_elements_of_infinite_order}
  Let $G$ be a group, and $g \in G$ such that $o(g) = \infty$. Then
  \begin{enumerate}
    \item $g^k = 1 \iff k = 0$;
    \item $g^k = g^r \iff k = m$;
    \item $\lra{g} = \{..., g^{-2}, g^{-1} 1, g, g^2, ...\}$ where each $g^i$ is distinct from others.
  \end{enumerate}
\end{propo}

\begin{proof}
  It suffices to prove 1, since 2 easily becomes true with 1, and 2 $\implies$ 3.

  \begin{enumerate}
    \item $(\impliedby) \; g^0 = 1$

      $(\implies)$ Suppose for contradiction that $g^k = 1$ for some $k \in \mathbb{Z} \; k \neq 0$. Then $g^{-k} = (g^k)^{-1} = 1$. Then we can assume that $k \geq 1$. This, however, implies that $o(g)$ is finite, which contradicts our assumption. Thus $k = 0$.

    \item \begin{equation*}
      g^k = g^m \iff g^{k - m} = 1 \overset{\text{by } 1}{\iff} k - m = 0 \iff k = m
    \end{equation*}
  \end{enumerate} \qed
\end{proof}

\begin{propo}[Orders of Powers of the Element]
\label{propo:orders_of_powers_of_the_element}
  Let $G$ be a group, and $g \in G$ with $o(g) = n \in \mathbb{N}$. We have that
  \begin{equation*}
    \forall d \in \mathbb{N} \enspace d \; | \; n \implies o(g^d) = \frac{n}{d}
  \end{equation*}
\end{propo}

\begin{proof}
  Let $k = \frac{n}{d}$. Note that $(g^d)^k = g^n = 1$. It remains to show that $k$ is the smallest such positive integer. Suppose $\exists r \in \mathbb{N} \enspace (g^d)^r = 1$. Since $o(g) = n$, then $n \; | \; dr$. Then $\exists q \in \mathbb{Z} \enspace dr = nq$ by definition of divisibility. $\because n = dk$ and $d \neq 0$, we have
  \begin{align*}
    dr = dkq \overset{d \neq 0}{\implies} r = kq \implies r > k \quad \because r, k \in \mathbb{N} \implies q \in \mathbb{N}
  \end{align*}\qed
\end{proof}

% subsection order_of_elements_continued (end)

\subsection{Cyclic Groups}%
\index{Cyclic Group}
\label{sub:cyclic_groups}
% subsection cyclic_groups

Recall the definition of a cyclic groups.

\begin{defn*}[Cyclic Groups]
  Let $G$ be a group and $g \in G$. Then we call $\lra{g}$ the \hlnoteb{cyclic subgroup} of $G$ generated by $g$. If $G = \lra{g}$ for some $g \in G$, then we say that $G$ is a \hlnoteb{cyclic group}, and $g$ is a \hldefn{generator} of $G$.
\end{defn*}

\begin{propo}[Cyclic Groups are Abelian]
\label{propo:cyclic_groups_are_abelian}
  All cyclic groups are abelian.
\end{propo}

\begin{proof}
  Note that a cyclic group $G$ is of the form $G = \lra{g}$. So
  \begin{gather*}
    \forall a, b \in G \enspace \exists m, n \in \mathbb{Z} \enspace a = g^m \, \land \, b = g^n \\
    a \cdot b = g^m g^n = g^{m + n} = g^{n + m} = g^n g^m = b \cdot a
  \end{gather*}\qed
\end{proof}

% subsection cyclic_groups (end)

% section subgroups_continued_3 (end)

% chapter lecture_7_may_16th_2018 (end)

\chapter{Lecture 8 May 18th 2018}%
\label{chp:lecture_8_may_18th_2018}
% chapter lecture_8_may_18th_2018

\section{Subgroups (Continued 4)}%
\label{sec:subgroups_continued_4}
% section subgroups_continued_4

\subsection{Cyclic Groups (Continued)}%
\label{sub:cyclic_groups_continued}
% subsection cyclic_groups_continued

\begin{note}
  Consider the converse of \cref{propo:cyclic_groups_are_abelian}: Are abelian groups cyclic? \hlimpo{No!} For example, $K_4 \cong C_2 \times C_2$ is abelian but not cyclic, since no one element can generate the entire group.
\end{note}

\begin{propo}[Subgroups of Cyclic Groups are Cyclic]
\label{propo:subgroups_of_cyclic_groups_are_cyclic}
  Every subgroup of a cyclic group is cyclic.
\end{propo}

\begin{proof}
  Let $G = \lra{g}$ and $H$ be a subgroup of $G$.
  \begin{align*}
    H = \{1\} &\implies H = \lra{1} \\
    H \neq \{1\} & \implies \exists k \neq 0 \in \mathbb{Z} \enspace g^k \in H \\
                 & \implies g^{-k} \in H \quad (\because H \text{ is a group })
  \end{align*}
  We may assume that $k \in \mathbb{N}$. By the \hlnotea{Well Ordering Principle}, let $m \in \mathbb{N}$ be the smallest positive integer such that $g^m \in H$. We will now show that $H = \lra{g^m}$.

  \begin{align*}
    g^m \in H &\implies \lra{g^m} \subseteq H \\
    \because H \subseteq G = \lra{g} &\quad \forall h \in H \; \exists k \in \mathbb{Z} \; h = g^k \\
    \hlnotea{Division Algorithm} &: \exists q, r \in \mathbb{Z} \; 0 \leq r < m \quad k = mq + r \\
    h = g^k \implies g^r = g^{k - mq} &= g^k (g^m)^{-q} = g^k (1) \in H \\
    r \neq 0 \implies \exists 0 < r < m &\quad g^r \in H \quad \text{\Lightning} \quad m \text{ is the smallest +ve integer } \\
    \implies g^k \in \lra{g^m} & \implies H \subseteq \lra{g^m}
  \end{align*}
  Finally,
  \begin{equation*}
    \lra{g^m} \subseteq H \, \land \, H \subseteq \lra{g^m} &\implies H = \lra{g^m}
  \end{equation*}\qed
\end{proof}

\begin{propo}[Other generators in the same group]
\label{propo:other_generators_in_the_same_group}
  Let $G = \lra{g}$ with $o(g) = n \in \mathbb{N}$. We have
\marginnote{If we have $k$ such that $g^k \in G$, and $k$ and $n$ are coprimes, then $g^k$ is also a generator of $G$.}
  \begin{equation*}
    G = \lra{g^k} \iff \gcd(k, n) = 1
  \end{equation*}
\end{propo}

\begin{proof}
  For $(\implies)$,
  \begin{align*}
    G = \lra{g^k} &\implies g \in \lra{g^k} \implies \exists x \in \mathbb{Z} \quad g = g^{kx} \\
      &\implies 1 = g^{kx - 1} \implies n \, | \, (kx - 1) \quad (\because \cref{propo:properties_of_elements_of_finite_order}) \\
      &\implies \exists y \in \mathbb{Z} \quad kx - 1 = ny \quad (\because \hlnotea{Division Algorithm}) \\
      &\implies 1 = kx + ny
  \end{align*}
  Then
  \begin{gather*}
    \because 1 \, | \, kx \enspace \land \enspace 1 \, | \, ny \enspace \land \enspace 1 = kx + ny \\
    \gcd(k, n) = 1 \qquad (\because \hlnotea{gcd Characterization})
  \end{gather*}

  For $(\impliedby)$, note that $g \in G \implies \lra{g^k} \subseteq G$. It suffices to show that $G \subseteq \lra{g^k}$, i.e. $g \in \lra{g^k}$.
  \begin{align*}
    \gcd(k, n) = 1 &\implies \exists x, y \in \mathbb{Z} \enspace 1 = kx + ny \quad (\because \hlnotea{Bezout's Lemma}) \\
        &\implies g = g^1 = g^{kx + ny} = (g^k)^x (g^n)^y = (g^k)^x \in \lra{g^k}
  \end{align*}\qed
\end{proof}

\begin{thm}[Fundamental Theorem of Finite Cyclic Groups]
\label{thm:fundamental_theorem_of_finite_cyclic_groups}
  \marginnote{This is a significant result that classifies the structure of a cyclic group (hence its name). The theorem tells us that for a group with finite order, it has only finitely many subgroups, and the order of each of these subgroups are multiples of $n$. Inversely, there are no subgroups of $G$ where its order is some integer that does not divide $n$.
  
\noindent  \hlimpo{Note:} It is clear that $d \in \mathbb{N}$ and $d \leq n$.

In a sense, this theorem is more powerful than \cref{propo:subgroups_of_cyclic_groups_are_cyclic}.
  }
  Let $G = \lra{g}$ with $o(g) = n \in \mathbb{N}$.
  \begin{enumerate}
    \item $H$ is a subgroup of $G \implies \exists d \in \mathbb{N} \enspace d \, | \, n \quad H = \lra{g^d} \implies \abs{H} \, | \, n$.
    \item $k \, | \, n \implies \lra{g^{\frac{k}{n}}}$ is the unique subgroup of $G$ of order $k$.
  \end{enumerate}
\end{thm}

\begin{proof}
  \begin{enumerate}
    \item Note
      \begin{equation*}
        \cref{propo:subgroups_of_cyclic_groups_are_cyclic} \implies \exists m \in \mathbb{N} \enspace H = \lra{g^m} 
      \end{equation*}
      Let $d = \gcd(m, n)$. Want to show that $H = \lra{g^d}$.
      \begin{align*}
        d = \gcd(m, n) &\implies d \, | \, m \implies \exists k \in \mathbb{Z} \enspace m = dk \\
          &\implies g^m = g^{dk} = (g^d)^k \in \lra{g^d} \implies H \subseteq \lra{g^d} \\
        d = \gcd(m, n) &\implies \exists x, y \in \mathbb{Z} \quad d = mx + ny \quad (\because \hlnotea{Bezout's Lemma}) \\
          &\implies g^d = g^{mx + ny} = (g^m)^x (g^n)^y = (g^m)^x (1) \in H \\
          &\implies \lra{g^d} \subseteq H \\
          &\therefore H = \lra{g^d}
      \end{align*}
      Note: $d = \gcd(m, n) \implies d \, | \, n \implies \abs{H} = o(g^d) = \frac{n}{d}$ \\ $\because \cref{propo:orders_of_powers_of_the_element}$. Thus $\abs{H} \, | \, n$.

    \item Let $K$ be a subgroup of $G$ with order $k$ such that $k \, | \, n$. By 1, we have $K = \lra{g^d}$ with $d \, | \, n$. Note that
    \begin{equation*}
      k = \abs{K} \overset{(1)}{=} o(g^d) \overset{(2)}{=} \frac{n}{d}
    \end{equation*}
    where $(1)$ is by \cref{propo:properties_of_elements_of_finite_order} and $(2)$ is by \cref{propo:orders_of_powers_of_the_element}. Thus $d = \frac{n}{k}$ and $K = \lra{g^{\frac{n}{k}}}$
  \end{enumerate}\qed
\end{proof}

% subsection cyclic_groups_continued (end)

% section subgroups_continued_4 (end)

% chapter lecture_8_may_18th_2018 (end)

\chapter{Lecture 9 May 22nd 2018}%
\label{chp:lecture_9_may_22nd_2018}
% chapter lecture_9_may_22nd_2018

\section{Subgroups (Continued 5)}%
\label{sec:subgroups_continued_5}
% section subgroups_continued_5

\subsection{Examples of Non-Cyclic Groups}%
\label{sub:examples_of_non_cyclic_groups}
% subsection examples_of_non_cyclic_groups

\begin{eg}
  The Klein $4$-group is
  \begin{equation*}
    K_4 = \{1, a, b, c\} \enspace \text{where } a^2 = b^2 = c^2 = 1 \text{ and } ab = c.
  \end{equation*}
  We may also write
  \begin{equation*}
    K_4 = \lra{ a, b : a^2 = 1 = b^2, \, ab = ba }.
  \end{equation*}
  Note that we can replace $( a, \, b )$ by $( a, \, c )$ or $( b, \, c )$.
\end{eg}

\begin{eg}
  The symmetric group of degree $3$ is
  \begin{equation*}
    S_3 = \{\epsilon, \sigma, \sigma^2, \tau, \tau \sigma, \tau \sigma^2 \}
  \end{equation*}
  where $\sigma^3 = \epsilon = \tau^2$ and $\sigma \tau = \tau \sigma^2$. We may also express $S_3$ as
  \begin{equation*}
    S_3 = \lra{ \sigma, \tau : \sigma^3 = \epsilon = \tau^2, \, \sigma \tau = \tau \sigma^2 }
  \end{equation*}
\end{eg}

\begin{defn}[Dihedral Group]\index{Dihedral Group}
\label{defn:dihedral_group}
\marginnote{Recall from Assignment 1 that the dihedral group is a set of rigid motions for transforming a regular polygon back to its original position while changing the index of its vertices.}
  For $n \geq 2$, the \hlnoteb{dihedral group} of order $2n$ is
  \begin{equation*}
    D_{2n} = \{1, a, ..., a^{n - 1}, b, ba, ..., b^{n - 1}\ }
  \end{equation*}
  where $a^n = 1 = b^2$ and $aba = b$. Note that $a$ represents a rotation of $\frac{2 \pi}{n}$ radians, and $b$ represents a reflection through the $x$-axis
\end{defn}

\begin{eg}
  We may write the dihedral group as
  \begin{equation*}
    D_{2n} = \lra{ a, b : a^n = 1 = b^2, \, aba = b }
  \end{equation*}
\end{eg}

\begin{ex}
  Prove the following:
  \begin{enumerate}
    \item $D_4 \cong K_4$
    \item $D_6 \cong S_3$
  \end{enumerate}
\end{ex}

% subsection examples_of_non_cyclic_groups (end)

% section subgroups_continued_5 (end)

\section{Normal Subgroup}%
\label{sec:normal_subgroup}
% section normal_subgroup

\subsection{Homomorphism and Isomorphism}%
\label{sub:homomorphism_and_isomorphism}
% subsection homomorphism_and_isomorphism

\begin{defn}[Homomorphism]\index{Homomorphism}
\label{defn:homomorphism}
  Let $G, H$ be groups. A mapping
  \begin{equation*}
    \alpha : G \to H
  \end{equation*}
  is called a \hlnoteb{homomorphism} if $\forall a, b \in G$,\sidenote{
  Note that $ab$ uses the operation of $G$ while $\alpha(a)\alpha(b)$ uses the operation of $H$.}
  \begin{equation*}
    \alpha(ab) = \alpha(a)\alpha(b).
  \end{equation*}
\end{defn}

\begin{eg}[A classical example]\label{eg:homomorphism_classical_eg}
  Consider the determinant map:\marginnote{Note that $\mathbb{R}^*$ is the set of real numbers that has a multiplicative inverse.}
  \begin{equation*}
    \det : GL_n(\mathbb{R}) \to \mathbb{R}^* \quad \text{given by } A \to \det A
  \end{equation*}
  Since
  \marginnote{This is a classical example to show a homomorphism, especially since the group $GL_n(\mathbb{R})$ uses \hlnoteb{matrix multiplication} while $\mathbb{R}^*$ uses regular \hlnoteb{arithmetic multiplication}.}
  \begin{equation*}
    \det AB = \det A \det B
  \end{equation*}
  we have that the determinant map is a homomorphism.
\end{eg}

\begin{propo}[Properties of Homomorphism]
\label{propo:properties_of_homomorphism}
  Let $\alpha: G \to H$ be a group homomorphism. Then
  \begin{enumerate}
    \item $\alpha(1_G) = 1_H$
    \item $\forall g \in G \enspace \alpha(g^{-1}) = \alpha(g)^{-1}$
    \item $\forall g \in G \; \forall k \in \mathbb{Z} \enspace \alpha(g^k) = \alpha(g)^k$
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item Note that
      \begin{equation*}
        \alpha(1_G) \alpha(g) = \alpha(1_G \cdot g) = \alpha(g) = \alpha(g \cdot 1_G) = \alpha(g) \alpha(1_G)
      \end{equation*}
      Thus it must be that $\alpha(1_G) = 1_H$ for only the identity of $H$ satisfies this equation.

    \item Since $H$ is a group, we know that
      \begin{equation*}
        1_H = \alpha(g)\alpha(g)^{-1}.
      \end{equation*}
      Now with part 1, we have that
      \begin{equation*}
        \alpha(g)\alpha(g^{-1}) = \alpha(gg^{-1}) = \alpha(1_G) = 1_H = \alpha(g)\alpha(g)^{-1}.
      \end{equation*}
      By \cref{propo:cancellation_laws}, we have that $\alpha(g^{-1}) = \alpha(g)^{-1}$.

    \item This is simply a result of applying the definition repeatedly, which we can then perform an induction procedure to complete the proof.\qed
  \end{enumerate}
\end{proof}

\begin{defn}[Isomorphism]\index{Isomorphism}
\label{defn:isomorphism}
  Let $G, H$ be groups. Consider a mapping
  \begin{equation*}
    \alpha: G \to H
  \end{equation*}
  We say that $\alpha$ is an \hlnoteb{isomorphism} if it is a homomorphism and bijective.

  If $\alpha$ is an isomorphism, we say that $G$ is \hldefn{isomorphic to} to $H$, or that $G$ and $H$ are \hldefn{isomorphic}, and denote that by $G \cong H$.
\end{defn}

\begin{propo}[Isomorphism as an Equivalence Relation]
\label{propo:isomorphism_as_an_equivalence_relation}\index{Equivalence Relation}
  \begin{enumerate}
    \item \hlnotea{(Reflexive)} The identity map $G \to G$ is an isomorphism.
    \item \hlnotea{(Symmetric)} If $\sigma : G \to H$ is an isomorphism, then the inverse map $\sigma^{-1} : H \to G$ is also an isomorphism.
    \item \hlnotea{(Transitive)} If $\sigma : G \to H$ and $\tau : H \to K$, then the composition map $\tau \sigma : G \to K$ is also an isomorphism.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item The identity map is clearly bijective. For all $g_1, g_2 \in G$, we have that
      \begin{equation*}
        \alpha(g_1 g_2) = g_1 g_2 = \alpha(g_1)\alpha(g_2).
      \end{equation*}
      Thus the identity map is a homomorphism, and hence an isomorphism.
    
    \item Since $\sigma$ is a bijective map, its inverse $\sigma^{-1}$ exists and is also a bijective map. Since $\sigma$ is bijective, we have that
      \begin{equation*}
        \forall h_1, h_2 \in H \enspace \exists ! g_1, g_2 \in G \quad \sigma(g_1) = h_1, \, \sigma(g_2) = h_2.
      \end{equation*}
      Note that since $\sigma$ has a bijective inverse, we also have
      \begin{equation*}
        g_1 = \sigma^{-1}(h_1) \text{ and } g_2 = \sigma^{-1}(h_2).
      \end{equation*}
      Then since $\sigma$ is a homomorphism,
      \begin{align*}
        \sigma^{-1}(h_1 h_2) &= \sigma^{-1}(\sigma(g_1)\sigma(g_2)) = \sigma^{-1}(\sigma(g_1 g_2)) \\
          &= g_1 g_2 = \sigma^{-1}(h_1) \sigma^{-1}(h_2).
      \end{align*}

    \item We know that the composition map of two bijective map is bijective. Let $g_1, g_2 \in G$, then since both $\tau$ and $\sigma$ are homomorphisms
      \begin{equation*}
        \tau \sigma (g_1 g_2) = \tau( \sigma(g_1) \sigma(g_2) ) = \tau \sigma(g_1) \tau \sigma(g_2),
      \end{equation*}
      where we note that $\sigma(g_1), \sigma(g_2) \in H$.
\end{enumerate}\qed
\end{proof}

\begin{eg}
  Let $\mathbb{R}^+ = \{r \in \mathbb{R} : r \geq 0 \}$. Show that $(\mathbb{R}, +) \cong (\mathbb{R}^+, \cdot)$.

  \begin{solution}
    Consider the map
    \begin{equation*}
      \alpha : (\mathbb{R}, +) \to (\mathbb{R}^+, \cdot) \quad r \mapsto e^r,
    \end{equation*}
    where $e$ is the natural exponent. Note that the exponential map from $\mathbb{R}$ to $\mathbb{R}^+$ is bijective\sidenote{The image of the map covers all positive real numbers while taking all real numbers, which is the perfect candidate as a map here.}. Also, $\forall r, s \in \mathbb{R}$ we have that
    \begin{equation*}
      \alpha(r + s) = e^{r + s} = e^r e^s = \alpha(r) \alpha(s).
    \end{equation*}
    Therefore, $\alpha$ is an isomorphism and $(\mathbb{R}, +) \cong (\mathbb{R}^+, \cdot)$.\qed
  \end{solution}
\end{eg}

\begin{eg}
  Show that $(\mathbb{Q}, +) \not\cong (\mathbb{Q}^*, \cdot)$.

  \begin{solution}
    Suppose, for contradiction, that $\tau : (\mathbb{Q}, +) \to (\mathbb{Q}^*, \cdot)$ is an isomorphism. In particular, we have that $\tau$ is onto. Then $\exists q \in \mathbb{Q}$ such that $\tau(q) = 2$. Let $\tau(\frac{q}{2}) = \alpha$. Since $\tau$ is an isomorphism, we have
    \begin{equation*}
      \alpha^2 = \tau(\frac{q}{2}) \tau(\frac{q}{2}) = \tau(\frac{q}{2} + \frac{q}{2}) = \tau(q) = 2.
    \end{equation*}
    But that implies that $\alpha = \sqrt{2}$, which is clearly not rational. Thus, we know that there is no such $\tau$ and
    \begin{equation*}
      (\mathbb{Q}, +) \not\cong (\mathbb{Q}^*, \cdot)
    \end{equation*}
    as required. \qed
  \end{solution}
\end{eg}

% subsection homomorphism_and_isomorphism (end)

\subsection{Cosets and Lagrange's Theorem}%
\label{sub:cosets_and_lagrange_s_theorem}
% subsection cosets_and_lagrange_s_theorem

\begin{defn}[Coset]\index{Coset}
\label{defn:coset}
  Let $H$ be a subgroup of a group $G$.
  \begin{equation*}
    \forall a \in G \quad Ha = \{ha : h \in H\} \quad \text{is the right coset of H generated by } a
  \end{equation*}
  and
  \begin{equation*}
    \forall a \in G \quad aH = \{ah : h \in H\} \quad \text{is the left coset of H generated by } a
  \end{equation*}
\end{defn}

\begin{note}
  Note that $1H = H = H1$. Also, since $a1 = a$ and $1 \in H$, we have that $a \in aH$, and similarly so for $a \in Ha$.

  In general, $aH$ and $Ha$ are not subgroups of $G$. See example

  Also, in general, $aH \neq Ha$, since not all groups are abelian.
\end{note}

\begin{propo}[Properties of Cosets]
\label{propo:properties_of_cosets}
  Let $H$ be a subgroup of $G$, and let $a, b \in G$. Then
  \begin{enumerate}
    \item $Ha = Hb \iff ab^{-1} \in H$. In particular, $Ha = H \iff a \in H$.
    \item $a \in Hb \implies Ha = Hb$.
    \item $Ha = Hb \veebar Ha \cap Hb = \emptyset$.\sidenote{$\veebar \equiv $ XOR} Then the distinct right cosets of $H$ forms a partition of $G$.\sidenote{Note that this is true because by definition, we iterate over all elements of $G$ to construct the cosets of the subgroup $H$. The earlier part of this statement implies that cosets must be distinct (otherwise, they are the same set), and so if we take the union of these cosets, by iterating through all elements of $G$, we get that
    \begin{equation*}
      \bigcup_{a \in G} Ha = G.
    \end{equation*}
    Summarizing the above argument, we observe that the distinct cosets partitions $G$.
    }
  \end{enumerate}
  We can create an analogued version of this proposition for the left cosets.
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item For $(\implies)$,
      \begin{align*}
        Ha = Hb &\implies a = 1a \in Ha = Hb \\
                &\implies \exists h \in H \enspace a = hb \\
                &\implies ab^{-1} = h \in H.
      \end{align*}
      For $(\impliedby)$,
      \begin{align*}
        ab^{-1} \in H &\implies \forall h \in H \enspace ha = h(ab^{-1})b \in Hb \\
            &\implies Ha \subseteq Hb \\
        ab^{-1} \in H &\implies (ab^{-1})^{-1} = ba^{-1} \in H \\
            &\implies \forall h \in H \enspace hb = h(ba^{-1})a \in Ha \\
            &\implies Hb \subseteq Ha
      \end{align*}
      Let $b = 1$. Then
      \begin{equation*}
        Ha = H \iff a \in H \qquad \because 1^{-1} = 1
      \end{equation*}

    \item Note
      \begin{equation*}
        a \in Hb \implies \exists h \in H \enspace a = hb \implies ab^{-1} \in H \overset{\text{by 1}}{\implies} Ha = Hb
      \end{equation*}

    \item Trivially, if $Ha \cap Hb = \emptyset$, we are done.
      \begin{align*}
        &Ha \cap Hb \neq \emptyset \\ 
        &\implies \exists x \in Ha \cap Hb \\
        &\implies ( x \in Ha \overset{\text{by 1}}{\implies} Hx = Hb ) \, \land \, ( x \in Hb \overset{\text{by 1}}{\implies} Hx = Hb ) \\
        &\implies Ha = Hb
      \end{align*}
  \end{enumerate}\qed
\end{proof}

By \cref{propo:properties_of_cosets}, we have that $G$ can be written as a disjoint union of cosets of a subgroup $H$. We now define the following terminology that we shall use for the upcoming content.

\begin{defn}[Index]\index{Index}
\label{defn:index}
  Let $H$ be a subgroup of a group $G$. We call the number of disjoint cosets of $H$ in $G$ as the \hlnoteb{index} of $H$ in $G$, and denote this number by $[G : H]$.
\end{defn}

% subsection cosets_and_lagrange_s_theorem (end)

% section normal_subgroup (end)

% chapter lecture_9_may_22nd_2018 (end)

\chapter{Lecture 10 May 23rd 2018}%
\label{chp:lecture_10_may_23rd_2018}
% chapter lecture_10_may_23rd_2018

\section{Normal Subgroup (Continued)}%
\label{sec:normal_subgroup_continued}
% section normal_subgroup_continued

\subsection{Cosets and Lagrange's Theorem (Continued)}%
\label{sub:cosets_and_lagrange_s_theorem_continued}
% subsection cosets_and_lagrange_s_theorem_continued

\begin{thm}[Lagrange's Theorem]
\index{Lagrange's Theorem}
\label{thm:lagrange_s_theorem}
  Let $H$ be a subgroup of a \hlimpo{finite} group $G$. Then
  \begin{equation*}
    \abs{H} \, \Big| \, \abs{G} \text{ and } [G : H] = \frac{\abs{G}}{\abs{H}}
  \end{equation*}
\end{thm}

\begin{proof}
  Since $G$ is finite, there can only be finitely many cosets of $H$. Let $k = [G : H]$ and $Ha_1, Ha_2, ..., Ha_k$ be the distinct right cosets of $H$ in $G$. By \cref{propo:properties_of_cosets}, we have that these cosets partition $G$, i.e.
  \begin{equation*}
    G = \bigcup_{i = 1}^{k} Ha_i.
  \end{equation*}
  Note that by the definition of a right coset, the map 
  \begin{equation*}
    H \to Hb \; \text{ defined by } \; h \mapsto hb
  \end{equation*}
  is a surjection from $H$ to $Hb$. By \hyperref[propo:cancellation_laws]{Cancellation Laws}, the map is injective, since if $hb_1 = hb_2$, then $b_1 = b_2$. Therefore, for $i = 1, ..., k$,
  \begin{equation*}
    \abs{H} = \abs{Ha_i}.
  \end{equation*}
  Then we have
  \begin{equation*}
    \abs{G} = k \abs{H} \implies \abs{H} \, \Big| \, \abs{G} \, \land \, [G : H] = k = \frac{\abs{G}}{\abs{H}}
  \end{equation*}\qed
\end{proof}

\begin{crly}
\label{crly:lagrange_s_theorem_crly1}
  \begin{enumerate}
    \item If $G$ is a finite group and $g \in G$, then $o(g) \, \Big| \, G$.
    \item If $G$ is a finite group and $\abs{G} = n$, then $g^n = 1$.
  \end{enumerate}
\end{crly}

\begin{proof}
  \begin{enumerate}
    \item Let $H = \lra{g}$. Then by \autoref{thm:lagrange_s_theorem}, $o(g) = \abs{H} \, \Big| \, \abs{G}$.

    \item For some $g \in G$, let $o(g) = m \in \mathbb{Z} \setminus \{0\}$. Then by 1, $m \, | \, n$ and so $g^n = (g^m)^{\frac{n}{m}} = 1$.
  \end{enumerate}\qed
\end{proof}

\begin{note}
  Let $n \in \mathbb{N} \setminus \{1\}$. \hldefn{Euler's Totient Function}, or more generally written as \hldefn{Euler's $\phi$-function} is defined as
  \begin{equation}\label{eq:euler_s_totient_function}
    \phi(n) \equiv \Big| \big\{k \in \{1, ..., n - 1\} \, : \, \gcd(k, n) = 1 \big\} \Big|.
  \end{equation}
  Note that the set $\mathbb{Z}_n^*$ under multiplication has a similar definition to the set on the RHS, since the only numbers from $1$ to $n$ that has an inverse are those that are coprime with $n$. Thus $\phi(n) = \abs{\mathbb{Z}_n^*}$.

  With \cref{crly:lagrange_s_theorem_crly1}, we have \hldefn{Euler's Theorem} that states that
  \begin{equation}\label{eq:euler_s_theorem}
    \forall a \in \mathbb{Z} \enspace \gcd(a, n) = 1 \implies a^{\phi(n)} \equiv 1 \mod n.
  \end{equation}
  If $n = p$ where $p$ is some prime number, then Euler's Theorem implies \hldefn{Fermat's Little Theorem}, i.e. $a^{p - 1} \equiv 1 \mod p$.
\end{note}

\begin{crly}
\label{crly:lagrange_s_theorem_crly2}
  If $p$ is prime, then every group $G$ of order $p$ is cyclic. In fact, $g = \lra{g}$ fpr $g \neq 1 \in G$. Hence, the only subgroup of $G$ are $\{1\}$ and $G$ itself.
\end{crly}

\begin{proof}
  \hlwarn{There's something that I'd like to make sure before putting down this proof}
\end{proof}

\begin{crly}
\label{crly:lagrange_s_theorem_crly3}
  Let $H$ and $K$ be finite subgroups of $G$. If $\gcd(\abs{H}, \abs{K}) = 1$, then $H \cap K = \{1\}$.
\end{crly}

\begin{proof}
  Since $H \cap K$ is a subgroup of $H$ and of $K$, by \autoref{thm:lagrange_s_theorem}, $\abs{H \cap K} \Big| \abs{H} \, \land \, \abs{H \cap K} \Big| \abs{K}$. By assumption that $\gcd(\abs{H}, \abs{K}) = 1$, we have\sidenote{$\abs{H \cap K}$ is a common divisor for $\abs{H}$ and $\abs{K}$. But $\gcd(\abs{H}, \abs{K}) = 1$} that $\abs{H \cap K} = 1$, and hence $\abs{H \cap K} = \{ 1 \}$. \qed
\end{proof}

% subsection cosets_and_lagrange_s_theorem_continued (end)

\subsection{Normal Subgroup}%
\label{sub:normal_subgroup}
% subsection normal_subgroup

We have seen that given $H$ is a subgroup of a group $G$ and $g \in G$, $gH$ and $Hg$ are generally not the same.

\begin{defn}[Normal Subgroup]\index{Normal Subgroup}
\label{defn:normal_subgroup}
  Let $H$ be a subgroup of a group $G$. If $\forall g \in G$, we have $Hg = gH$, then we say that $H$ is a \hlnoteb{normal subgroup} of $G$, and write
  \begin{equation*}
    H \triangleleft G
  \end{equation*}
\end{defn}

\begin{eg}
  $\{1\} \triangleleft G$ and $G \triangleleft G$.
\end{eg}

\begin{eg}
  The \hyperref[defn:center_of_a_group]{center}, $Z(G)$, of a group $G$ is an abelian group. By \cref{defn:normal_subgroup},
  \begin{equation*}
    Z(G) \triangleleft G.
  \end{equation*}
\end{eg}

\begin{eg}
  If $G$ is abelian, then every subgroup of $G$ is normal in $G$.
\end{eg}

\begin{propo*}[Normality Test]
  Let $H$ be a subgroup of $G$. The following are equivalent:
  \begin{enumerate}
    \item $H \triangleleft G$;
    \item $\forall g \in G \quad gHg^{-1} \subseteq H$;
    \item $\forall g \in G \quad gHg^{-1} = H$ \sidenote{This means that
    \begin{equation*}
      H \triangleleft G \iff H \text{ is the only conjugate of } H
    \end{equation*}}
  \end{enumerate}
\end{propo*}

% subsection normal_subgroup (end)

% section normal_subgroup_continued (end)

% chapter lecture_10_may_23rd_2018 (end)

\nobibliography*
\bibliography{bibliography}

\printindex

\chapter{List of Symbols}
% chapter List of Symbols

\begin{tabular}{l l}
  $M_n(\mathbb{R})$  & set of $n \times n$ matrices over $\mathbb{R}$ \\
  $\mathbb{Z}_n^*$   & set of integers modulo $n$; each element has its multiplicative inverse \\
  $S_n$              & symmetry group of degree $n$ \\
  $D_{2n}$           & dihedral group of degree $n$; a subset of $S_n$ \\
  $K_n$              & Klein $n$-group \\
  $A_n$              & alternating group of degree $n$; a subset of $S_n$ \\
  $\abs{D_{2n}}$     & order of the dihedral group; the size of the dihedral group \\
  $\begin{pmatrix} 1 & 2 & \hdots & n \end{pmatrix}$ & An $n$-cycle \\
  $\det A$           & determinant of matrix $A$ \\
  $GL_n(\mathbb{R})$ & \tworow{l}{general linear group of degree $n$;}{the set that contains elements of $M_n(\mathbb{R})$ with non-zero determinant} \\
  $SL_n(\mathbb{R})$ & \tworow{l}{special linear group of order $n$;}{the set that contains elements of $GL_n(\mathbb{R})$ with determinant of $1$} \\
  $Z(G)$             & center of group $G$ \\
  $\lra{g}$          & cyclic group with generator $g$ \\
  $n \; | \; d$      & $n$ divides $d$
\end{tabular}

% chapter List of Symbols (end)

\end{document}
