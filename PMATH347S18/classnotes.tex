\documentclass[notoc,notitlepage]{tufte-book}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\renewcommand{\baselinestretch}{1.1}

\usepackage{tikz-cd}
\input{latex-classnotes-preamble.tex}

% Main Body
\title{PMATH347S18 - Groups \& Rings}
\author{Johnson Ng}

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\tableofcontents

\chapter*{List of Definitions}
\theoremlisttype{all}
\listtheorems{defn}

\chapter*{List of Theorems}
\theoremlisttype{allname}
\listtheorems{axiom,lemma,thm,crly,propo}

\chapter{Lecture 1 May 02nd 2018}
  \label{chapter:lecture_1_may_02nd_2018}

\section{Introduction} % (fold)
\label{sec:introduction}

\subsection{Numbers} % (fold)
\label{sub:numbers}

The following are some of the number sets that we are already familiar with:
\begin{gather*}
  \mathbb{N} = \{1, 2, 3, ...\} \qquad \mathbb{Z} = \{.., -2, -1, 0, 1, 2, ...\} \\
  \mathbb{Q} = \left\{\frac{a}{b} : a \in \mathbb{Z}, b \in \mathbb{N} \right\} \qquad \mathbb{R} = \text{ set of real numbers} \\
  \mathbb{C} = \{a + bi : a, b \in \mathbb{R}, i = \sqrt{-1} \} = \text{ set of complex numbers} 
\end{gather*}
For $n \in \mathbb{Z}$, let $\mathbb{Z}_n$ denote the set of integers modulo $n$, i.e.
\begin{equation*}
  \mathbb{Z}_n = \{ [0], [1], ..., [n - 1] \}
\end{equation*}
where the $[r]$, $0 \leq r \leq n - 1$, are the congruence classes, i.e.
\begin{equation*}
  [r] = \{z \in \mathbb{Z} : z \equiv r \mod n\}
\end{equation*}

These sets share some common properties, e.g. $+$ and $\times$. Let's try to break that down to make further observation.

\newthought{Note that} for $R = \mathbb{N}, \, \mathbb{Z}, \, \mathbb{Q}, \, \mathbb{R}, \, \mathbb{C},$ or $\mathbb{Z}_n$, $R$ has 2 operations, i.e. addition and multiplication.

\paragraph{Addition} If $r_1, r_2, r_3 \in R$, then
\begin{itemize}
  \item (\hldefn{closure}) $r_1 + r_2 \in R$
  \item (\hldefn{associativity}) $r_1 + (r_2 + r_3) = (r_1 + r_2) + r_3$
\end{itemize}
Also, if $R \neq \mathbb{N}$, then $\exists 0 \in R$ (the \hldefn{additive identity}) such that
\begin{equation*}
  \forall r \in R \quad r + 0 = r = 0 + r.
\end{equation*}
Also, $\forall r \in R$, $\exists (-r) \in R$ such that
\begin{equation*}
  r + (-r) = 0 = (-r) + r.
\end{equation*}

\paragraph{Multiplication} For $r_1, r_2, r_3 \in R$, we have
\begin{itemize}
  \item (\hlnoteb{closure}) $r_1 r_2 \in R$
  \item (\hlnoteb{associativity}) $r_1 (r_2 r_3) = (r_1 r_2) r_3$
\end{itemize}
Also, $\exists 1 \in R$ (a.k.a the \hldefn{mutiplicative identity}), such that
\begin{equation*}
  \forall r \in R \quad r \cdot 1 = r = 1 \cdot r.
\end{equation*}
Finally, for $R = \mathbb{Q}, \, \mathbb{R},$ or $\mathbb{C}$, $\forall r \in R, \, \exists r^{-1} \in R$ such that
\begin{equation*}
  r \cdot r^{-1} = 1 = r^{-1} \cdot r.
\end{equation*}
Note that for $R = \mathbb{Z}_n$, where $n \in \mathbb{Z}$, not all $[r] \in \mathbb{Z}_n$ have a multiplicative inverse. For example, for $[2] \in \mathbb{Z}_4$, there is no $[x] \in \mathbb{Z}_4$ such that $[2][x] = [1]$.\sidenote{This is best proven using techniques introduced in MATH135/145.}

% subsection numbers (end)

\subsection{Matrices}
  \label{sub:matrices}

For $n \in \mathbb{N} \setminus \{1\}$, an $n \times n$ matrix over $\mathbb{R}$ \sidenote{$\mathbb{R}$ can be replaced by $\mathbb{Q}$ or $\mathbb{C}$.} is an $n \times n$ array that can be expressed as follows:
\begin{equation*}
  A = [a_{ij}] = \begin{bmatrix}
    a_{11} & a_{12} & \hdots & a_{1n} \\
    a_{21} & a_{22} & \hdots & a_{2n} \\
    \vdots & \vdots &        & \vdots \\
    a_{n1} & a_{n2} & \hdots & a_{nn}
  \end{bmatrix}
\end{equation*}
where for $1 \leq i, j \leq n$, $a_{ij} \in \mathbb{R}$. We denote $M_n(\mathbb{R})$ as the set of all $n \times n$ matrices over $\mathbb{R}$.

As in \cref{sub:numbers}, we can perform \hlnotea{addition and multiplication} on $M_n(\mathbb{R})$.

\paragraph{Matrix Addition} Given $A = [a_{ij}], B = [b_{ij}], C = [c_{ij}] \in M_n(\mathbb{R})$, we define matrix addition as
\begin{equation*}
  A + B = [a_{ij} + b_{ij}],
\end{equation*}
which immediately gives the \hlnoteb{closure property}, since $a_{ij} + b_{ij} \in \mathbb{R}$ and hence $A + B \in M_n(\mathbb{R})$. Also, by this definition, we also immediately obtain the \hlnoteb{associativity property}, i.e.
\begin{equation*}
  A + (B + C) = (A + B) + C.
\end{equation*}
We define the zero matrix as
\begin{equation*}
  0 = \begin{bmatrix}
    0      &   0    & \hdots &   0 \\
    0      &   0    & \hdots &   0 \\
    \vdots & \vdots &        & \vdots \\
    0      &   0    & \hdots &   0
  \end{bmatrix}.
\end{equation*}
Then we have that $0$ is the \hlnoteb{additive identity}, i.e.
\begin{equation*}
  A + 0 = A = 0 + A.
\end{equation*}
Finally, $\forall A \in M_n(\mathbb{R})$, $\exists (-A) \in M_n(\mathbb{R})$ (the \hlnoteb{additive inverse}) such that
\begin{equation*}
  A + (-A) = 0 - (-A) + A.
\end{equation*}

Note that in this case, we also have that that the operation is \hlnoteb{commutative}, i.e.
\begin{equation*}
  A + B = B + A.
\end{equation*}

\paragraph{Matrix Multiplication} Given $A = [a_{ij}], B = [b_{ij}], C = [c_{ij}] \in M_n(\mathbb{R})$, we define the matrix multiplication as
\begin{equation*}
  AB = [d_{ij}] \text{ where } c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj} \in \mathbb{R}.
\end{equation*}
Clearly, $AB \in M_n(\mathbb{R})$, i.e. it is \hlnoteb{closed under matrix multiplication}. Also, we have that, under such a defintion, matrix multiplication is \hlnoteb{associative}, i.e.
\begin{equation*}
  A(BC) = (AB)C.
\end{equation*}
Define the identity matrix, $I \in M_n(\mathbb{R})$, as follows:
\begin{equation*}
  I = \begin{bmatrix}
    1      &   0    & \hdots & 0 \\
    0      &   1    & \hdots & 0 \\
    \vdots & \vdots &        & \vdots \\
    0      &   0    & \hdots & 1
  \end{bmatrix}.
\end{equation*}
Then we have that $I$ is the \hlnoteb{multiplicative identity}, since
\begin{equation*}
  AI = A = IA.
\end{equation*}
However, contrary to matrix addition, $\forall A \in M_n(\mathbb{R})$, it is not always true that $\exists A^{-1} \in M_n(\mathbb{R})$ such that\marginnote{This is especially true if the \hlnotea{determinant} of $A$ is $0$.}
\begin{equation*}
  AA^{-1} = I = A^{-1} A.
\end{equation*}

Also, we can always find some $A, B \in M_n(\mathbb{R})$ such that
\begin{equation*}
  AB \neq BA,
\end{equation*}
i.e. matrix multiplication is not always commutative.

\newthought{The common properties} of the operations from above: \hlimpo{closure, associativity, and existence of an inverse}, are not unique to just addition and multiplication. We shall see in the next lecture that there are other operations where these properties will continue to hold, e.g. \hlnoteb{permutations}.

% subsection matrices (end)

% section introduction (end)

% chapter lecture_1_may_02nd_2018 (end)

\chapter{Lecture 2 May 04th 2018}
  \label{chapter:lecture_2_may_04th_2018}

\section{Introduction (Continued)} % (fold)
\label{sec:introduction_continued}

\subsection{Permutations} % (fold)
\label{sub:permutations}

\begin{defn}[Injectivity]\label{defn:injectivity}
\index{Injectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{injective} (or \hldefn{one-to-one}) if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
\end{defn}

\begin{defn}[Surjectivity]\label{defn:surjectivity}
\index{Surjectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{surjective} (or \hldefn{onto}) if $\forall y \in Y \enspace \exists x \in X \enspace f(x) = y$.
\end{defn}

\begin{defn}[Bijectivity]\label{defn:bijectivity}
\index{Bijectivity}
  Let $f: X \to Y$ be a function. We say that $f$ is \hlnoteb{bijective} if it is both \hlnoteb{injective} and \hlnoteb{surjective}.
\end{defn}

\begin{defn}[Permutations]\label{defn:permutations}
\index{Permutations}
  Given a non-empty set $L$, a permutation of $L$ is a bijection from $L$ to $L$. The set of all permutations of $L$ is denoted by $S_L$.
\end{defn}

\begin{eg}
  \label{eg:permutations_first}
  Consider the set $L = \{1, 2, 3\}$, which has the following $6$ different permutations: \marginnote{\begin{note}
    \begin{equation*}
      \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}
    \end{equation*}
    indicates the bijection $\sigma: \{1, 2, 3\} \to \{1, 2, 3\}$ with $\sigma(1) = 1$, $\sigma(2) = 3$ and $\sigma(3) = 2$.
  \end{note}}

  \begin{gather*}
     \begin{pmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{pmatrix} \\
     \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 1 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 3 & 1 & 2 \end{pmatrix} \quad \begin{pmatrix} 1 & 2 & 3 \\ 3 & 2 & 1 \end{pmatrix}
   \end{gather*} 
\end{eg}

\newthought{For $n \in \mathbb{N}$}, we denote $S_n := S_{\{1, 2, ..., n\}}$, the set of all permutations of $\{1, 2, ..., n\}$. \cref{eg:permutations_first} shows the elements of the set $S_3$.

\begin{defn}[Order]\label{defn:order}
\index{Order}
  The \hlnoteb{order} of a set $A$, denoted by $\abs{A}$, is the cardinality of the set.
\end{defn}

\begin{eg}
  \label{eg:order_of_prev_eg}
  We have seen that the order of $S_3$, $\abs{S_3}$ is $6 = 3!$.
\end{eg}

\begin{propo}\label{propo:order_of_Sn_is_n}
  $\abs{S_n} = n!$
\end{propo}

\begin{proof}
  $\forall \sigma \in S_n$, there are $n$ choices for $\sigma(1)$, $n - 1$ choices for $\sigma(2)$, ..., $2$ choices for $\sigma(n - 1)$, and finally $1$ choice for $\sigma(n)$. \qed
\end{proof}

\paragraph{Do elements of $S_n$ share the same properties as what we've seen in the numbers?} Given $\sigma, \tau \in S_n$, we can \hlnotea{compose} the 2 together to get a third element in $S_n$, namely $\sigma \tau$ (wlog), where $\sigma \tau : \{1, ..., n\} \to \{1, ..., n\}$ is given by $\forall x \in \{1, ..., n\}$, $x \mapsto \sigma( \tau(x) )$.

It is important to note that $\because \sigma, \tau$ are \hlimpo{both bijective}, $\sigma \tau$ is also bijective. Thus, together with the fact that $\sigma \tau : \{1, ..., n\} \to \{1, ..., n\}$, we have that $\sigma \tau \in S_n$ by definition of $S_n$.

$\therefore \forall \sigma, \tau \in S_n, \; \sigma \tau, \tau \sigma \in S_n$, but $\sigma \tau \neq \tau \sigma$ in general. The following is an example of the stated case:

\begin{eg}
  \label{eg:commutativity_of_Sn}
  Let
  \begin{equation*}
    \sigma &= \begin{pmatrix}
      1 & 2 & 3 & 4 \\
      3 & 4 & 1 & 2
    \end{pmatrix}, \text{ and } 
    \tau &= \begin{pmatrix}
      1 & 2 & 3 & 4 \\
      2 & 4 & 3 & 1
    \end{pmatrix}.
  \end{equation*}
  Compute $\sigma \tau$ and $\tau \sigma$ to show that they are not equal.

  \begin{solution}
    \begin{equation*}
      \sigma \tau &= \begin{pmatrix}
        1 & 2 & 3 & 4 \\
        4 & 2 & 1 & 3
      \end{pmatrix} \text{ but } 
      \tau \sigma &= \begin{pmatrix}
        1 & 2 & 3 & 4 \\
        3 & 1 & 2 & 4
      \end{pmatrix}
    \end{equation*}
  \end{solution}
\end{eg}

Perhaps what is interesting is the question of: \textbf{when does commutativity occur?} One such case is when $\sigma$ and $\tau$ have support sets that are disjoint\sidenote{This is proven in A1}.

On the other hand, the associative property holds\sidenote{
  \begin{ex}
    Prove this as an exercise.
  \end{ex}
}, i.e.
\begin{equation*}
  \forall \sigma, \tau, \mu \in S_n \enspace \sigma (\tau \mu) = (\sigma \tau) \mu
\end{equation*}

The set $S_n$ also has an identity element\sidenote{
  \begin{ex}
    Verify that the given identity element is indeed the identity, i.e.
    \begin{equation*}
      \forall \sigma \in S_n \enspace \sigma \epsilon = \sigma = \epsilon \sigma.
    \end{equation*}
  \end{ex}
}, namely
\begin{equation*}
  \epsilon = \begin{pmatrix}
    1 & 2 & \hdots & n \\
    1 & 2 & \hdots & n
  \end{pmatrix}
\end{equation*}

Finally, $\forall \sigma \in S_n$, since $\sigma$ is a bijection, we have that its inverse function, $\sigma^-1$ is also a bijection, and thus satisfies the requirements to be in $S_n$. We call $\sigma^{-1} \in S_n$ to be the \hldefn{inverse permutation} of $\sigma$, such that
\begin{equation*}
  \forall x, y \in \{1, ..., n\} \quad \sigma^{-1}(x) = y \iff \sigma(y) = x.
\end{equation*}
It follows, immediately, that
\begin{equation*}
  \sigma \big( \sigma^{-1}(x) \big) = x \, \land \, \sigma^{-1} \big( \sigma(y) \big) = y.
\end{equation*}
$\therefore$ We have that 
\begin{equation*}
  \sigma \sigma^{-1} = \epsilon = \sigma^{-1} \sigma.
\end{equation*}

\begin{eg}
  \label{eg:inverse_permutation}
  Find the inverse of
  \begin{equation*}
    \sigma = \begin{pmatrix}
      1 & 2 & 3 & 4 & 5 \\
      4 & 5 & 1 & 2 7 3
    \end{pmatrix}
  \end{equation*}

  \begin{solution}
    By rearranging the image in ascending order, using them now as the object and their respective objects as their image, construct
    \begin{equation*}
      \tau = \begin{pmatrix}
        1 & 2 & 3 & 4 & 5 \\
        3 & 4 & 5 & 1 & 2
      \end{pmatrix}.
    \end{equation*}
    It can easily (although perhaps not so prettily) be shown that
    \begin{equation*}
      \sigma \tau = \epsilon = \tau \sigma.
    \end{equation*}
  \end{solution}
\end{eg}

With all the above, we have for ourselves the following proposition:

\begin{propo}[Properties of $S_n$]\label{propo:properties_of_Sn}
  We have
  \begin{enumerate}
    \item $\forall \sigma, \tau \in S_n \enspace \sigma \tau, \tau \sigma \in S_n$.
    \item $\forall \sigma, \tau, \mu \in S_n \enspace \sigma (\tau \mu) = (\sigma \tau) \mu$.
    \item $\exists \epsilon \in S_n \enspace \forall \sigma \in S_n \enspace \sigma \epsilon = \sigma = \epsilon \sigma$.
    \item $\forall \sigma \in S_n \enspace \exists! \sigma^{-1} \in S_n \enspace \sigma \sigma^{-1} = \epsilon = \sigma^{-1} \sigma$.
  \end{enumerate}
\end{propo}

\newthought{Consider}
\begin{equation*}
  \sigma = \begin{pmatrix}
    1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    3 & 1 & 7 & 6 & 9 & 4 & 2 & 5 & 8 & 10
  \end{pmatrix} \in S_{10}
\end{equation*}
If we represent the action of $\sigma$ geometrically, we get

\begin{tabular}{c c c}
  \begin{tikzcd}
   & 1 \arrow[rd] &  \\
  2 \arrow[ru] &  & 3 \arrow[ld] \\
   & 7 \arrow[lu] & 
  \end{tikzcd}
  &
  \begin{tikzcd}
  4 \arrow[dd, bend left] \\
   \\
  6 \arrow[uu, bend left]
  \end{tikzcd}
  &
  \begin{tikzcd}
   & 5 \arrow[rdd] &  \\
   &  &  \\
  8 \arrow[ruu] &  & 9 \arrow[ll]
  \end{tikzcd}
  \\ &
  \begin{tikzcd}
    10 \arrow[loop below]
  \end{tikzcd}
\end{tabular}
We observe that $\sigma$ can be \hlnotea{decomposed} into one $4$-cycle, $\begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix}$, one $2$-cycle, $\begin{pmatrix} 4 & 6 \end{pmatrix}$, one $3$-cycle, $\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}$, and one $1$-cycle, $\begin{pmatrix} 10 \end{pmatrix}$.

Note that these cycles are (pairwise) \hlnotea{disjoint}, and we can write\sidenote{We generally do not include the $1$-cycle and assume that by excluding them, it is known that any number that is supposed to appear loops back to themselves.}
\begin{equation*}
  \sigma = \begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix}\begin{pmatrix} 4 & 6 \end{pmatrix}\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}
\end{equation*}
Note that we may also write
\begin{align*}
  \sigma &= \begin{pmatrix} 4 & 6 \end{pmatrix}\begin{pmatrix} 5 & 9 & 8 \end{pmatrix}\begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix} \\
    &= \begin{pmatrix} 6 & 4 \end{pmatrix}\begin{pmatrix} 9 & 8 & 5 \end{pmatrix}\begin{pmatrix} 7 & 2 & 1 & 3 \end{pmatrix}
\end{align*}
It is interesting to note that the cycles can rotate their ``elements'' in a \hlnotea{cyclic} manner, i.e.
\begin{gather*}
  \begin{pmatrix} 1 & 3 & 7 & 2 \end{pmatrix} = \begin{pmatrix} 7 & 2 & 1 & 3 \end{pmatrix} \neq \begin{pmatrix} 1 & 2 & 7 & 3 \end{pmatrix}.
\end{gather*}
Although the decomposition of the cycle notation is not unique (i.e. you may rearrange them), each individual cycle is unique, and is proven below\sidenote{See bonus question of A1. Proof will be included in the notes once the assignment is over.}.

\begin{thm}[Cycle Decomposition Theorem]\label{thm:cycle_decomposition_theorem}
\index{Cycle Decomposition Theorem}
  If $\sigma \in S_n$, $\sigma \neq \epsilon$, then $\sigma$ is a product of (one or more) disjoint cycles of length at least $2$. This factorization is unique up to the order of the factors.
\end{thm}

\begin{note}[Convention]
 Every permutation in $S_n$ can be regarded as a permutation of $S_{n + 1}$ by fixing the permutation of $n + 1$. Therefore, we have that
 \begin{equation*}
   S_1 \subseteq S_2 \subseteq \hdots \subseteq S_n \subseteq S_{n + 1} \subseteq \hdots
 \end{equation*}
\end{note}

% subsection permutations (end)

% section introduction_continued (end)

% chapter lecture_2_may_04th_2018 (end)

\chapter{Lecture 3 May 07th 2018}
  \label{chapter:lecture_3_may_07th_2018}

\section{Groups} % (fold)
\label{sec:groups}

\subsection{Groups} % (fold)
\label{sub:groups}

\begin{defn}[Groups]\label{defn:groups}
\index{Groups}
  Let $G$ be a set and $*$ an operation on $G \times G$. We say that $G = (G, *)$ is a \hlnoteb{group} if it satisfies\sidenote{If you wonder why the uniqueness is not specified for \hlnoteb{Identity} and \hlnoteb{Inverse}, see \cref{propo:uniqueness_of_group_identity_and_group_element_inverse}.}
  \begin{enumerate}
    \item \hlnoteb{Closure}: $\forall a, b \in G \quad a * b \in G$
    \item \hlnoteb{Associativity}: $\forall a, b, c \in G \quad a * (b * c) = (a * b) * c$
    \item \hlnoteb{Identity}: $\exists e \in G \enspace \forall a \in G \quad a * e = a = e * a$
    \item \hlnoteb{Inverse}: $\forall a \in G \enspace \exists b \in G \quad a * b = e = b * a$
  \end{enumerate}
\end{defn}

\begin{defn}[Abelian Group]\label{defn:abelian_group}
\index{Abelian Group}
  A group $G$ is said to be abelian if $\forall a, b \in G$, we have $a * b = b * a$.
\end{defn}

\begin{propo}[Group Identity and Group Element Inverse]\label{propo:uniqueness_of_group_identity_and_group_element_inverse}
  Let $G$ be a group and $a \in G$.
  \begin{enumerate}
    \item The identity of $G$ is unique.
    \item The inverse of $a$ is unique.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item If $e_1, e_2 \in G$ are both identities of $G$, then we have
      \begin{equation*}
        e_1 \overset{(1)}{=} e_1 * e_2 \overset{(2)}{=} e_2
      \end{equation*}
      where $(1)$ is because $e_2$ is an identity and $(2)$ is because $e_1$ is an identity.

    \item Let $a \in G$. If $b_1, b_2 \in G$ are both the inverses of $a$, then we have
      \begin{equation*}
        b_1 = b_1 * e = b_1 * (a * b_2) \overset{(1)}{=} e * b_2 = b_2
      \end{equation*}
      where $(1)$ is by associativity.
  \end{enumerate}
\end{proof}

\begin{eg}
  The sets $(\mathbb{Z}, +), \, (\mathbb{Q}, +), \, (\mathbb{R}, +)$, and $(\mathbb{C}, +)$ are all abelian, wehre the additive identity is $0$, and the additive inverse of an element $r$ is $(-r)$.
\end{eg}

\begin{note}
  $(\mathbb{N}, +)$ is not a group for neither does it have an identity nor an inverse for any of its elements.
\end{note}

\begin{eg}
  The sets $(\mathbb{Q}, \cdot), \, (\mathbb{R}, \cdot)$ and $(\mathbb{C}, \cdot)$ are \hlwarn{not} groups, since $0$ has no multiplicative inverse in $\mathbb{Q}, \mathbb{R}$ or $\mathbb{C}$.
\end{eg}

We may define that for a set $S$, let $S^* \subseteq S$ contain all the elements of $S$ that has a multiplicative inverse. For example, $\mathbb{Q}^* = \mathbb{Q} \setminus \{0\}$. Then, $(\mathbb{Q}, \cdot), (\mathbb{R}, \cdot)$ and $(\mathbb{C}, \cdot)$ are groups and are in fact abelian, where the multiplicative identity is $1$ and the multiplicative of an element $r$ is $\frac{1}{r}$.

\begin{eg}
  The set $\big( M_n(\mathbb{R}), + \big)$ is an abelian group, where the additive identity is the zero matrix, $0 \in M_n(\mathbb{R})$, and the additive inverse of an element $M = [a_{ij}] \in M_n(\mathbb{R})$ is $-M = [-a_{ij}] \in M_n(\mathbb{R})$.
\end{eg}

\newthought{Consider} the set $M_n(\mathbb{R})$ under the matrix mutiplication operation that we have introduced in \nameref{chapter:lecture_1_may_02nd_2018}. We found that the identity matrix is
\begin{equation*}
  I = \begin{bmatrix}
    1 & 0 & \hdots & 0 \\
    0 & 1 & \hdots & 0 \\
    \vdots & \vdots & & \vdots \\
    0 & 0 & \hdots & 1
  \end{bmatrix} \in M_n(\mathbb{R}).
\end{equation*}
But since not all elements of $M_n(\mathbb{R})$ have a multiplicative inverse\sidenote{The multiplicative inverse of a matrix does not exist if its determinant is $0$.}, $(M_n(\mathbb{R}), \cdot)$ is not a group.

But we can try to do something similar as to what we did before: by excluding the elements that do not have an inverse. In this case, we exclude elements whose determinant is $0$. Define the set
\begin{equation*}
  GL_n(\mathbb{R}) := \{ M \in M_n(\mathbb{R}) \, : \, \det M \neq 0 \}
\end{equation*}
Note that $\because \det I = 1 \neq 0$, we have that $I \in GL_n(\mathbb{R})$. \\
Also, $\forall A, B \in GL_n(\mathbb{R} )$, we have that $\because \det A \neq 0 \, \land \, \det B \neq 0$,
\begin{equation*}
  \det AB = \det A \det B \neq 0,
\end{equation*}
and therefore $AB \in GL_n(\mathbb{R} )$. Finally, $\forall M \in GL_n(\mathbb{R})$, $\exists M^{-1} \in GL_n(\mathbb{R})$ such that
\begin{equation*}
  MM^{-1} = I = M^{-1} M
\end{equation*}
since $\det M \neq 0$. $\therefore (GL_n(\mathbb{R}), \cdot)$ is a group, and is in fact called the \hldefn{general linear group} \hlnoteb{of degree $n$ over $\mathbb{R}$}.

\newthought{Since} we have introduced permutations in \nameref{chapter:lecture_2_may_04th_2018}, we shall formalize the purpose of its introduction below.

\begin{eg}
  Consider $S_n$, the set of all permutations on $\{1, 2, ..., n\}$. By \cref{propo:properties_of_Sn}, we know that $S_n$ is a group. We call $S_n$ the \hldefn{symmetry group} \hlnoteb{of degree $n$}. For $n \geq 3$, the group $S_n$ is not abelian\sidenote{Let us make this an exercise.
  \begin{ex}
    For $n \geq 3$, prove that the group $S_n$ is not abelian.
  \end{ex}}.
\end{eg}

\newthought{Now that} we have a fairly good idea of the basic concept of a group, we will now proceed to look into handling multiple groups. One such operation is known as the \hldefn{direct product}.

\begin{eg}
  \label{eg:direct_product}
  Let $G$ and $H$ be groups. Their direct product is the set $G \times H$ with the component-wise operation defined by
  \begin{equation*}
    (g_1, h_1) * (g_2, h_2) = (g_1 *_G g_2, h_1 *_H h_2)
  \end{equation*}
  where $g_1, g_2 \in G$, $h_1, h_2 \in H$, $*_G$ is the operation on $G$, and $*_H$ is the operation on $H$.

  The \hlnoteb{closure} and \hlnoteb{associativity} property follow immediately from the definition of the operation. The identity is $(1_G, \, 1_H)$ where $1_G$ is the identity of $G$ and $1_H$ is the identity of $H$. The inverse of an element $(g_1, \, h_1) \in G \times H$ is $(g_1^{-1}, \, h_1^{-1})$.
\end{eg}

By induction, we can show that if $G_1, G_2, ..., G_n$ are groups, then so is $G_1 \times G_2 \times \hdots \times G_n$.

To facilitate our writing, use shall use the following notations:

\begin{notation}
  Given a group $G$ and $g_1, g_2 \in G$, we often denote its identity by $1$, and write $g_1 * g_2 = g_1 g_2$. Also, we denote the unique inverse of an element $g \in G$ as $g^{-1}$.

  We will write $g^0 = 1$. Also, for $n \in \mathbb{N}$, we define
  \begin{equation*}
     g^n = \underbrace{g * g * \hdots * g}_{n \text{ times}}
  \end{equation*}
  and
  \begin{equation*}
    g^{-n} = (g^{-1})^n
  \end{equation*}
\end{notation}

With the above notations,

\begin{propo}\label{propo:group_notations}
  Let $G$ be a group and $g, h \in G$. We have \marginnote{
    \begin{ex}
      Prove \cref{propo:group_notations} as an exercise.
    \end{ex}
  }
  \begin{enumerate}
    \item $(g^{-1})^{-1} = g$
    \item $(gh)^{-1} = h^{-1} g^{-1}$
    \item $g^n g^m = g^{n + m}$ for all $n, m \in \mathbb{Z}$
    \item $(g^n)^m = g^{nm}$ for all $n, m \in \mathbb{Z}$
  \end{enumerate}
\end{propo}

\begin{warning}
  In general, it is not true that if $g, h \in G$, then $(gh)^n = g^n h^n$. For example,
  \begin{equation*}
    (gh)^2 = ghgh \quad \text{but} \quad g^2 h^2 = gghh.
  \end{equation*}
  The two are only equal if and only if $G$ is abelian.
\end{warning}

% subsection groups (end)

% section groups (end)

% chapter lecture_3_may_07th_2018 (end)

\chapter{Lecture 4 May 09 2018}
  \label{chapter:lecture_4_may_09_2018}

\section{Groups (Continued)} % (fold)
\label{sec:groups_continued}

\subsection{Groups (Continued)} % (fold)
\label{sub:groups_continued}

\begin{propo}[Cancellation Laws]\label{propo:cancellation_laws}
  Let $G$ be a group and $g, h, f \in G$. Then
  \begin{enumerate}
    \item \begin{enumerate}
        \item (\hlnoteb{Right Cancellation}) $gh = gf \implies h = f$
        \item (\hlnoteb{Left Cancellation}) $hg = fg \implies h = f$
      \end{enumerate}
    \item The equation $ax = b$ and $ya = b$ have unique solution for $x, y \in G$.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item \begin{enumerate}
      \item By left multiplication and associativity,
        \begin{equation*}
          gh = gf \iff g^{-1} gh = g^{-1} gf \iff h = f
        \end{equation*}
      \item By right multiplication and associativity,
        \begin{equation*}
          hg = fg \iff hgg^{-1} = fgg^{-1} \iff h = f
        \end{equation*}
    \end{enumerate}

    \item Let $x = a^{-1} b$. Then
      \begin{equation*}
        a x = a (a^{-1} b) = (aa^{-1}) b = b.
      \end{equation*}
      If $\exists u \in G$ that is another solution, then
      \begin{equation*}
        au = b = ax \implies u = x
      \end{equation*}
      by Left Cancellation. The proof for $ya = b$ is similar by letting $y = ba^{-1}$.
  \end{enumerate}\qed
\end{proof}

% subsection groups_continued (end)

\subsection{Cayley Tables} % (fold)
\label{sub:cayley_tables}

For a finite group, defining its operation by means of a table is sometimes convenient.

\begin{defn}[Cayley Table]\label{defn:cayley_table}
\index{Cayley Table}
  Let $G$ be a group. Given $x, y \in G$, let the product $xy$ be an entry of a table in the row corresponding to $x$ and column corresponding to $y$. Such a table is called a \hlnoteb{Cayley Table}.
\end{defn}

\begin{note}
  By \autoref{propo:cancellation_laws}, the entries in each row (and respectively, column) of a Cayley Table are all distinct.
\end{note}

\begin{eg}
  Consider the group $(\mathbb{Z}_2, +)$. Its Cayley Table is
  \begin{center}
    \begin{tabular}{c|c|c}
      $\mathbb{Z}_2$ & $[0]$ & $[1]$ \\
      \hline
      $[0]$     & $[0]$ & $[1]$ \\
      $[1]$     & $[1]$ & $[0]$ 
    \end{tabular}
  \end{center}
  where note that we must have $[1] + [1] = [0]$; otherwise if $[1] + [1] = [1]$ then $[1]$ does not have its additive inverse, which contradicts the fact that it is in the group.
\end{eg}

\marginnote {
  If we replace $1$ by $[0]$ and $-1$ by $[1]$, the Cayley Tables of $\mathbb{Z}_2$ and $\mathbb{Z}^*$ are the same. In thie case, we say that $\mathbb{Z}_2$ and $\mathbb{Z}^*$ are \hlnotea{isomorphic}, which we denote by $\mathbb{Z}_2 \cong \mathbb{Z}^*$.
}

\begin{eg}
  Consider the group $\mathbb{Z}^* = \{1. -1\}$. Its Cayley Table (under multiplication) is
  \begin{center}
    \begin{tabular}{c|c|c}
      $\mathbb{Z}^*$ & $1$    & $-1$ \\
      \hline
      $1$              & $1$  & $-1$ \\
      $-1$             & $-1$ & $1$
    \end{tabular}
  \end{center}
\end{eg}

\begin{eg}\label{eg:cyclic_group_cayley_table}
  Given $n \in \mathbb{N}$, the \hldefn{cyclic group} of order $n$ is defined by
  \begin{equation*}
    C_n = \{1, a, a^2, ..., a^{n - 1}\} \quad \text{with } a^n = 1.
  \end{equation*}
  We write $C_n = \langle a : a^n = 1 \rangle$ and $a$ is called a generator of $C_n$. The Cayley Table of $C_n$ is
  \begin{center}
    \begin{tabular}{c | c c c c c c}
      $C_n$     & $1$       & $a$       & $a^2$  & \hdots & $a^{n - 2}$ & $a^{n - 1}$ \\
      \hline
      $1$       & $1$       & $a$       & $a^2$  & \hdots & $a^{n - 2}$ & $a^{n - 1}$ \\
      $a$       & $a$       & $a^2$     & $a^3$  & \hdots & $a^{n - 1}$ & $1$ \\
      $a^2$     & $a^2$     & $a^3$     & $a^4$  & \hdots & $1$         & $a$ \\
      \vdots    & \vdots    & \vdots    & \vdots &        & \vdots      & \vdots \\
      $a^{n-2}$ & $a^{n-2}$ & $a^{n-1}$ & $1$    & \hdots & $a^{n-4}$   & $a^{n-3}$ \\
      $a^{n-1}$ & $a^{n-1}$ & $1$       & $a$    & \hdots & $a^{n-3}$   & $a^{n-2}$
    \end{tabular}
  \end{center}
\end{eg}

\begin{propo}\label{propo:small_groups}
  Let $G$ be a group. Up to isomorphism, we have
  \begin{enumerate}
    \item if $\abs{G} = 1$, then $G \cong \{1\}$.
    \item if $\abs{G} = 2$, then $G \cong C_2$.
    \item if $\abs{G} = 3$, then $G \cong C_3$.
    \item if $\abs{G} = 4$, then either $G \cong C_4$ or $G \cong K_4 \cong C_2 \times C_2$ \marginnote{$K_n$ is known as the \hldefn{Klein n-group}}.
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item If $\abs{G} = 1$, then it can only be $G = \{1\}$ where $1$ is the identity element.
    \item $\abs{G} = 2 \implies G = \{1, g\}$ with $g \neq 1$. The Cayley Table of $G$ is thus
      \begin{center}
        \begin{tabular}{c | c c}
        $G$ & $1$ & $g$ \\
        \hline
        $1$ & $1$ & $g$ \\
        $g$ & $g$ & $1$
        \end{tabular}
      \end{center}
      where we note that $g^2 = 1$; otherwise if $g^2 = g$, then we would have $g = 1$ by \autoref{propo:cancellation_laws}, which contradicts the fact that $g \neq 1$. Comparing the above Cayley Table with that of $C_2$, we see that $G = \langle g : g^2 = 1 \rangle \cong C_2$.
    \item $\abs{G} = 3 \implies G = \{1, g, h\}$ with $g \neq 1 \neq h$ and $g \neq h$. We can then start with the following Cayley Table:
      \begin{center}
        \begin{tabular}{c | c c c}
        $G$ & $1$ & $g$ & $h$ \\
        \hline
        $1$ & $1$ & $g$ & $h$ \\
        $g$ & $g$ &     &     \\
        $h$ & $h$ &     &     
        \end{tabular}
      \end{center}
      We know that by \autoref{propo:cancellation_laws}, $gh \neq g$ and $gh \neq h$. Thus $gh = 1$. Similarly, we get that $hg = 1$.

      \underline{Claim:} Entries in a row (or column) must be distinct. Suppose not. Then say $g^2 = 1$. But since $gh = 1$, by \autoref{propo:cancellation_laws}, we have that $h = g$, which is a contradiction.

      With that, we can proceed to fill in the rest of the entries: with $g^2 = h$ and $h^2 = g$. Therefore,
      \begin{center}
        \begin{tabular}{c | c c c}
        $G$ & $1$ & $g$ & $h$ \\
        \hline
        $1$ & $1$ & $g$ & $h$ \\
        $g$ & $g$ & $h$ & $1$ \\
        $h$ & $h$ & $1$ & $g$
        \end{tabular}
      \end{center}

      Recall that the Cayley Table for $C_3$ is:
      \begin{center}
        \begin{tabular}{c | c c c}
        $C_3$ & $1$   & $a$   & $a^2$ \\
        \hline
        $1$   & $1$   & $a$   & $a^2$ \\
        $a$   & $a$   & $a^2$ & $1$ \\
        $a^2$ & $a^2$ & $1$   & $a$
        \end{tabular}
      \end{center}
      $\therefore G \cong C_3$ (by identifying $g = a$ and $h = a^2$).

    \item \hlwarn{Proof will be added once assignment 1 is over}
  \end{enumerate}
\end{proof}

% subsection cayley_tables (end)

% section groups_continued (end)

\section{Subgroups}
\label{sec:subgroups}

\subsection{Subgroups}
\label{sub:subgroups}

\begin{defn}[Subgroup]\label{defn:subgroup}
\index{Subgroup}
  Let $G$ be a group and $H \subseteq G$. If $H$ itself is a group, then we say that $H$ is a subgroup of $G$
\end{defn}

% subsection subgroups (end)

% section subgroups (end)

% chapter lecture_4_may_09_2018 (end)

\chapter{Lecture 5 May 11th 2018}
\label{chp:lecture_5_may_11th_2018}

\section{Subgroups (Continued)}
\label{sec:subgroups_continued}
% section Subgroups (Continued)

\subsection{Subgroups (Continued)}
\label{sub:subgroups_continued}
% subsection Subgroups (Continued)

\begin{note}[Recall: definition of a subgroup]
  Let $G$ be a group and $H \subseteq G$. If $H$ itself is a group, then we say that $H$ is a subgroup of $G$.
\end{note}

\begin{note}
  Since $G$ is a group, $\forall h_1, h_2, h_3 \in H \subseteq G$, we have $h_1 (h_2 h_3) = (h_1 h_2) h_3$. So $H$ is a subgroup of $G$ if it satisfies the following conditions, which we shall hereafter refer to as the Subgroup Test.

\noindent\hldefn{Subgroup Test} \\
  \marginnote{Note that the identity in $H$ must also be the identity in $G$. This is because if $h_1, h_1^{-1} \in H$, then $h_1 h_1^{-1} = 1_H$, but $h_1, h_1^{-1} \in G$ as well, and so $h_1 h_1^{-1} = 1_G$. Thus $1_H = 1_G$.}
  \begin{enumerate}
    \item $h_1 h_2 \in H$
    \item $1_G \in H$
    \item $\exists h_1^{-1} \in H$ such that $h_1 h_1^{-1} = 1_G$
  \end{enumerate}
\end{note}

\begin{eg}
  Given a group $G$, it is clear that $\{1\}$ and $G$ are both subgroups of $G$.
\end{eg}

\begin{eg}
  We have the following chain of groups:
  \begin{equation*}
    (\mathbb{Z}, +) \subseteq (\mathbb{Q}, +) \subseteq (\mathbb{R}, +) \subseteq (\mathbb{C}, +)
  \end{equation*}
\end{eg}

Recall that the general linear group is defined as:
\begin{equation*}
  GL_n(\mathbb{R}) = (GL_n(\mathbb{R}), \cdot) = \{A \in M_n(\mathbb{R}) : \det A \neq 0 \}
\end{equation*}

\begin{defn}[Special Linear Group]\label{defn:special_linear_group}
\index{Special Linear Group}
  The \hlnoteb{special linear group} of order $n$ of $\mathbb{R}$ is defined as
  \begin{equation*}
    SL_n(\mathbb{R}) = (SL_n(\mathbb{R}), \cdot) = \{A \in M_n(\mathbb{R}) : \det A = 1 \}
  \end{equation*}
\end{defn}

\begin{eg}
  Clearly, $SL_n(\mathbb{R}) \subseteq GL_n(\mathbb{R})$. Note that the identity matrix $I$ must be in $SL_n(\mathbb{R})$ since $\det I = 1$. Also, $\forall A, B \in SL_n(\mathbb{R})$, we have that
  \begin{equation*}
    \det AB = \det A \det B = 1
  \end{equation*}
  $\therefore AB \in SL_n(\mathbb{R})$. Also, since $\det A^{-1} = \frac{1}{\det A} = 1$, we also have that $\A^{-1} \in SL_n(\mathbb{R})$. We see that $SL_n(\mathbb{R})$ satisfies the \hlnoteb{Subgroup Test}, and hence it is a subgroup of $GL_n(\mathbb{R})$.
\end{eg}

\begin{defn}[Center of a Group]\label{defn:center_of_a_group}
\index{Center of a Group}
  Given a group $G$, the \hlnoteb{the center of a group $G$} is defined as
  \begin{equation*}
    Z(G) = \{z \in G \, : \, \forall g \in G \enspace zg = gz \}
  \end{equation*}
\end{defn}

\begin{eg}
  For a group $G$, $Z(G)$ is an abelian subgroup of $G$.

  \begin{proof}
    Clearly, $1_G \in Z(G)$. Let $y, z \in G$. $\forall g \in G$, we have that
    \begin{equation*}
      (yz)g = y(zg) = y(gz) = (yg)z = (gy)z = g(yz)
    \end{equation*}
    Therefore $yz \in Z(G)$ and so $Z(G)$ is closed under its operation. Also, $\forall h 
    in G$, we can write $h = (h^{-1})^{-1} = g^{-1}$. Since $z \in Z(G)$, we have that $\forall g \in G$,
    \begin{align*}
      zg = gz \iff (zg)^{-1} = (gz)^{-1} &\iff g^{-1} z^{-1} = z^{-1} g^{-1} \\
          &\iff hz^{-1} = z^{-1} h
    \end{align*}
    Therefore $z^{-1} \in Z(G)$. By the \hlnoteb{Subgroup Test}, it follows that $Z(G)$ is a subgroup of $G$.

    Finally, since $Z(G) \subseteq G$, by its definition, we have that $\forall x, y \in Z(G)$, $x, y \in G$ as well, and we have that $xy = yx$. Therefore, $Z(G)$ is abelian. \qed
  \end{proof}
\end{eg}

\begin{propo}[Intersection of Subgroups is a Subgroup]\label{propo:intersection_of_subgroups_is_a_subgroup}
  Let $H$ and $K$ be subgroups of a group $G$. Then their intersection
  \begin{equation*}
    H \cap K = \{g \in G : g \in H \, \land \, g \in K\}
  \end{equation*}
  is also a subgroup of $G$.
\end{propo}

\begin{proof}
  Since $H$ and $K$ are subgroups, we have that $1 \in H$ and $1 \in K$ and hence $1 \in H \cap K$. Let $a, b \in H \cap K$. Since $H$ and $K$ are subgroups, we have that $ab \in H$ and $ab \in K$. Therefore, $ab \in H \cap K$. Similarly, since $a^{-1} \in H$ and $a^{-1} \in K$, $a^{-1} \in H \cap K$. By the \hlnoteb{Subgroup Test}, $H \cap K$ is a subgroup of $G$. \qed
\end{proof}

\begin{propo}[Finite Subgroup Test]\label{propo:finite_subgroup_test}
\index{Finite Subgroup Test}
\marginnote{This result says that if $H$ is a finite nonempty subset, then we only need to prove that it is closed under its operation to prove that it is a subgroup. The other two conditions in the \hlnoteb{Subgroup Test} are automatically implied.}
  If $H$ is a finite nonempty subset of a group $G$, then $H$ is a subgroup if and only if $H$ is closed under its operation.
\end{propo}

\begin{proof}
  The forward direction of the proof is trivially true, since $H$ must satisfy the closure property for it to be a subgroup.

  For the converse, since $H \neq \emptyset$, let $h \in H$. Since $H$ is closed under its operation, we have that
  \begin{equation*}
    h, h^2, h^3, ...
  \end{equation*}
  are all in $H$. Since $H$ is finite, not all of the $h^n$'s are distinct. Then, $\forall n \in \mathbb{N}$, there must $\exists m \in \mathbb{N}$ such that $h^n = h^{n + m}$. Then by \autoref{propo:cancellation_laws}, $h^m = 1$ and so $1 \in H$. Also, because $1 = h^{m - 1} h$, we have that $h^{-1} = h^{m - 1}$, and thus the inverse of $h$ is also in $H$. Therefore, $H$ is a subgroup of $G$ as requried. \qed
\end{proof}

% subsection Subgroups (Continued) (end)

% section Subgroups (Continued) (end)

% chapter lecture_5_may_11th_2018 (end)

\nobibliography*
\bibliography{bibliography}

\printindex
\end{document}
