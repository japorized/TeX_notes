\documentclass[notoc,notitlepage]{tufte-book}
% \nonstopmode % uncomment to enable nonstopmode

\usepackage{classnotetitle}

\title{ACTSC 431 --- Loss Model I}
\author{Johnson Ng}
\subtitle{Classnotes for Fall 2018}
\credentials{BMath (Hons), Pure Mathematics major, Actuarial Science Minor}
\institution{University of Waterloo}

\input{latex-classnotes-preamble.tex}
\input{probnotation.tex}

\setsidenotefont{\color{light}\footnotesize}
\setmarginnotefont{\color{light}\footnotesize}

\DeclareMathOperator{\VaR}{VaR}
\DeclareMathOperator{\TVaR}{TVaR}
\DeclareMathOperator{\LER}{LER }

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\tableofcontents

\chapter*{\faBook\ \enspace\ List of Definitions}
\addcontentsline{toc}{chapter}{List of Definitions}
\theoremlisttype{all}
\begin{fullwidth}
\listtheorems{defn}
\end{fullwidth}

\chapter*{\faCoffee\ \enspace\ List of Theorems}
\addcontentsline{toc}{chapter}{List of Theorems}
\theoremlisttype{allname}
\begin{fullwidth}
\listtheorems{axiom,lemma,thm,crly,propo}
\end{fullwidth}

\chapter{Lecture 1 Sep 06}%
\label{chp:lecture_1_sep_06}
% chapter lecture_1_sep_06

\section{Introduction and Overview}%
\label{sec:introduction_and_overview}
% section introduction_and_overview

\paragraph{Course Objective} In Loss Model I, the focus of our study is to learn the basic methods which are used by insurers to quantify risk from mathematical/statistical models, in order for insurers to make various decisions\sidenote{e.g.\ setting premiums, control expenses, deciding for reinsurance, etc.}. By quantifying risk, it helps us monitor underlying risks so that not only are we aware of them, but also so that we can take actions or preventive measures against them.

Our main interest of this course is:
\begin{itemize}
  \item to quantify and seek protection against the loss of funds due either to \hlnoteb{too many claims} or \hlnoteb{a few large claims};
  \item to reduce adverse financial impact of random events that prevent the realization of reasonable expectations.
\end{itemize}

\newthought{The main model that shall be the focus} of this course is \hlnoteb{models for liability risk}.

\begin{defn}[Liability Risk]\index{Liability Risk}\label{defn:liability_risk}
  A \hlnoteb{liability risk} is a risk that insurance companies assume by selling insurance contracts.
\end{defn}

In particular, the liability that we shall focus on is \hlnotea{insurance claims}.\marginnote{Many of the models that we shall see later in the course are also applied for other types of risks, e.g.\ investment risk, credit risk, liquidity risk, and operational risk.}

\newthought{We are interested} in modelling the total amount of claims, i.e.\ the \hldefn{aggregate claim amount}, of a group fo insurance policies over a given period of time. In the actuarial literature, there are two main approaches that have been proposed to model the aggrement claim amount of an insurance portfolio, namely:
\begin{itemize}
  \item individual risk model;
  \item collective risk model.
\end{itemize}

\subsection{Individual Risk Model}%
\label{sub:individual_risk_model}
% subsection individual_risk_model

\begin{defn}[Individual Risk Model]\index{Individual Risk Model}\label{defn:individual_risk_model}
  In an \hlnoteb{individual risk model}, the aggregate claim is modeled by
  \begin{equation*}
    S = \sum_{i=1}^{n} Z_i
  \end{equation*}
  where $n$ is a \hlnotea{deterministic}\sidenote{i.e.\ fixed} integer that represents the \hlnotec{total number of insurance policies}, and $Z_i$ is a random variable for the \hlnotec{potential loss of the $i$\textsuperscript{th} insurance policy.}
\end{defn}

\begin{note}
  Since a policy may or may not incur a loss\sidenote{Since a claim may or may not be made!}, we have that
  \begin{equation*}
    P(Z_i = 0) > 0.
  \end{equation*}
  Thus, in an individual risk model, we may also express the aggregate claim amount as
  \begin{equation*}
    S = \sum_{i=1}^{n} X_i I_i
  \end{equation*}
  where $I_i$ is the indicator function about the claimant of policy $i$, while $X_i$ represents the size of the claim(s) for the $i$\textsuperscript{th} policy provided that there is a claim.\sidenote{\hlimpo{This is actually incorrect, despite being in the recommended textbook. See \cref{sec:individual_risk_model_an_alternate_view}.}}
\end{note}

However, in an individual risk model, according to Dhaene and Vyncke (2010)\cite{DhaeneVyncke2010},

\begin{quotebox}{be-red}{light}
  A third type of error that may arise when computing aggregate claims follows from the fact that the assumption of mutual independency of the individual claim amounts may be violated in practice.
\end{quotebox}

Due to complications such as this, the individual risk model will not be the focus of our studies.

% subsection individual_risk_model (end)

\subsection{Collective Risk Model}%
\label{sub:collective_risk_model}
% subsection collective_risk_model

\begin{defn}[Collective Risk Model]\index{Collective Risk Model}\label{defn:collective_risk_model}
  In a \hlnoteb{collective risk model}, the aggregate claim is modeled by
  \begin{equation*}
    S = \sum_{i=1}^{N} X_i, \end{equation*}
  where $N$ is a non-negative integer-valued random variable that denotes \hlnotec{the number of claims among a given set of policies}, while $X_i$ denotes the \hlnotec{size of the $i$\textsuperscript{th} policy.}
\end{defn}

\begin{note}
  In a collective risk model, we need to determine:
  \begin{itemize}
    \item the distribution of the total number of claims for the entire portfolio, i.e.\ the distribution of $N$; and
    \item the distribution of the loss amount per claim, i.e.\ the distribution of $X_i$.
  \end{itemize}
\end{note}

% subsection collective_risk_model (end)

In this course, the primary focus of our studies will be on \hlnotea{collective risk models}.

\paragraph{Terminologies} To end today's lecture, the following terminologies are introduced:

\begin{defn}[Severity Distribution]\index{Severity Distribution}\label{defn:severity_distribution}
  The \hlnoteb{severity distribution} is the distribution of the loss amount of the amount paid by the insurer on a given loss/claim.
\end{defn}

\begin{defn}[Frequency Distribution]\index{Frequency Distribution}\label{defn:frequency_distribution}
  The \hlnoteb{frequency distribution} is the distributino fo the number of losses/claims paid by the insurer over a given period of time.
\end{defn}

\begin{note}
  The frequency distribution is typically a discrete distribution.
\end{note}

\begin{defn}[Aggrement Payment / Loss]\index{Aggrement Payment}\index{Aggregate Loss}\label{defn:aggrement_payment_loss}
  The \hlnoteb{aggregate payment (loss)} is the total amout of all claim payments (losses) over a given period of time.
\end{defn}

\begin{note}
  There is a distinction between an aggregate payment and an aggregate loss, since an aggregate payment is ``essentially'' an aggregate loss after certain claim adjustments, such as deductibles, limits, and coinsurance.
\end{note}

% section introduction_and_overview (end)

% chapter lecture_1_sep_06 (end)

\chapter{Lecture 2 Sep 11th}%
\label{chp:lecture_2_sep_11th}
% chapter lecture_2_sep_11th

\section{Review of Probability Theory}%
\label{sec:review_of_probability_theory}
% section review_of_probability_theory

Firstly, we shall review the definition of a random variable.

\begin{defn}[Random Variable]\index{Random Variable}\label{defn:random_variable}
  Let $\Omega$ be a sample space and $\mathcal{F}$ its $\sigma$-algebra\sidenote{For definitions of $\Omega$ and $\mathcal{F}$, see notes on STAT330.}. A \hlnoteb{random variable} (rv) $X : \Omega \to (\Omega, \mathcal{F})$ is a function from a possible set of outcomes to a measurable space $(\Omega, \mathcal{F})$. Within the context of our interest, $X$ is real-valued, i.e. $(\Omega, \mathcal{F}) = \mathbb{R}$.
\end{defn}

\subsection{Discrete Random Variables}%
\label{sub:discrete_random_variables}
% subsection discrete_random_variables

\begin{defn}[Discrete Random Variable]\index{Discrete Random Variable}\label{defn:discrete_random_variable}
  A \hlnoteb{discrete random variable} (drv) is an rv $X$ that takes only countable (finite) real values.
\end{defn}

\begin{note}
  Let $X$ be a drv.
  \begin{itemize}
    \item The \hlnotea{probability mass function} (pmf) of $X$ is: for $i \in \mathbb{N}$,
      \begin{equation*}
        p(x_i) = P(X = x_i)
      \end{equation*}

    \item The \hlnotea{cumulative distribution function} (cdf) of $X$ is
      \begin{equation*}
        F(x) = P(X \leq x) = \sum_{x_i \leq x} p(x_i).
      \end{equation*}

    \item The $k$th \hldefn{moment} of $X$ is\sidenote{This implicitly uses the \href{https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician}{Law of the Unconcious Statistician}.}
      \begin{equation*}
        E[X^k] = \sum_{i \in \mathbb{N}} x_i^k p(x_i)
      \end{equation*}
      if $E[X^k]$ is finite.

    \item Some commonly seen/introduced discrete distributions are: Poisson, Binomial, Negative Binomial
  \end{itemize}
\end{note}

\begin{eg}
  Let $X$ take values from $\{x_1, x_2, x_3, x_4\}$, and
  \begin{equation*}
    p(x_i) = P(X = x_i) \text{ for } i = 1, 2, 3, 4.
  \end{equation*}
  The cdf of $X$ is\marginnote{
    It is recommended to visualize the cdf first before putting it down in pencil.
    \resizebox{4.5cm}{!}{
    \begin{tikzpicture}
      % axes
      \draw[->] (0, 0) -- (0, 5) node[above] {$F(x)$};
      \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
      \node[below=1.5mm] at (1, 0) {$x_1$};
      \node[below=1.5mm] at (2, 0) {$x_2$};
      \node[below=1.5mm] at (3, 0) {$x_3$};
      \node[below=1.5mm] at (4, 0) {$x_4$};

      % cdf
      \draw[-,line width=0.5mm] (0, 0) -- (1, 0);
      \draw[-,line width=0.5mm] (1, 1) -- (2, 1);
      \draw[-,line width=0.5mm] (2, 2) -- (3, 2);
      \draw[-,line width=0.5mm] (3, 3) -- (4, 3);
      \draw[->,line width=0.5mm] (4, 4) -- (5, 4);
      \node[circle,inner sep=2pt,draw] at (1, 0) {};
      \node[circle,inner sep=2pt,draw] at (2, 1) {};
      \node[circle,inner sep=2pt,draw] at (3, 2) {};
      \node[circle,inner sep=2pt,draw] at (4, 3) {};
      \node[circle,inner sep=2pt,fill] at (1, 1) {};
      \node[circle,inner sep=2pt,fill] at (2, 2) {};
      \node[circle,inner sep=2pt,fill] at (3, 3) {};
      \node[circle,inner sep=2pt,fill] at (4, 4) {};
      \draw[dotted] (4, 4) -- (0, 4) node[left] {$1$};

      % jumps
      \draw[dotted] (1, 1) -- (1, 0) node[midway,left] {$p(x_1)$};
      \draw[dotted] (2, 2) -- (2, 1) node[midway,left] {$p(x_2)$};
      \draw[dotted] (3, 3) -- (3, 2) node[midway,left] {$p(x_3)$};
      \draw[dotted] (4, 4) -- (4, 3) node[midway,left] {$p(x_4)$};
    \end{tikzpicture}
    }
  }
  \begin{equation*}
    F(x) = \begin{cases}
      0               & x < x_1 \\
      p(x_1)          & x_1 \leq x < x_2 \\
      p(x_1) + p(x_2) & x_2 \leq x < x_3 \\
      1 - p(x_4)      & x_3 \leq x < x_4 \\
      1               & x \geq x_4
    \end{cases}
  \end{equation*}
\end{eg}

\begin{note}
  \begin{itemize}
    \item It is important that we stress the need for showing \hlnotea{right continuity} in the graph.
    \item Note that the cdf always sums to $1$.
    \item The ``\hlnotea{jumps}'' at $x_i$ correspond to $p(x_i)$, for $i = 1, 2 ,3, 4$.
  \end{itemize}
\end{note}

\begin{defn}[Probability Generating Function]\index{Probability Generating Function}\label{defn:probability_generating_function}
  Suppose a drv $X$ only takes \hlimpo{non-negative integer values}. The \hlnoteb{probability generating function} (pgf) of $X$ is defined as
  \begin{equation*}
    G(z) = E\left[ z^X \right] = \sum_{k=1}^{\infty} z^k p(k)
  \end{equation*}
  where we note that if $\max X = n$, then $p(m) = 0$ for all $m > n$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item The pgf uniquely identifies the distribution of the drv\sidenote{\faHandPaperO\ This was given as is without proof, and I cannot find any resources that proves this.}.
    \item To get the probability for $k \in \{0, 1, 2, \ldots\}$, we simply need to do
      \begin{equation*}
        p(k) = \frac{1}{k!} G^{(k)}(x) \at{x = 0}{}.
      \end{equation*}
  \end{itemize}
\end{note}

\begin{eg}[Lecture Slides: Example 1]
  Consider a drv $X$ with pmf
  \begin{equation*}
    p(x) = P(X = x) = \begin{cases}
      0.5 & x = 0 \\
      0.4 & x = 1 \\
      0.1 & x = 2
    \end{cases}
  \end{equation*}
  Its cdf is\marginnote{
    \resizebox{4.5cm}{!}{
    \begin{tikzpicture}
      % axes
      \draw[->] (0, 0) -- (0, 5) node[above] {$F(x)$};
      \draw[->] (0, 0) -- (3.5, 0) node[right] {$x$};
      \node[below=1.5mm] at (1, 0) {$0$};
      \node[below=1.5mm] at (2, 0) {$1$};
      \node[below=1.5mm] at (3, 0) {$2$};

      % cdf
      \draw[-,line width=0.5mm] (0, 2) -- (1, 2);
      \draw[-,line width=0.5mm] (1, 3.6) -- (2, 3.6);
      \draw[->,line width=0.5mm] (2, 4) -- (3.5, 4);
      \node[circle,inner sep=2pt,draw] at (1, 2) {};
      \node[circle,inner sep=2pt,draw] at (2, 3.6) {};
      \node[circle,inner sep=2pt,fill] at (0, 2) {};
      \node[circle,inner sep=2pt,fill] at (1, 3.6) {};
      \node[circle,inner sep=2pt,fill] at (2, 4) {};
      \draw[dotted] (3, 4) -- (0, 4) node[left] {$1$};

      % jumps
      \draw[dotted] (0, 0) -- (0, 2) node[midway,left] {$0.5$};
      \draw[dotted] (1, 2) -- (1, 3.6) node[midway,left] {$0.4$};
      \draw[dotted] (2, 3.6) -- (2, 4) node[midway,left] {$0.1$};
    \end{tikzpicture}
    }
  }
  \begin{equation*}
    F(x) = P(X \leq x) \begin{cases}
      0   & x < 0 \\
      0.5 & 0 \leq x < 1 \\
      0.9 & 1 \leq x < 2 \\
      1   & x \geq 2
    \end{cases}
  \end{equation*}
  and its pgf is
  \begin{equation*}
    G(z) = E\left[ z^X \right] = 0.5 + 0.4z + 0.1z^2.
  \end{equation*}
\end{eg}

% subsection discrete_random_variables (end)

\subsection{Continuous Random Variables}%
\label{sub:continuous_random_variables}
% subsection continuous_random_variables

\begin{defn}[Continuous Random Variable]\index{Continuous Random Variable}\label{defn:continuous_random_variable}
  A \hlnoteb{continuous random variable} (crv) takes on a continuum of values.
\end{defn}

\begin{note}
  Let $X$ be a crv.
  \begin{itemize}
    \item $\exists f : X \to \mathbb{R}$ called a \hlnotea{probability density function} (pdf) such that its cdf is
      \begin{equation*}
        F(x) = \int_{-\infty}^{x} f(y) \dif{y},
      \end{equation*}
      and consequently by the \hlnotea{Fundamental Theorem of Calculus}, we have
      \begin{equation*}
        f(x) = F'(x).
      \end{equation*}

    \item The $k$th moment of $X$ is
      \begin{equation*}
        E[X^k] = \int_{x} x^k f(x) \dif{x} 
      \end{equation*}
      so long that $E[X^k]$ is defined.
      
    \item Some commonly introduced distributions are: Uniform, Exponential, Gamma, Weibull, and Normal.
  \end{itemize}
\end{note}

\begin{defn}[Moment Generating Function]\index{Moment Generating Function}\label{defn:moment_generating_function}
Let $X$ be an rv. The \hlnoteb{moment generating function} (mgf)\marginnote{The mgf is also defined for drvs.} of $X$ is, for $t \in \mathbb{R}$ (appropriately so),
  \begin{equation*}
    M_X(t) = E\left[e^{tX}\right] = \int_{x} e^{tx} f(x) \dif{x}
  \end{equation*}
  provided that the integral is well-defined.
\end{defn}

\begin{note}
  \begin{itemize}
    \item The mgf uniquely determines the distribution of its rv\sidenote{\faHandPaperO\ This shall, also, not be proven in this course.}

    \item With the mgf, we can obtain the $k$th moment of an rv $X$ by
      \begin{equation*}
        E\left[X^k\right] = \frac{d^k}{dt^k} M_X(t) \at{t = 0}{}
      \end{equation*}
  \end{itemize}
\end{note}

\begin{eg}[Lecture Notes: Example 2]
  Consider an exponential rv $X$ with pdf\sidenote{When not explicitly stated, it shall be assumed that domains at which we did not specify $x$ shall have probability $0$.}
  \begin{equation*}
    f(x) = 0.1e^{-0.1x}, \; x > 0.
  \end{equation*}
  Its cdf is
  \begin{equation*}
    F(x) = \int_{-\infty}^{x} f(y) \dif{y} = \begin{cases}
      1 - e^{-0.1 x} & x \geq 0 \\
      0              & \text{otherwise}
    \end{cases}
  \end{equation*}
  and its mgf is
  \begin{align*}
    M_X(t) &= E\left[ e^{tX} \right] = \int_{0}^{\infty} e^{tx} 0.1 e^{-0.1x} \dif{x} \\
           &= 0.1 \int_{0}^{\infty} e^{( t - 0.1 )x} \dif{x} \\
           &= \frac{0.1}{0.1 - t}, \enspace t < 0.1,
  \end{align*}
  where we note that we must have $t < 0.1$, for otherwise the value of the exponent would render the integral undefined.
\end{eg}

\begin{defn}[Hazard Rate Function]\index{Hazard Rate Function}\label{defn:hazard_rate_function}
  For a crv $X$, the \hlnoteb{hazard rate function} (aka \hldefn{failure rate}) of $X$ is defined as
  \begin{equation*}
    h(x) = \frac{f(x)}{\bar{F}(x)} = - \frac{d}{dx} \ln \bar{F}(x),
  \end{equation*}
  where $\bar{F}(x) = 1 - F(x)$ is the \hlnotea{survival function}\sidenote{You should be familiar with this if you have studied for Exam P.}
\end{defn}

\begin{note}
  \begin{itemize}
    \item We may also express the survival function in terms of the hazard rate by
      \begin{equation*}
        \bar{F}(x) = e^{- \int_{-\infty}^{x} h(y) \dif{y}}.
      \end{equation*}

    \item In terms of limits, we can express the hazard rate function, for small enough $\delta > 0$, as
      \begin{align*}
        h(x) &= \frac{f(x)}{\bar{F}(x)} = \frac{F'(x)}{\bar{F}(x)} \\
             &\approx \frac{F(x + \delta) - F(x)}{\delta \bar{F}(x)} \\
             &= \frac{P(x < X \leq x + \delta)}{\delta F(X > x)} \\
             &= \frac{1}{\delta} P(x < X \leq x + \delta \mid X > x).
      \end{align*}
      We can make sense of this expression by recalling the notion of the probability of survival from Exam MLC\sidenote{This also tells us that the hazard rate gets its name from life insurance.}, where if a life has survived over $x$, the hazard rate is the probability that the life does not survive beyond another $\delta$ \sidenote{From the perspective of life insurance, the greater the probability, the more likely the claim is going to happen.}.
  \end{itemize}
\end{note}

% subsection continuous_random_variables (end)

% section review_of_probability_theory (end)

% chapter lecture_2_sep_11th (end)

\chapter{Lecture 3 Sep 13th}%
\label{chp:lecture_3_sep_13th}
% chapter lecture_3_sep_13th

\section{Review of Probability Theory (Continued)}%
\label{sec:review_of_probability_theory_continued}
% section review_of_probability_theory_continued

\subsection{Continuous Random Variables (Continued)}%
\label{sub:continuous_random_variables_continued}
% subsection continuous_random_variables_continued

\begin{eg}[Lecture Notes: Example 3 --- Hazard Rate of Weibull Distribution]\label{eg:weibull_hazard_rate}
  Suppose $X \sim \Wei(\theta, \tau)$ with pdf
  \begin{equation*}
    f(x) = \frac{\tau {\left( \frac{x}{\theta} \right)}^\tau e^{-{\left( \frac{x}{\theta} \right)}^\tau}}{x}, \quad x > 0,
  \end{equation*}
  where $\theta, \tau > 0$. Find its hazard rate function.
\end{eg}

\begin{solution}
  We first require the survival function\sidenote{\hlnotea{Weibull Survival Function}\label{note:weibull_survival_function}}:
  \begin{align*}
    \bar{F}(x) &= \int_{x}^{\infty} \frac{1}{y} \tau {\left( \frac{y}{\theta} \right)}^\tau e^{-{\left( \frac{y}{\theta} \right)}^\tau} \dif{y} \\
               &= \int_{\frac{x}{\theta}}^{\infty} \frac{1}{u} \tau u^\tau e^{-u^\tau} \dif{u} \qquad \text{ where } u = \frac{y}{\theta} \\
               &= \int_{\frac{x}{\theta}}^{\infty} \tau u^{\tau - 1} e^{-u^\tau} \dif{u} \\
               &= -e^{-u^\tau} \at{\frac{x}{\theta}}{\infty} = e^{-{\left( \frac{x}{\theta} \right)}^\tau}
  \end{align*}
  The hazard rate is therefore
  \begin{equation*}
    h(x) = \frac{f(x)}{\bar{F}(x)} = \frac{\tau}{x} {\left( \frac{x}{\theta} \right)}^\tau
  \end{equation*}
\end{solution}

% subsection continuous_random_variables_continued (end)

\subsection{Mixed Random Variable}%
\label{sub:mixed_random_variable}
% subsection mixed_random_variable

\begin{defn}[Mixed Random Variable]\index{Mixed Random Variable}\label{defn:mixed_random_variable}
  We call $X$ a \hlnoteb{mixed random variable} (mixed rv) if it has both discrete and continuous components.
\end{defn}

\begin{note}
  \begin{itemize}
    \item Mixed rvs are important in modeling insurance claims, e.g., the loss amount is usually a continuous random variable with a probability mass at $0$.
  \end{itemize}
\end{note}

The following is a type of mixed random variable:

\begin{defn}[Deductibles]\index{Deductibles}\label{defn:deductibles}
  Let $X$ be an rv and $d$ be a fixed value.
  \begin{equation*}
    {[ X - d ]}_+ = \begin{cases}
      X - d & x \geq d \\
      0     & \text{ otherwise }
    \end{cases}
  \end{equation*}
\end{defn}

\begin{note}
  If $X$ be an rv and $d$ a fixed value, the deductible ${[X - d]}_+$ has a mass point at $0$ since
  \begin{equation*}
    P( {[ X - d ]}_+ = 0 ) = P(X < d) > 0
  \end{equation*}
\end{note}

\begin{note}
  Let $\{ x_1, x_2, \ldots \}$ be a sequence of real numbers in an increasing order. Suppose $X$ is a rv that takes on values on the real, and has a \hlnotea{density function} $f$ on each interval $(x_i, x_{i + 1})$, and has \hlnotea{discrete mass points} at the boundaries of these intervals, i.e.\marginnote{In other words, we treat the discrete and continuous part of a mixed rv separately.}
  \begin{equation*}
    P(X = x_i) = p(x_i) > 0 \quad i \in \mathbb{N}.
  \end{equation*}
  Since $X$ is an rv, it must be the case that
  \begin{equation*}
    \sum_{i \in \mathbb{N}} p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} f(x) \dif{x}  = 1.
  \end{equation*}
  The cdf of a mixed rv $X$ is
  \begin{equation*}
    F(x) = P(X \leq x) = \sum_{i \in \mathbb{N}} p(x_i) \mathbb{1}_{\{x_i \leq x\}} + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} f(y) \mathbb{1}_{\{ y \leq x \} } \dif{y}.
  \end{equation*}
  The $k$th moment of $X$ is
  \begin{equation*}
    E\left[X^k\right] = \sum_{i \in \mathbb{N}} {(x_i)}^k p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} x^k f(x) \dif{x}.
  \end{equation*}
  The mgf of $X$ is
  \begin{equation*}
    M_X(t) = E\left[ e^{tX} \right] = \sum_{i \in \mathbb{N}} e^{tx_i} p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} e^{tx} f(x) \dif{x}.
  \end{equation*}
\end{note}

\begin{eg}[Lecture Notes: Example 4]
  Assume a claim amount of an insurance policy is modeled by a non-negative rv $X$ which has probability mass of $p$ and $0$, and otherwise continuous with a pdf $f$ over $(0, \infty)$. Find its cdf, $k$th moment, and mgf.
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      p + \int_{0}^{x} f(y) \dif{y} & x \geq 0 \\
      0                             & \text{ otherwise }
    \end{cases}
  \end{equation*}
  The $k$th moment of $X$ is
  \begin{equation*}
    E\left[ X^k \right] = \int_{0}^{\infty} x^k f(x) \dif{x}.
  \end{equation*}
  The mgf of $X$ is
  \begin{equation*}
    M_X(t) = p + \int_{0}^{\infty} e^{tx} f(x) \dif{x}.
  \end{equation*}
\end{solution}

% subsection mixed_random_variable (end)

% section review_of_probability_theory_continued (end)

\section{Distributional Quantities and Risk Measures}%
\label{sec:distributional_quantities_and_risk_measures}
% section distributional_quantities_and_risk_measures

\newthought{This chapter} introduces us to some \hlnotea{distributional quantities} for a given rv $X$. These distributional quantities are informative values to describe the characteristics of a risk.

\subsection{Distributional Quantities}%
\label{sub:distributional_quantities}
% subsection distributional_quantities

\begin{defn}[Central Moment]\index{Central Moment}\label{defn:central_moment}
  The \hlnoteb{$k$th central moment} of an rv $X$ is defined as
  \begin{equation*}
    E\left[ {(X - E(X))}^k \right].
  \end{equation*}
\end{defn}

\begin{note}
  The second central moment is the \hldefn{variance}. The square root of the variance is the \hldefn{standard deviation}.
\end{note}

\begin{eg}[Lecture Notes: Example 5]
  Consider an rv $Y = \begin{cases} Y_1 & U = 1 \\ Y_2 & U = 2 \end{cases} \; $\sidenote{This notation is just syntatic sugar for saying $Y_1 = Y \mid ( U = 1 )$ and $Y_2 = Y \mid ( U = 2 )$.}, where $Y_1 = 0$, $Y_2 \sim \Exp(10)$, and $P(U = 1) = P(U = 2) = 0.5$.
  \begin{enumerate}
    \item Find the cdf of $Y$.
    \item Find the mean and variance of $Y$.
    \item Let $Z = \frac{1}{2} Y_1 + \frac{1}{2} Y_2$. Does $Z$ have the same distribution as $Y$? Answer this by solving the mean and variance of $Z$.
  \end{enumerate}
\end{eg}

\begin{solution}
  \begin{enumerate}
    \item Note that
      \begin{equation*}
        F(y) = P( Y_1 \leq y \mid U = 1 ) P(U = 1) + P( Y_2 \leq y \mid U = 2 ) P(U = 2).
      \end{equation*}
      Observe that
      \begin{equation*}
        P(Y_1 \leq y \mid U = 1) = \begin{cases}
          1 & y \geq 0 \\
          0 & y < 0
        \end{cases}
      \end{equation*}
      and
      \begin{equation*}
        P(Y_2 \leq y \mid U = 2) = \begin{cases}
          1 - e^{-10y} & y \geq 0 \\
          0            & y < 0
        \end{cases}
      \end{equation*}
      Therefore
      \begin{equation*}
        F(y) = \begin{cases}
          1 - \frac{1}{2} e^{-10 y} & y \geq 0 \\
          0                         & y < 0
        \end{cases}
      \end{equation*}

    \item The mean of $Y$ is
      \begin{equation*}
        E(Y) = E(Y \mid U = 1) P(U = 1) + E(Y \mid U = 2) P(U = 2) = 10 \cdot \frac{1}{2} = 5.
      \end{equation*}
      To calculate the variance of $Y$, we require
      \begin{align*}
        E\left[Y^2\right] &= E\left[Y^2 \mid U = 1\right] P(U = 1) + E\left[Y^2 \mid U = 2\right] P(U = 2) \\
                          &= ( \Var(Y_2) + E{(Y_2)}^2 ) \cdot \frac{1}{2} = 100.
      \end{align*}
      Therefore
      \begin{equation*}
        \Var(Y) = 100 - 5^2 = 75.
      \end{equation*}

    \item The mean of $Z$ is
      \begin{equation*}
        E[Z] = E[ \frac{1}{2} Y_1 + \frac{1}{2} Y_2 ] = 5.
      \end{equation*}
      The variance of $Z$ is
      \begin{equation*}
        \Var(Z) = \frac{1}{4} \Var(Y_1) + \frac{1}{4} \Var(Y_2) = 25.
      \end{equation*}
      Therefore, $Z$ does not have the same distribution as $Y$.
  \end{enumerate}
\end{solution}

\begin{defn}[Quantiles]\index{Quantiles}\label{defn:quantiles}
  The \hlnoteb{$100p\%$ quantile} (or \hldefn{percentile}) of an rv $X$ is a set $\pi_p$ such that\marginnote{This definition may also be presented as: any number $\pi_p$ such that
  \begin{equation*}
    P(X < \pi_p) \leq p \leq P(X \leq \pi_p).
  \end{equation*}}
  \begin{equation*}
    \pi_p = \{ x \in X \mid P(X < x) \leq p \leq P(X \leq x) \}.
  \end{equation*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item If $X$ is a continuous random variable, we have that $P(X < \pi_p) = P(X \leq \pi_p)$ and so we have to define the quantile as
      \begin{equation*}
        \pi_p = F^{-1} (p)
      \end{equation*}
      where $F^{-1}$ is the inverse function of $F$, the cdf of $X$.

    \item A quantile \hlimpo{can be a set of numbers}.
    \item $\pi_{0.5}$ is called the \hldefn{median} of $X$.
  \end{itemize}
\end{note}

\marginnote{Graphical method to interpret this notion will be included.}

\begin{eg}[Lecture Notes: Example 1]
  Find the $100p\%$ quantile of the loss distribution $F(x) = 1 - e^{-\frac{x}{\theta}}$, $x > 0$.
\end{eg}

\begin{solution}
  Note that $F$ is the cdf of an exponential distribution, which is a continuous distribution. Therefore,
  \begin{equation*}
    F(\pi_p) = 1 - e^{-\frac{\pi_p}{\theta}} = p \implies \pi_p = - \theta \ln (1 - p).
  \end{equation*}
\end{solution}

\begin{eg}[Lecture Notes: Example 2]
  Find the median $\pi_{0.5}$ for the following cdf
  \begin{equation*}
    F(x) = \begin{cases}
      0                                & x < 0 \\
      0.6 + 0.4 (1 - e^{-\frac{x}{3}}) & x \geq 0
    \end{cases}
  \end{equation*}
\end{eg}

\begin{solution}
  Since $F(0) = 0.6$ and $F$ is an increasing function, we have that $F(x) = 0$ for all $x < 0$. Therefore
  \begin{equation*}
    \pi_{0.5} = 0.
  \end{equation*}
\end{solution}

\begin{eg}[Lecture Notes: Example 3]
  Find the median $\pi_{0.5}$ for a loss $X$ with pmf
  \begin{equation*}
    p(0) = 0.25, \, p(1) = 0.25, \, p(2) = 0.5.
  \end{equation*}
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      0    & x < 0 \\
      0.25 & 0 \leq x < 1 \\
      0.5  & 1 \leq x < 2 \\
      1    & x \geq 2
    \end{cases}
  \end{equation*}
  since $F(x) = 0.5$ when $1 \leq x < 2$, we have that
  \begin{equation*}
    \pi_{0.5} = [1, 2].
  \end{equation*}
\end{solution}

% subsection distributional_quantities (end)

% section distributional_quantities_and_risk_measures (end)

% chapter lecture_3_sep_13th (end)

\chapter{Lecture 4 Sep 18th}%
\label{chp:lecture_4_sep_18th}
% chapter lecture_4_sep_18th

\section{Distributional Quantities and Risk Measures (Continued)}%
\label{sec:distributional_quantities_and_risk_measures_continued}
% section distributional_quantities_and_risk_measures_continued

\subsection{Risk Measures}%
\label{sub:risk_measures}
% subsection risk_measures

\begin{defn}[Risk Measure]\index{Risk Measure}\label{defn:risk_measure}
  A \hlnoteb{risk measure} is a mapping from the loss rv to the real line $\mathbb{R}$.
\end{defn}

Klugman, Panjer \& Wilmot (2012)~\cite{KlugmanPanjerWillmot2012} on risk measure:

\begin{quotebox}{be-yellow}{light}
  The level of exposure to risk is often described by one number, or at least a small set of numbers. These numbers are necessarily functions of the model and are often called ‘key risk indicators’. Such key risk indicators indicate to risk managers the degree to which the company is subject to particular aspects of risk.
\end{quotebox}

To ensure its solvency, insurers will have to charge on these risks, i.e.\ we have to \hlnotea{price these exposures to risks}.

\begin{defn}[Premium Principle]\index{Premium Principle}\label{defn:premium_principle}
  A \hlnoteb{premium principle} (or \hldefn{insurance pricing}) is a rule for assigning a premium to an insurance risk.
\end{defn}

\begin{note}
  The following are some of the common principles used by insurers:
  \begin{itemize}
    \item \hldefn{Expectation Principle}
      \begin{equation*}
        \Pi(X) = ( 1 + \theta ) E(X), \quad \theta > 0
      \end{equation*}
    \item \hldefn{Standard Deviation Principle}
      \begin{equation*}
        \Pi(X) = E(X) + \theta \sqrt{\Var(X)}, \quad \theta > 0
      \end{equation*}
    \item \hldefn{Dutch Principle}
      \begin{equation*}
        \Pi(X) = E(X) + \theta E( {[ X - E(X) ]}_+ ), \quad \theta > 0
      \end{equation*}
  \end{itemize}
\end{note}

One particular measure is known as the \hlnotea{Value-at-Risk} (VaR).

\subsubsection{Value-At-Risk}\label{ssub:Value-At-Risk}

\begin{defn}[Value-at-Risk (VaR)]\index{Value-at-Risk}\index{VaR}\label{defn:value_at_risk}
The \hlnoteb{Value-at-Risk (VaR)} is a \hlnotea{quantile} of the distribution of aggregate losses, i.e.\ the $VaR$ of a risk $X$ at the $100\%p$ level is defined as\sidenote{I must find out why we define using $\inf$ instead of $\min$ (see following remark), and I will not take ``safe definition'' as an answer without full justification.}
  \begin{align*}
    \pi_p = \VaR_p (X) &= \inf \{ x \in \mathbb{R} : P (X > x) \leq 1 - p \} \\
               &= \inf \{ x \in \mathbb{R} : P (X \leq x) \geq p \}.
  \end{align*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item $\VaR$ is often called a \hldefn{quantile risk measure}.
    \item $\VaR$ is the standard risk measure used to evaluate exposure to risks.
    \item $\VaR$ measures the amount of capital required by the insurer to remain solvent, with high certainty, in the face of large claims.
    \item In practice, $p$ is generally high: $99.95\%$ or as low as $95\%$.
  \end{itemize}
\end{note}

\begin{remark}
  Observe that\marginnote{This remark basically points out that the left endpoint of the interval $B$ is always included, which should be quite clear by right-continuity of $F$.}
  \begin{equation*}
    B = \{ x \in \mathbb{R} \mid F_X(x) \geq p \} = (A, \infty) \text{ or } [A, \infty)
  \end{equation*}
  for some $A \in \mathbb{R}$, since $F$ is an increasing function. Now let $x_0 \in B$ such that
  \begin{equation*}
    F(x_0) = P(X \leq x_0) \geq p \quad \land \quad F(x_0-) = P(X < x_0) \leq p,
  \end{equation*}
  i.e.\ it is not necessary that $P(X = x_0) = p$ (see the two example graphs on the margin).
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (4, 0) node[right] {$x$};
      \draw[->] (0, 0) -- (0, 4) node[above] {$F(x)$};
      \draw (0, 1) -- (1, 1);
      \draw (1, 2) -- (2, 2);
      \draw[->] (2, 3) -- (4, 3);
      \node[circle,fill,inner sep=1pt] at (1, 2) {};
      \node[circle,fill,inner sep=1pt] at (2, 3) {};
      \node[circle,draw,inner sep=1pt] at (1, 1) {};
      \node[circle,draw,inner sep=1pt] at (2, 2) {};
      \draw[dotted] (4, 1.5) -- (0, 1.5) node[left] {$p$};
      \draw[dotted] (1, 2) -- (1, 0) node[below] {$x_0$};
    \end{tikzpicture}
    \caption{Discrete cdf}
  \end{marginfigure}
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (4, 0) node[right] {$x$};
      \draw[->] (0, 0) -- (0, 4) node[above] {$F(x)$};
      \draw[->,smooth,domain=0:4] plot (\x,{sqrt(\x)});
      \draw[dotted] (4, 1.5) -- (0, 1.5) node[left] {$p$};
      \draw[dotted] (2.25, 2) -- (2.25, 0) node[below] {$x_0$};
      \node[circle,fill,inner sep=1pt] at (2.25,1.5) {};
    \end{tikzpicture}
    \caption{Continuous cdf}
  \end{marginfigure}
  Let ${\{ x_n \}}_{n \in \mathbb{N}}$ be a decreasing sequence of points on $\mathbb{R}$ such that $x_n \to x_0$ as $n \to \infty$. Since $F$ is right-continuous, we have that $F(x_n) \to F(x_0)$ as $n \to \infty$. Therefore,
  \begin{equation*}
    B = [ x_0 , \infty )
  \end{equation*}\marginnote{The lecturer asserts that we can really define $\VaR$ using $\min$ instead of $\inf$, but even with this, I am not completely satisfied or convinced.}
  This justifies the definition of $\pi_p$.
\end{remark}

\begin{note}
  \begin{itemize}
    \item Note that by definition, we have
      \begin{equation*}
        P(X < \pi_p) \leq p \leq P(X \leq \pi_p)
      \end{equation*}
    \item If $X$ is a crv whose cdf is strictly increasing, i.e.\ no constant points, then
      \begin{equation*}
        \pi_p = F^{-1}(p)
      \end{equation*}
      since $P(X < \pi_p) = P(X \leq \pi_p)$.
  \end{itemize}
\end{note}

\begin{warning}[Shortcomings of $\VaR$]
  \begin{itemize}
    \item $\VaR$ cannot tell us the size of the potential loss in the $100(1 - p)\%$ cases, making it difficult for us to prepare the right amount in order to safeguard against insolvency.
    \item $\VaR$ actually fails to satisfy properties to be a \hlnotea{coherent risk measure}\sidenote{See \cref{sec:coherent_risk_measure}.}, for example, \hlnotea{subadditivity}.
    \item $\VaR$ is extensively used in financial risk management of trading risk over a fixed (usually short) time period, which are usually normally distributed, and $\VaR$ satisfies all coherency requirements.
    \item In insurance losses, instead of normal distributions, in general, skewed distributions are used, and in this cases, $\VaR$ is flawed as it lacks subadditivity.
  \end{itemize}
\end{warning}

\begin{eg}\label{eg:varp_pareto}
  Suppose that $X$ has a Pareto distribution with cdf
  \begin{equation*}
    F(x) = 1 - {\left( \frac{\theta}{x + \theta} \right)}^\alpha , \quad x > 0
  \end{equation*}
  where $\alpha, \theta > 0$. Find $\VaR_p(X)$.
\end{eg}

\begin{solution}
  Since $F$ is continuous and strictly increasing, we have that
  \begin{equation*}
    \pi_p = F^{-1}(p) = \theta \left[ {(1 - p)}^{-\frac{1}{\alpha}} - 1 \right]
  \end{equation*}
\end{solution}

\begin{eg}
  Find $\VaR_{0.95}(X)$, $\VaR_{0.5}(X)$, and $\VaR_{0.3}(X)$ for a random loss with pmf
  \begin{equation*}
    p(0) = 0.25, \, p(1) = 0.25, \, \text{ and } p(2) = 0.5.
  \end{equation*}
\end{eg}

\begin{solution}
  Note that the cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      0    & x < 0 \\
      0.25 & 0 \leq x < 1 \\
      0.5  & 1 \leq x < 2 \\
      1    & x \geq 2
    \end{cases}.
  \end{equation*}
  Therefore,
  \begin{equation*}
    \VaR_{0.95}(X) = 2, \, \VaR_{0.5}(X) = 1, \, \text{ and } \VaR_{0.3}(X) = 1.
  \end{equation*}
\end{solution}

\subsubsection{Tail-Value-at-Risk}\label{ssub:Tail-Value-at-Risk}

To compensate for the weakness of $\VaR$ at giving us the size of the loss $X$ of which we cannot measure, we use the \hlnotea{Tail-Value-at-Risk}.

\begin{defn}[Tail-Value-at-Risk (TVaR)]\index{Tail-Value-at-Risk}\label{defn:tail_value_at_risk}
Let $X$ be an rv. The \hlnoteb{Tail-Value-at-Risk (TVaR)} of $X$ at the $100p\%$ level, denoted as $\TVaR_p(X)$, is defined as the average of all $\VaR$ values above the level $p$, and expressed as\marginnote{TVaR also has the following names, used by different regions:
\begin{itemize}
  \item \hlnotea{Conditional Tail Expectation} (CTE) --- NA
  \item \hlnotea{Tail Conditional Expectation} (TCE)
  \item \hlnotea{Expected Shortfall} (ES) --- EU
\end{itemize}}
  \begin{equation*}
    \TVaR_p(X) = \frac{1}{1 - p} \int_{p}^{1} \VaR_\alpha(X) \dif{\alpha} = \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha}
  \end{equation*}
\end{defn}

\begin{remark}
  By considering the average of $\VaR$ from $p$'s going up to $1$, we take into account even the extreme cases of which $\VaR$ fails to account for.
\end{remark}

Perhaps a clearer definition would be the following, although the expression is only sensible if $X$ is a crv:

\begin{defn}[Tail-Value-at-Risk (TVaR)]\index{Tail-Value-at-Risk}\label{defn:tail_value_at_risk_v2}
  Let $X$ be an rv. The \hlnoteb{Tail-Value-at-Risk (TVAR)} of $X$ at the $100p\%$ level, denoted $\TVaR_p(X)$, is the expected loss given that the loss exceeds the $100p$ percentile (or quantile) of the distribution of $X$, expressible as
  \begin{equation*}
    \TVaR_p(X) = E[ X \mid X > \pi_p ] = \frac{1}{\bar{F}(\pi_p)} \int_{\pi_p}^{\infty} x f(x) \dif{x}.
  \end{equation*}
\end{defn}

Note that the two definitions agree with one another:

\begin{align*}
  \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} &= \frac{1}{1 - F(\pi_p)} \int_{p}^{1} F^{-1}(\alpha) \dif{\alpha} \\
                                                       &= \frac{1}{\bar{F}(\pi_p)} \int_{\pi_p}^{1} x f(x) \dif{x}
\end{align*}
where we let $\alpha = F(x)$ as substitution.

\begin{note}
  While it is not difficult to notice that
  \begin{equation*}
    \TVaR_p(X) \geq \VaR_p(X),
  \end{equation*}
  the proof is also simple:
  \begin{align*}
    \TVaR_p(X) &= \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} \\
               &\geq \frac{1}{1 - p} \pi_p \int_{p}^{1} \dif{\alpha} = \pi_p = \VaR_p(X).
  \end{align*}
\end{note}

\begin{eg}
  Find $\TVaR_p(X)$ for $X \sim \Exp(\theta)$.
\end{eg}

\begin{solution}
  Since $X$ is a crv, and $F(x) = 1 - e^{- \frac{x}{\theta}}$, we have that
  \begin{equation*}
    \pi_p = F^{-1}(p) = - \theta \ln (1 - p).
  \end{equation*}
  Therefore,
  \begin{align*}
    \TVaR_p(X) &= \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} = \frac{- \theta}{1 - p} \int_{p}^{1} \ln (1 - \alpha) \dif{\alpha} \\
               &= \frac{- \theta}{1 - p} \int_{-\infty}^{\ln (1 - p)} ue^u \dif{u} \quad \text{ let } u = \ln ( 1 - \alpha ) \\
               &= \frac{-\theta}{1 - p} \left[ ue^u \at{-\infty}{\ln (1 - p)} - \int_{-\infty}^{\ln(1-p)} e^u \dif{u} \right] \text{ by IBP } \\
               &= \frac{-\theta}{1 - p} \left[ (1 - p) \ln (1 - p) - ( 1 - p ) \right]\\
               &= \theta [ 1 - \ln (1 - p) ]
  \end{align*}
\end{solution}

\begin{note}
  From the last example, by the memoryless property of $\Exp(\theta)$, notice that we may also do
  \begin{align}
    \TVaR_p(X) &= E[ X \mid X > \pi_p ] = E [ X - \pi_p + \pi_p \mid X > \pi_p ] \nonumber \\
               &= E[ X - \pi_p \mid X > \pi_p ] + E[ \pi_p \mid X > \pi_p ] \label{eq:tvar_memoryless_exp}\\
               &= E[ X ] + \pi_p \nonumber
  \end{align}
\end{note}

% subsection risk_measures (end)

% section distributional_quantities_and_risk_measures_continued (end)

% chapter lecture_4_sep_18th (end)

\chapter{Lecture 5 Sep 20th}%
\label{chp:lecture_5_sep_20th}
% chapter lecture_5_sep_20th

\section{Distrbutional Quantities and Risk Measures (Continued 2)}%
\label{sec:distrbutional_quantities_and_risk_measures_continued_2}
% section distrbutional_quantities_and_risk_measures_continued_2

\subsection{Risk Measures (Continued)}%
\label{sub:risk_measures_continued}
% subsection risk_measures_continued

Before ending this section, we introduce a notion that is related to $\TVaR$.

\begin{defn}[Mean Excess Loss]\index{Mean Excess Loss}\label{defn:mean_excess_loss}
  Let $X$ be an rv, and $d \in \mathbb{R}$. The \hlnoteb{mean excess loss}, denoted $e_X(d)$, is defined as
  \begin{equation*}
    e_X(d) = E[ X - d \mid X > d ]
  \end{equation*}
  and $e_X(d) = 0$ for those $d$ such that $P(X > d) = 0$.
\end{defn}

\begin{propo}[Relation of $\TVaR_p(X)$ and $e_X(d)$]\label{propo:relation_of_tvar_p_x_and_e_x_d_}
  For a crv $X$, we have
  \begin{equation*}
    \TVaR_p(X) = e_X(\pi_p) + \VaR_p(X)
  \end{equation*}
\end{propo}

\begin{proof}
  By \cref{eq:tvar_memoryless_exp}, we have that
  \begin{equation*}
    \TVaR_p(X) = E [ X - \pi_p \mid X > \pi_p ] + \pi_p = e_X(\pi_p) + \pi_p.
  \end{equation*}\qed\
\end{proof}

\begin{propo}[Expection from Survival Function]\label{propo:expection_from_survival_function}
  Let $X$ be a non-negative rv such that $E[X^k] < \infty$, for any $k \in \mathbb{N} \setminus \{ 0 \}$. Then\sidenote{Note that this works for the discrete case as well, by replacing $\int$ with $\sum$.}
  \begin{equation*}
    E\left[X^k\right] = k \int_{0}^{\infty} x^{k - 1} \bar{F}(x) \dif{x}
  \end{equation*}
\end{propo}

\begin{proof}
  Firstly, note that since $E[X^k] < \infty$ for all $k \in \mathbb{N} \setminus \{0\}$, we have that $\bar{F}(x)$ decays faster than $x^k$ as $x \to \infty$. Now
  \begin{align*}
    E\left[ X^k \right] &= \int_{0}^{\infty} x^k f(x) \dif{x} \quad \because \text{ Law of the Unconscious Statistician} \\
                        &= \int_{0}^{\infty} x^k \dif{F(x)} \quad \because \dif{F(x)} = f(x) \dif{x} \\
                        &= - \int_{0}^{\infty} x^k \dif{\bar{F}(x)} \\
                        &= - \left[ x^k \bar{F}(x) \at{0}{\infty} - \int_{0}^{\infty} kx^{k - 1} \bar{F}(x) \dif{x} \right] \quad \because \text{ IBP } \\
                        &= k \int_{0}^{\infty} x^{k - 1} \bar{F}(x) \dif{x}
  \end{align*}\qed\
\end{proof}

\begin{eg}
  Calculate $e_X(d)$ and $\TVaR_p(X)$ for a Pareto distribution $X$ with cdf
  \begin{equation*}
    F(x) = 1 - {\left( \frac{\theta}{x + \theta} \right)}^\alpha, \quad x > 0,
  \end{equation*}
  where $\alpha > 1$ and $\theta > 0$.
\end{eg}

\begin{solution}
  Using \cref{propo:expection_from_survival_function},
  \begin{align*}
    e_X(d) &= \int_{0}^{\infty} P(X - d > x \mid X > d) \dif{x} = \int_{0}^{\infty} \frac{P(X - d > x, X > d)}{P(X > d)} \dif{x} \\
           &= \int_{0}^{\infty} \frac{P(X > x + d)}{P(X > d)} \dif{x} = \int_{0}^{\infty} \frac{\bar{F}(x + d)}{\bar{F}(d)} \dif{x} \\
           &= \int_{0}^{\infty} {\left( \frac{d + \theta}{x + d + \theta} \right)}^\alpha \dif{x} = \frac{{( d + \theta )}^\alpha}{1 - \alpha} {\left( \frac{1}{x + d + \theta} \right)}^{ \alpha - 1 } \at{0}{\infty} \\
           &= \frac{d + \theta}{\alpha - 1}
  \end{align*}
  By \cref{eg:varp_pareto}, we have
  \begin{equation*}
    \pi_p = \theta \left[ {( 1 - p )}^{-\frac{1}{\alpha}} - 1 \right]
  \end{equation*}
  and so
  \begin{align*}
    \TVaR_p(X) &= e_X(\pi_p) + \pi_p \\
               &= \frac{\theta\left[ {(1 - p)}^{-\frac{1}{\alpha}} - 1 \right] + \theta}{\alpha - 1} + \theta\left[ {( 1 - p )}^{-\frac{1}{\alpha}} - 1 \right] \\
               &= \frac{\theta{(1 - p)}^{-\frac{1}{\alpha}}}{\alpha - 1} + \frac{\theta(\alpha - 1){(1 - p)}^{-\frac{1}{\alpha}}}{\alpha - 1} - \theta \\
               &= \frac{\theta \alpha {( 1 - p )}^{-\frac{1}{\alpha}}}{\alpha - 1} - \theta
  \end{align*}
\end{solution}

\begin{propo}[Expected Deductible]\label{propo:expected_deductible}
  We have
  \begin{equation*}
    E( {[ X - d ]}_+ ) = \int_{d}^{\infty} \bar{F}(x) \dif{x}
  \end{equation*}
\end{propo}

\begin{proof}
  By the Law of the Unconscious Statistician and IBP on the last step,
  \begin{equation*}
    E( {[ X - d ]}_+ ) = \int_{d}^{\infty} ( x - d ) \dif{F(x)} = - \int_{d}^{\infty} (x - d) \dif{\bar{F}(x)} = \int_{d}^{\infty} \bar{F}(x) \dif{x}
  \end{equation*}\qed\
\end{proof}

\begin{propo}[An Expression for Mean Excess Value]\label{propo:an_expression_for_mean_excess_value}
  If $\bar{F}(d) > 0$, we have
  \begin{equation*}
    e_X(d) = \frac{\int_{d}^{\infty} \bar{F}(x) \dif{x}}{\bar{F}(d)}
  \end{equation*}
\end{propo}

\begin{proof}
  Observe that by \cref{propo:expected_deductible}, we have
  \begin{align*}
    e_X(d) &= E[ X - d \mid X > d ] = \frac{E[ ( X - d ) \mathbb{1}_{X > d} ]}{P(X > d)} \\
           &= \frac{E( {[ X - d ]}_+ )}{\bar{F}(d)} = \frac{\int_{d}^{\infty} \bar{F}(x) \dif{x}}{\bar{F}(d)}
  \end{align*}\qed\
\end{proof}

% subsection risk_measures_continued (end)

% section distrbutional_quantities_and_risk_measures_continued_2 (end)

\section{Severity Distributions --- Creating Severity Distributions}%
\label{sec:severity_distributions_creating_severity_distributions}
% section severity_distributions_creating_severity_distributions

Recall the definition of a severity distribution.

\begin{defnnonum}[Severity Distribution]\index{Severity Distribution}
  A \hlnoteb{severity distribution} is a distribution used to describe single random losses in an insurance portfolio.
\end{defnnonum}

When a loss occurs, the full amount of the loss is not necessarily the amount paid by the insurer, since an insurance policy typically involves some form of adjustment (e.g. \hlnotea{deductible, limit, coinsurance}). A distinction needs to be made between the actual loss prior to any of the adjustments (aka \hldefn{ground-up loss}) and the amount ultimately paid by the insurer.

Our goal is to find a reasonable model for the \hlnotea{ground-up loss} rv $X$. The following are two desirable properties for $X$:
\begin{itemize}
  \item $\text{Im}(X) = \mathbb{R}_{> 0}$, since losses are positive;
  \item pf of $X$ is right-skewed, since we want the ``tail'' of the distribution to be not heavy.
    \begin{itemize}
      \item The motivation for this property is due to the \hlnotea{20-80 rule}: 20\% of the largest claims accountn for 80\% of the total claim amount.
    \end{itemize}
\end{itemize}

\newthought{There are} two approaches to constructing a severity distribution:
\begin{itemize}
  \item \hlnotea{Parametric approach}\sidenote{This approach shall be the focus of this course.}: specify a ``form'' for the distribution with a finite number of parameters.
  \item Nonparametric approach: no form is specified; the distribution is constructed directly from the empirical data.
\end{itemize}

A weakness of the \textbf{Nonparametric approach} is, if there is not enough data, such as in catasthropic risks, is becomes difficult to obtain reliable information. We shall look at one such example in this approach.

\begin{defn}[Empirical Distribution Function]\index{Empirical Distribution Function}\label{defn:empirical_distribution_function}
  Let $\{ X_1, \ldots, X_n \}$ be an iid sample of a risk $X$. Then its \hlnoteb{empricial distribution function (edf)} is defined as
  \begin{equation*}
    \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^{n}  \mathbb{1}_{\{X_i \leq x\}}, \quad x \in \mathbb{R}.
  \end{equation*}
\end{defn}

\begin{remark}
  Simply put, the edf assigns a probability of $\frac{1}{n}$ to each sample point $X_i$.
\end{remark}

\begin{eg}
  Consider a random sample of a risk with size $5$: $\{ 30, 80, 150, 150, 200 \}$. Find the edf of the risk.
\end{eg}

\begin{solution}
  The edf is given by
  \begin{equation*}
    \hat{F}_n(x) = \frac{1}{5} \sum_{i=1}^{5} \mathbb{1}_{\{ X_i \leq x_i \}} = \begin{cases}
      0           & x < 30 \\
      \frac{1}{5} & 30 \leq x < 80 \\
      \frac{2}{5} & 80 \leq x < 150 \\
      \frac{4}{5} & 150 \leq x < 200 \\
      1           & x \geq 200
    \end{cases}
  \end{equation*}
\end{solution}

% section severity_distributions_creating_severity_distributions (end)

% chapter lecture_5_sep_20th (end)

\chapter{Lecture 6 Sep 25th}%
\label{chp:lecture_6_sep_25th} % chapter lecture_6_sep_25th

\section{Severity Distributions --- Creating Severity Distributions (Continued)}%
\label{sec:severity_distributions_creating_severity_distributions_continued}
% section severity_distributions_creating_severity_distributions_continued

\paragraph{The Parametric Approach} The following is a graph showing the process of a parametric approach:

\begin{figure}[h]
  \begin{tikzpicture}[
    block/.style = {rectangle, draw, rounded corners,
               text width =15em, align=center,
               color=dark,fill=be-blue},
    sblock/.style = {rectangle, draw, rounded corners,
               text width =4em, align=center},
    ]
    \node [block] (selection) {\textbf{Model Selection}\\
      select a model based on prior knowledge of historical datasets};
    \node [block, below=of selection] (estimation) {\textbf{Model Estimation}\\
      estimate parameter values based on data};
    \node [block, below=of estimation] (validation) {\textbf{Model Validation} \\
      test for goodness-of-fit};
    \node [block, below=of validation] (decision) {Is model acceptable?};
    \node [sblock, left=of decision,color=dark,fill=be-red] (no) {\textbf{No}};
    \node [sblock, right=of decision,color=dark,fill=be-green] (yes) {\textbf{DONE!}};
    
    \draw[-latex'] (selection) -- (estimation);
    \draw[-latex'] (estimation) -- (validation);
    \draw[-latex'] (validation) -- (decision);
    \draw[-latex'] (decision) -- (no);
    \draw[-latex'] (no) |- (selection.west);
    \draw[-latex'] (decision) -- (yes);
  \end{tikzpicture}
  \caption{Process of a Parametric Approach}\label{fig:process_of_a_parametric_approach}
\end{figure}

\paragraph{Common Techniques in Creating New Parametric Distributions} Before diving into the topic, first, a definition:

\begin{defn}[Parametric Distribution]\index{Parametric Distribution}\label{defn:parametric_distribution}
  A \hlnoteb{parametric distribution} is a set of distribution functions, of which each member is determined by specifying one or more parameters.
\end{defn}

Some common techniques are the following:
\begin{itemize}
  \item Multiplication by a constant
  \item Raising to a power
  \item Exponentiation
  \item Mixture of distributions
\end{itemize}

\subsection{Multiplication By A Constant}%
\label{sub:multiplication_by_a_constant}
% subsection multiplication_by_a_constant

This transformation is equivalent to applying inflation uniformly across all loss levels, and is known as a change of scale.

\begin{propo}[Multiplication by a Constant]\label{propo:multiplication_by_a_constant}
  Let $X$ be a crv with cdf $F_X$ and pdf $f_X$. Let $Y = cX$ for some $c > 0$. Then
  \begin{equation*}
    F_Y(y) = F_X\left( \frac{y}{c} \right), \quad f_Y(y) = \frac{1}{c}f_X\left( \frac{y}{c} \right).
  \end{equation*}
\end{propo}

\begin{proof}
  \begin{gather*}
    F_Y(y) = P(Y \leq y) = P( cX \leq y ) = P\left( X \leq \frac{y}{c} \right) = F_X\left( \frac{y}{c} \right) \\
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X\left( \frac{y}{c} \right) = \frac{1}{c}f_X\left( \frac{y}{c} \right)
  \end{gather*}\qed\
\end{proof}

\begin{defn}[Scale Distribution]\index{Scale Distribution}\label{defn:scale_distribution}
  We say that a parametric distribution is a \hlnoteb{scale distribution} if $Y = cY$ for any positive constant $c$ is from the same set of distributions as $X$.
\end{defn}

It is clear that we have the following result:

\begin{crly}\label{crly:constant_multiplication_crly}
  The parameter $c$ in \cref{propo:multiplication_by_a_constant} is a scale parameter, and $Y$ is a scale distribution.
\end{crly}

\begin{eg}\label{eg:scale_distn_exp}
  Let $X \sim \Exp(\theta)$ with pdf
  \begin{equation*}
    f_X(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}, \quad x > 0.
  \end{equation*}
  Let $y = cX$ with $c > 0$, it follows that
  \begin{equation*}
    f_Y(y) = \frac{1}{c}f_X\left(\frac{y}{c}\right) = \frac{1}{c \theta} e^{- \frac{y}{c \theta}}, \quad y > 0.
  \end{equation*}
  Thus $Y \sim \Exp(c\theta)$ and so $Y$ is a scale distribution. In particular, the exponential distribution belongs to a family of scale distributions.
\end{eg}

\begin{defn}[Scale Parameter]\index{Scale Parameter}\label{defn:scale_parameter}
  A parameter $\theta$ is called a \hlnoteb{scale paramter} of a parametric distribution $X$ if it satisfies the following condition: the parametric value of $cX$ is $c \theta$ for any positive constant $c$, and other parameters (if any) remain unchanged.
\end{defn}

\begin{eg}
  From \cref{eg:scale_distn_exp}, we had that
  \begin{equation*}
    f_X(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}, \quad x > 0.
  \end{equation*}
  We showed that $Y = cX \sim \Exp(c\theta)$. Therefore, the parameter $\theta$ is a scale parameter.
\end{eg}

\begin{eg}
  Determine whether the lognormal distribution $X \sim \LogN(\mu, \sigma^2)$, i.e. $\ln(X) \sim \Nor(\mu, \sigma^2)$, is a scale distribution or not. If yes, determine whether it has any scale parameter.
\end{eg}

\begin{solution}
  Let $Y = cX$ for some $c > 0$. Observe that
  \begin{equation*}
    \ln Y = \ln cX = \ln c + \ln X \sim \Nor(\mu + \ln c, \sigma^2).
  \end{equation*}
  For the last equation, note that if we let $Z = \ln X \sim \Nor(\mu, \sigma^2)$
  \begin{align*}
    E\left[ e^{t( Z + \ln c )} \right] &= e^{t \ln c} e^{\mu t + \frac{\sigma^2 t^2}{2}} = e^{t ( \mu + \ln c ) + \frac{\sigma^2 t^2}{2}}
  \end{align*}
  we see that the above is the mgf of $\Nor(\mu + \ln c, \sigma^2)$. Thus we have that $Y$ has the same distribution as $X$ and so it is a scale distribution. However, we also see that it has no scale parameters.
\end{solution}

% subsection multiplication_by_a_constant (end)

\subsection{Raising to a Power}%
\label{sub:raising_to_a_power}
% subsection raising_to_a_power

\begin{propo}[Raising to a Power]\label{propo:raising_to_a_power}
  Let $X$ be a crv with pdf $f_X$ and cdf $F_X$ with $F_X(0) = 0$. Let $Y = X^{\frac{1}{\tau}}$. If $\tau > 0$, then
  \begin{equation*}
    F_Y(y) = F_X(y^\tau), \quad f_Y(y) = \tau y^{\tau - 1} f_X(y^\tau), \quad y > 0,
  \end{equation*}
  while if $\tau < 0$, then
  \begin{equation*}
    F_Y(y) = 1 - F_X(y^\tau), \quad f_Y(y) = - \tau y^{\tau - 1} f_X(y^\tau), \quad y > 0.
  \end{equation*}
\end{propo}

\begin{proof}
  When $\tau > 0$,
  \begin{equation*}
    F_Y(y) = P(Y \leq y) = P\left(X^{\frac{1}{\tau}}\right) = P\left(X \leq y^\tau\right) = F_X\left(y^\tau\right)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy}f_X\left(y^\tau\right) = \tau y^{\tau - 1} f_X(y^\tau).
  \end{equation*}
  When $\tau < 0$,
  \begin{equation*}
    F_Y(y) = P(Y \leq y) = P\left(X^{\frac{1}{\tau}} \leq y\right) = P\left( X \geq y^\tau \right) = \bar{F}_X(y^\tau)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} (1 - F_X\left(y^\tau\right)) = - \tau y^{\tau - 1} f_X\left(y^\tau\right).
  \end{equation*}\qed\
\end{proof}

\begin{eg}
  Let $X \sim \Exp(\theta)$ and $Y = X^{\frac{1}{\tau}}$ for $\tau > 0$, we have
  \begin{equation*}
    F_Y(y) = F_X\left(t^{\tau}\right) = 1 - e^{ \frac{-y^\tau}{\theta} } = 1 - e^{-{\left( \frac{y}{\alpha} \right)}^\tau},
  \end{equation*}
  where $\alpha = \theta^{\frac{1}{\tau}}$. In particular, we have that $Y \sim \Wei(\alpha, \tau)$.
\end{eg}

% subsection raising_to_a_power (end)

\subsection{Exponentiation}%
\label{sub:exponentiation}
% subsection exponentiation

\begin{propo}[Exponentiation Method]\label{propo:exponentiation_method}
  Let $X$ be a crv with pdf $f_X$ and cdf $F_X$. Let $Y = e^X$. Then
  \begin{equation*}
    F_Y(y) = F_X(\ln y), \quad f_Y(y) = \frac{1}{y} f_X(\ln y).
  \end{equation*}
\end{propo}

\begin{proof}
  We have
  \begin{equation*}
    F_Y(y) = P\left( e^X \leq y \right) = P( X \leq \ln y ) = F_X(\ln y)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X(\ln y) = \frac{1}{y} f_X(\ln y).
  \end{equation*}\qed\
\end{proof}

\begin{ex}[Lognormal Distribution]\label{eg:lognormal_distribution}
  Let $X \sim \Nor(\mu, \sigma^2)$. The cdf and pdf of $Y = e^X$ is
  \begin{gather*}
    F_Y(y) = F_X(\ln y) = \Phi\left( \frac{\ln y - \mu}{\sigma} \right) \\
    f_Y(y) = \frac{1}{y} f_X(\ln y) = \frac{1}{y} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \cdot {\left( \frac{\ln y - \mu}{\sigma} \right)}^2}
  \end{gather*}
\end{ex}

% subsection exponentiation (end)

\subsection{Mixing Distributions}%
\label{sub:mixing_distributions}
% subsection mixing_distributions

The rationale behind mixing distributions is to define an rv $X$ conditional on a second rv, say $\Theta$ (aka \hldefn{mixing rv}). The mixing rv $\Theta$ can either be discrete or be continuous, which leads to two types of mixtures:
\begin{itemize}
  \item \hlnotea{discrete mixture}: when $\Theta$ is discrete; and
  \item \hlnotea{continuous mixture}: when $\Theta$ is continuous.
\end{itemize}

\begin{defn}[Discrete Mixed Distribution]\index{Discrete Mixed Distribution}\label{defn:discrete_mixed_distribution}
  Let $\Theta$ be a drv taking values on $\{ \theta_1, \theta_2, \ldots, \theta_n \}$ with
  \begin{equation*}
    P(\Theta = \theta_i) = p_i > 0, \quad i = 1, \ldots, n,
  \end{equation*}
  and the rv $Y_i := X \mid \Theta = \theta_i$ has cdf
  \begin{equation*}
    F_{Y_i}(x) = P(X \leq x \mid \Theta = \theta_i), x \in \mathbb{R}.
  \end{equation*}
  Then $X$ is called a \hlnoteb{discrete mixed distribution} with cdf
  \begin{equation*}
  F_X(x) = \sum_{i=1}^{n} P(X \leq x \mid \Theta = \theta_i) P(\Theta = \theta_i) = \sum_{i=1}^{n} p_i F_{Y_i}(x).
  \end{equation*}
\end{defn}

Following the above definition, by the Law of the Unconscious Statistician, we have
\begin{equation*}
  E[g(X)] = \sum^{n}_{i=1} E[g(X) \mid \Theta = \theta_i] P(\Theta = \theta_i) = \sum_{i=1}^{n} p_i E[g(Y_i)],
\end{equation*}
for any function $g$ such that the expectation exists. In particular, we have
\begin{equation*}
  E[X] = \sum_{i=1}^{n} p_i E[Y_i] \text{ and } E\left[ X^2 \right] = \sum_{i=1}^{n} p_i E\left[Y_i^2\right].
\end{equation*}

\begin{eg}
  Let $Y_i \sim \Exp(i)$ for $i = 1, 2, 3$. Define $X$ to be an equal mixture of these three exponential rvs. Fidn the cdf, pdf, and mean of $X$.
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{align*}
    F_X(x) &= \sum_{i=1}^{3} \frac{1}{3}F_{Y_i}(x) = \frac{(1 - e^{-x}) + (1 - e^{- x / 2}) + (1 - e^{-x / 3})}{3} \\
           &= 1 - \frac{1}{3} \left( e^{-x} + e^{-\frac{x}{2}} + e^{-\frac{x}{3}} \right), x > 0.
  \end{align*}
  The pdf of $X$ is
  \begin{equation*}
    f_X(x) = \frac{1}{3} \left( e^{-x} + \frac{1}{2} e^{-\frac{x}{2}} + \frac{1}{3} e^{-\frac{x}{3}} \right), x > 0.
  \end{equation*}
  The mean of $X$ is therefore
  \begin{equation*}
    E[X] = \sum_{i=1}^{3} E[ Y_i ] = \frac{1}{3} ( 1 + 2 + 3 ) = 2.
  \end{equation*}
\end{solution}

% subsection mixing_distributions (end)

% section severity_distributions_creating_severity_distributions_continued (end)

% chapter lecture_6_sep_25th (end)

\chapter{Lecture 7 Sep 27th}%
\label{chp:lecture_7_sep_27th}
% chapter lecture_7_sep_27th

\section{Severity Distributions --- Creating Severity Distributions (Continued 2)}%
\label{sec:severity_distributions_creating_severity_distributions_continued_2}
% section severity_distributions_creating_severity_distributions_continued_2

\subsection{Mixing Distributions (Continued)}%
\label{sub:mixing_distributions_continued}
% subsection mixing_distributions_continued

\begin{defn}[Continuous Mixture]\index{Continuous Mixture}\label{defn:continuous_mixture}
  Let $\Theta$ be a crv with density $f_\Theta$, and the cdf and pdf of $X \mid \Theta = \theta$ are given by
  \begin{equation*}
    F_{X \mid \Theta}(x \mid \theta) = P(X \leq x \mid \Theta = \theta) \text{ and } f_{X \mid \Theta}(x \mid \theta) = P(X = x \mid \Theta = \theta).
  \end{equation*}
  The unconditional distribution of $X$ is said to be a \hlnoteb{continuous mixed distribution} with cdf and pdf
  \begin{align*}
    F_X(x) &= \int_{-\infty}^{\infty} F_{X \mid \Theta}(x\mid\theta) f_{\Theta}(\theta) \dif{\theta} \\
    f_X(x) &= \int_{-\infty}^{\infty} f_{X \mid \Theta}(x\mid\theta) f_{\Theta}(\theta) \dif{\theta}.
  \end{align*}
  Furthermore, for any function $H$,
  \begin{equation*}
    E[H(X)] = \int_{-\infty}^{\infty} E[H(X) \mid \Theta = \theta] f_{\Theta}(\theta) \dif{\theta}.
  \end{equation*}
\end{defn}

\begin{eg}
  Suppose that $X \mid \Lambda = \lambda$ is exponentially distributed with mean $\frac{1}{\lambda}$, and let $\Lambda$ be a gamma distributed rv with mean $\alpha / \theta$ and variance $\alpha / \theta^2$, i.e.
  \begin{equation*}
    f_{\Lambda}(\lambda) = \frac{\theta^\alpha \lambda^{\alpha - 1} e^{-\theta \lambda}}{\Gamma(\alpha)}, \lambda > 0,
  \end{equation*}
  where $\Gamma(\alpha) = \int_{0}^{\infty} t^{\alpha - 1} e^{-t} \dif{t}$ is the gamma function. Determine the conditional pdf of $X$.
\end{eg}

\begin{solution}
  We have
  \begin{align*}
    f_X(x) &= \int_{0}^{\infty} f_{X \mid \Lambda}(x \mid \lambda) f_{\Lambda}(\lambda) \dif{\lambda} \\
           &= \int_{0}^{\infty} \lambda e^{-x \lambda} \frac{\theta^{\alpha} \lambda^{\alpha - 1} e^{-\theta \lambda}}{\Gamma(\alpha)} \dif{\lambda} \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha)} \int_{0}^{\infty} \lambda^\alpha e^{-\lambda ( x + \theta )} \dif{\lambda} \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha) (x + \theta)} \int_{0}^{\infty} {\left( \frac{y}{x + \theta} \right)}^\alpha e^{-y} \dif{y} \enspace \text{ where } y = \lambda ( x + \theta ) \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha) {( x + \theta )}^{\alpha + 1}} \int_{0}^{\infty} y^\alpha e^{-y} \dif{y} \\
           &= \frac{\theta^\alpha \Gamma(\alpha + 1)}{\Gamma(\alpha) {( x + \theta )}^{\alpha + 1}} = \frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}.
  \end{align*}
\end{solution}

\begin{propo}[Total Expectation and Total Variance]\index{Total Expectation}\index{Total Variance}\label{propo:total_expectation_and_total_variance}
  For any rvs $X$ and $\Theta$, provided that the repsective expectation and variance exist, we have
  \begin{gather*}
    E[X] = E[ E[ X \mid \Theta ] ] \\
    \Var(X) = E[ \Var(X \mid \Theta) ] + \Var( E[ X \mid \Theta ] )
  \end{gather*}
\end{propo}

\begin{proof}
  \begin{align*}
    E[X] &= E\left( \int_{X} xf_{X \mid \Theta}(x \mid \Theta) \dif{x} \right) \\
         &= \int_{\Theta} \int_{X} xf_{X \mid \Theta}( x \mid \theta ) f_{\Theta}(\theta) \dif{x} \dif{\theta} \\
         &= \int_{X} x \int_{\Theta} f_{X, \Theta}(x, \theta) \dif{\theta} \dif{x} \enspace \because \text{ Fubini's Theorem } \\
         &= \int_{X} xf_X(x) \dif{x} = E[X].
  \end{align*}
  Note that
  \begin{equation*}
    \Var(X \mid \Theta) = E[ X^2 \mid \Theta ] + E{[ X \mid \Theta ]}^2.
  \end{equation*}
  And so
  \begin{align*}
    &E[\Var(X \mid \Theta)] + \Var( E[ X \mid \Theta ] ) \\
    &= E[ E[ X^2 \mid \Theta ] ] - E \left[ E{[ X \mid \Theta ]}^2 \right] + E \left[ E {[ X \mid \Theta ]}^2 \right] - E{[ E [ X \mid \Theta ] ]}^2 \\
    &= E\left[X^2\right] - E{[X]}^2 = \Var(X)
  \end{align*}\qed\
\end{proof}

\begin{eg}
  Suppose that $X \mid \Theta = \theta \sim \Exp(\theta)$ and $p_{\Theta}(\theta) = \frac{1}{3}$ for $\theta = 1, 2, 3$. Find the mean and variance of $X$.
\end{eg}

\begin{solution}
  The mean of $X$ is
  \begin{equation*}
    E[X] = EE[X \mid \Theta] = E[\Theta] = \frac{1}{3} (1 + 2 + 3) = 2.
  \end{equation*}
  The variance of $X$ is
  \begin{align*}
    \Var(X) &= E[\Var(X \mid \Theta)] + \Var( E[X \mid \Theta] ) \\
            &= E[\Theta^2] + \Var(\Theta) = 2E[\Theta^2] - E{[\Theta]}^2 \\
            &= \frac{2}{3}(1 + 4 + 9) - 4 = \frac{28}{3} - \frac{12}{3} = \frac{16}{3}
  \end{align*}
\end{solution}

\begin{eg}
  Suppose that $X \mid \Lambda = \lambda \sim \Exp(\lambda)$ and $\Lambda \sim \Gam(\alpha, \theta)$ with mean $\alpha \theta$ and variance $\alpha \theta^2$. Find the mean and variance of $X$.
\end{eg}

\begin{solution}
  The mean of $X$ is
  \begin{equation*}
    E[X] = EE[X \mid \Lambda] = E[ \Lambda ] = \alpha\theta.
  \end{equation*}
  The variance of $X$ is
  \begin{align*}
    \Var(X) &= E[\Var(X \mid \Lambda)] + \Var( E[ X \mid \Lambda ] ) \\
            &= E[\Lambda^2] + \Var(\Lambda) = 2\Var(\Lambda) + E{[\Lambda]}^2 \\
            &= 2 \alpha \theta^2 + \alpha^2 \theta^2.
  \end{align*}
\end{solution}

% subsection mixing_distributions_continued (end)

% section severity_distributions_creating_severity_distributions_continued_2 (end)

\section{Severity Distributions --- Tail of Distributions}%
\label{sec:severity_distributions_tail_of_distributions}
% section severity_distributions_tail_of_distributions

\begin{defn}[Tail]\index{Tail}\label{defn:tail}
  The \hlnoteb{tail} of a distribution (usually the right tail) is the portion of the distribution corresponding to large values of the random variable.
\end{defn}

It is important that we understand large possible loss values as they have the greatest impact on the total losses that we may have to endure. In general, a loss rv is said to be \hldefn{heavy-tailed} if it has a large probability to take large values.

Two measurements of tail weight:
\begin{itemize}
  \item \textbf{relative}: comparing ``sizes'' of the tails of two distributions;
  \item \textbf{absolute}: classifying distributions as heavy or light-tailed.
\end{itemize}

The following is a set of criteria to measure or compare the heaviness of the tails of loss distributions:
\begin{itemize}
  \item Existence of moments
  \item Limiting ratios
  \item Hazard rate function
  \item Mean excess loss function
\end{itemize}

\subsection{Existence of Moments}%
\label{sub:existence_of_moments}
% subsection existence_of_moments

Recall that the $k$th moment of a loss $X$ is
\begin{equation*}
  E\left[X^k\right] = \int_{0}^{\infty} x^k f_X(x) \dif{x}.
\end{equation*}
Now if $f_X$ takes on large values for large $x$, we may have $E\left[X^k\right]$ blow up to infinity, and so it is desirable to find/use some distribution with a \hlnotea{decaying} probability function, one at which its rate of decay is faster than the growth of $x^{-(k + 1)}$.

% subsection existence_of_moments (end)

% section severity_distributions_tail_of_distributions (end)

% chapter lecture_7_sep_27th (end)

\chapter{Lecture 8 Oct 02nd}%
\label{chp:lecture_8_oct_02nd}
% chapter lecture_8_oct_02nd

\section{Severity Distributions --- Tail of Distributions (Continued)}%
\label{sec:severity_distributions_tail_of_distributions_continued}
% section severity_distributions_tail_of_distributions_continued

\subsection{Existence of Moments (Continued)}%
\label{sub:existence_of_moments_continued}
% subsection existence_of_moments_continued

\begin{eg}
  For a Pareto distribution, as $x \to \infty$, we have that $f_X(x) \sim x^{-(\alpha + 1)}$, so its moments are finite if and only if $k < \alpha$.

  We say that the Pareto distribution has a \hlnotea{power tail}.
\end{eg}

\begin{eg}
  Given the transformed Gamma distribution, with pdf
  \begin{equation*}
    f_X(x) = \frac{{\left( \frac{x}{\theta} \right)}^\alpha e^{- \frac{x}{\theta}}}{x \Gamma(\alpha)}.
  \end{equation*}
  Now as $x \to \infty$, we have
  \begin{equation*}
    f_X(x) \sim x^{\alpha - 1} e^{-\frac{x}{\theta}}
  \end{equation*}
  We see that the exponential term decays faster than the rate of growth of $x^{\alpha - 1}$ for any $\alpha > 0$. Thus all moments of the Gamma distribution exists.

  We say that the Gamma distribution has a \hlnotea{exponential tail}.
\end{eg}

\begin{ex}
  The Normal distribution has an exponential tail.
\end{ex}

\begin{defn}[Heavy-Tails Light-Tails]\index{Heavy-Tailed Distribution}\index{Light-Tailed Distribution}\label{defn:heavy_tails_light_tails}
  We say that a distribution is a \hlnoteb{heavy-tailed distribution} if \hlimpo{its moments only exist up to some $k \in \mathbb{N} \setminus \{ 0 \}$}.

  We say that a distribution is a \hlnoteb{light-tail distribution} if \hlimpo{its moments exist for all $k \in \mathbb{N} \setminus \{ 0 \}$}.
\end{defn}

\begin{note}
  We may also use the mgf to determine if a distribution has a heavy or light tail; the inexistence of the $k$th moment implies the inexistence of the mgf, i.e.\ if the mgf does not exist, then the moments of the distribution is only finite up to some $k \in \mathbb{N} \setminus \{0\}$.
\end{note}

\subsubsection{Limiting Ratio: Survival Functions}%
\label{ssub:limiting_ratio_survival_functions}
% subsubsection limiting_ratio_survival_functions

\begin{defn}[Limiting Ratio]\index{Limiting Ratio}\label{defn:limiting_ratio}
  The \hlnoteb{limiting ratio} of \hlnoteb{two survival functions} is used to compare the heaviness of tails of the two losses. Consider two losses $X$ and $Y$, and consider the limit of the ratio
  \begin{equation*}
    \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)}.
  \end{equation*}
  If the limit does not exist, we say that the comparison is inconclusive. Otherwise, we have 3 cases:
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}[yscale=2]
      \draw[->] (-0.5,-0.1) -- (3, -0.1) node[right] {$x$};
      \draw[->] (0,-0.5) -- (0, 1.5) node[above] {$y$};
      \draw[-,domain=0:3,color=be-blue,thick] plot ({\x},{ 0.5 * exp( -0.5 * \x )});
      \draw[-,domain=0:3,color=be-red,thick] plot ({\x},{ 1 * exp( -1 * \x )});
    \end{tikzpicture}
    \caption{Limiting Ratio}\label{fig:limiting_ratio}
  \end{marginfigure}
  \begin{itemize}
    \item If $c = 0$, then $\bar{F}_X(x)$ decays faster than $\bar{F}_Y(x)$ as $x \to \infty$, i.e. $Y$ has a heavier tail than $X$;
    \item If $0 < c < \infty$, then $\bar{F}_X(x)$ and $\bar{F}_Y(x)$ decays at the smae rate, as $x \to \infty$, i.e. $X$ and $Y$ have similar tails;
    \item If $c = \infty$, then $\bar{F}_X(x)$ decays slower than $\bar{F}_Y(x)$ as $x \to \infty$, i.e. $X$ has a heavier tail than $Y$;
  \end{itemize}
  where we let
  \begin{equation*}
    c := \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)}
  \end{equation*}
\end{defn}

\begin{note}
  Not all distributions have an explicit survival function, but they will always have a pdf/pmf. Fortunately, by \hlnotea{L'H\^{o}pital's Rule}, the above definition can be applied to the pdfs of $X$ and $Y$, i.e.
  \begin{equation*}
    c = \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)} = \lim_{x \to \infty} \frac{-f_X(x)}{-f_Y(x)} = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(x)}
  \end{equation*}
\end{note}

\begin{eg}
  Show that the Pareto distribution has a heavier tail than the Gamma distribution using limiting ratio.
\end{eg}

\begin{solution}
  Let $X \sim \Pareto(\alpha, \theta)$ and $Y \sim \Gam(\tau, \lambda)$. We have
  \begin{align*}
    c = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(x)} &= \lim_{x \to \infty} \frac{\frac{\alpha \theta^{\alpha}}{{( x + \theta )}^{\alpha + 1}}}{\frac{x^{\tau - 1} e^{-\frac{x}{\lambda}}}{\lambda^\tau \Gamma(\tau)}} = \alpha \theta^\alpha \lambda^\tau \Gamma(\tau) \lim_{x \to \infty} \frac{e^{\frac{x}{\lambda}}}{x^{\tau - 1} {(x + \theta)}^{\alpha + 1}}
  \end{align*}
  Since the exponential term grows faster than the term in the denominator, we have $c = \infty$, i.e. $X$ has a heavier tail than $Y$, as required.
\end{solution}

\begin{eg}
  For two losses $X$ and $Y$, suppose that $f_X(x) = \frac{2}{\pi (1 + x^2)}$ and $f_Y(x) = \frac{1}{(1 + x^2)}$ for $x > 0$. Compare the tail heaviness of the two losses.
\end{eg}

\begin{solution}
  Notice that
  \begin{equation*}
    c = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(y)} = \lim_{x \to \infty} = \frac{2}{\pi} < \infty,
  \end{equation*}
  i.e. $X$ and $Y$ have similar tails.
\end{solution}

% subsubsection limiting_ratio_survival_functions (end)

\subsubsection{Hazard Rate}%
\label{ssub:hazard_rate}
% subsubsection hazard_rate

\newthought{Recall} \cref{defn:hazard_rate_function}. We had
\begin{gather*}
  h(x) = \frac{f(x)}{\bar{F}(x)} = - \frac{d}{dx} \ln \bar{F}(x), \\
  h_X(x) \Delta x \approx P(X \leq x + \Delta x \mid X > x)
\end{gather*}
and the hazard rate function relates to the survival function as
\begin{equation*}
  \bar{F}(x) = e^{-\int_{-\infty}^{x} h(y) \dif{y} }.
\end{equation*}

Notice that
\begin{itemize}
  \item if the hazard rate function is a \hlnotea{decreasing} function, that implies that the probability of the occurrence of $X \leq x + \Delta x$ decreases given $X > x$, as $x$ increases, i.e. it is more likely that we have $X > x + \Delta x \mid X > x$. So $X$ has a \hlnotea{heavy tail}.
  \item if the hazard rate function is a \hlnotea{increasing} function, that implies that the probability of the occurrence of $X \leq x + \Delta x$ increases given $X > x$, as $x$ increases, i.e. it is less likely that $X > x + \Delta x \mid X > x$. So $X$ has a \hlnotea{light tail}.
\end{itemize}

\begin{defn}[Decreasing and Increasing Failure Rates]\index{Decreasing Failure Rate}\index{Increasing Failure Rate}\label{defn:decreasing_and_increasing_failure_rates}
  Let $X$ be a loss with hazard rate function $h_X$. We say that\sidenote{The following source claims that the \hlnotea{failure rate} and hazard rate are, in fact, not always interchangable terms: \url{https://nomtbf.com/2013/11/difference-hazard-failure-rate/}. Perhaps this is worth looking into.}
  \begin{itemize}
    \item $X$ or $F_X$ has a \hlnoteb{decreasing failure rate (DFR)} if $h_X$ is decreasing;
    \item $X$ or $F_X$ has a \hlnoteb{increasing failure rate (IFR)} if $h_X$ is increasing.
  \end{itemize}
\end{defn}

\begin{note}
  Consequently,
  \begin{itemize}
    \item Distributions that have a DFR are heavy-tailed;
    \item Distributions that have an IFR are light-tailed.
  \end{itemize}
\end{note}

\begin{propo}[Exponential has Constant Hazard Rate]\label{propo:exponential_has_constant_hazard_rate}
  The exponential distribution has a constant hazard rate.
\end{propo}

\begin{proof}
  The pdf and survival function of $X \sim \Exp(\lambda)$ is
  \begin{equation*}
    f_X(x) = \lambda e^{-\lambda x} \text{ and } \bar{F}_X(x) = e^{-\lambda x},
  \end{equation*}
  respectively. Thus the hazard rate of $X$ is
  \begin{equation*}
    h(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \lambda,
  \end{equation*}
  which is a fixed value.\qed\
\end{proof}

\begin{note}
  We say that the exponential distribution is the only distribution which is said to have both DFR and IFR.\sidenote{\hlwarn{Why?}}
\end{note}

\begin{eg}
  Let $X \sim \Pareto(\alpha, \theta)$ with $f_X(x) = \frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}$ and $\bar{F}_X(x) = \frac{\theta^\alpha}{{(x + \theta)}^\alpha}$. Determine whether $X$ has a DFR or IFR.
\end{eg}

\begin{solution}
  The hazard rate function of $X$ is
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{\frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}}{\frac{\theta^\alpha}{{(x + \theta)}^\alpha}} = \frac{\alpha}{x + \theta}.
  \end{equation*}
  It is clear that $h_X$ is a decreasing function, and so $X \sim \Pareto(\alpha, \theta)$ has a DFR, i.e.\ it is heavy-tailed.
\end{solution}

It is not always easy to get the survival function. The following is an alternative approach to finding out if the hazard rate function is increasing or decreasing.

\begin{propo}[Ratio Comparison for DFR/IFR]\label{propo:ratio_comparison_for_dfr_ifr}
  Let $X$ be an rv, and\sidenote{\hlwarn{Any bounds on $y$?}}
  \begin{equation*}
    s(x) = \frac{f_X(x + y)}{f_X(x)}.
  \end{equation*}
  \begin{enumerate}
    \item If $s(x)$ is increasing in $x$ for every $y$, then $X$ has a DFR;
    \item If $s(x)$ is decreasing in $x$ for every $y$, then $X$ has an IFR.
  \end{enumerate}
\end{propo}

\begin{proof}
  We shall prove for one case as the other will follow analogously. Notice that
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{f_X(x)}{\int_{x}^{\infty} f_X(y) \dif{y}} = \frac{1}{\int_{0}^{\infty} \frac{f_X(x + y)}{f_X(x)}\dif{y} }
  \end{equation*}
  by a change of variable in the last equality. We notice that if $\frac{f_X(x + y)}{f_X(x)}$ is increasing, then $h_X(x)$ will be decreasing, and so $X$ has a DFR.\qed\
\end{proof}

% subsubsection hazard_rate (end)

% subsection existence_of_moments_continued (end)

% section severity_distributions_tail_of_distributions_continued (end)

% chapter lecture_8_oct_02nd (end)

\chapter{Lecture 9 Oct 11th}%
\label{chp:lecture_9_oct_11th}
% chapter lecture_9_oct_11th

\section{Severity Distributions --- Tail of Distributions (Continued 2)}%
\label{sec:severity_distributions_tail_of_distributions_continued_2}
% section severity_distributions_tail_of_distributions_continued_2

\subsection{Mean Excess Loss}%
\label{sub:mean_excess_loss}
% subsection mean_excess_loss

\begin{defn}[Excess Loss Random Variable]\index{Excess Loss Random Variable}\label{defn:excess_loss_random_variable}
  For a loss rv $X$, we define the \hlnoteb{excess loss rv} as
  \begin{equation*}
    T_d = X - d \mid X > d, \quad d > 0.
  \end{equation*}
  The survival function of $T_d$ is
  \begin{align*}
    \bar{F}_{T_d}(x) &= P(T_d > x) = P(X - d > x \mid X > d) \\
                     &= \frac{P(X > x + d)}{P(X > d)} = \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)}.
  \end{align*}
\end{defn}

As defined before in \cref{defn:mean_excess_loss},

\begin{defnnonum}[Mean Excess Loss]\index{Mean Excess Loss}
  The \hlnoteb{mean excess loss} (or \hldefn{mean residual life}) function is defined as
  \begin{equation*}
    e_X(d) = E[T_d] = \int_{0}^{\infty} \bar{F}_{T_d}(x) \dif{x} = \frac{\int_{0}^{\infty} \bar{F}_X(x + d) \dif{x}}{\bar{F}_X(d)} = \frac{\int_{d}^{\infty} \bar{F}_X(y) \dif{y} }{\bar{F}_X(d)}
  \end{equation*}
\end{defnnonum}

\begin{defn}[Increasing and Decreasing Mean Residual Lifetime]\index{Increasing Mean Residual Lifetime}\index{Decreasing Mean Residual Lifetime}\index{IMRL}\index{DMRL}\label{defn:increasing_and_decreasing_mean_residual_lifetime}
  Given a loss rv $X$,
  \begin{enumerate}
    \item we say $X$ or $F_X$ is an \hlnoteb{increasing mean residual lifetime (IMRL)} if $e_X(x)$ is increasing in $x$;
    \item we say $X$ or $F_X$ is an \hlnoteb{decreasing mean residual lifetime (DMRL)} if $e_X(x)$ is decreasing in $x$.
  \end{enumerate}
\end{defn}

\begin{note}
  \begin{itemize}
    \item IMRL distributions are \hlnotec{heavy-tailed};
    \item DMRL distributions are \hlnotec{light-tailed}.\marginnote{
      \begin{ex}
        Prove this note.
      \end{ex}
      }
  \end{itemize}
\end{note}

\begin{propo}[Relation between DFR/IFR and IMRL/DMRL]\label{propo:relation_between_dfr_ifr_and_imrl_dmrl}
  A DFR rv is IMRL, and an IFR rv is a DMRL.
\end{propo}

\begin{proof}
  Suppose $X$ has a DFR. The mean excess loss of $X$ is
  \begin{equation*}
    e_X(d) = \frac{\int_{0}^{\infty} \bar{F}_X(x + d) \dif{x} }{\bar{F}_X(d)} = \int_{0}^{\infty} \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)} \dif{x}.
  \end{equation*}
  Note that by the relationship between the survival function and the hazard rate\sidenote{We use the hazard rate here because it is provided by the assumption.},
  \begin{equation*}
    \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)} = \frac{e^{-\int_{0}^{x + d} h_X(y) \dif{y} }}{e^{-\int_{0}^{d} h_X(y) \dif{y} }} = e^{-\int_{d}^{x + d} h_X(y) \dif{y} } = e^{-\int_{0}^{x} h_X(z + d) \dif{z} }.
  \end{equation*}
  Since $X$ has a DFR, $h_X$ is decreasing, and thus $\frac{\bar{F}_X(x + d)}{\bar{F}_X(d)}$ is increasing. Thus $e_X(d)$ is increasing and so $X$ is a IMRL, as required. THe argument is similar for $IFL$ being a $DMRL$.\qed\
\end{proof}

\begin{eg}
  Let $X \sim \Wei(\theta, \tau)$. Determine whether $X$ is DMRL or IMRL.
\end{eg}

\begin{solution}
  Since
  \begin{equation*}
    f_X(x) = \frac{\tau x^{\tau - 1} e^{-{\left( \frac{x}{\theta} \right)}^\tau}}{\theta^\tau}
  \end{equation*}
  and from \hyperref[note:weibull_survival_function]{an earlier example}, we have
  \begin{equation*}
    \bar{F}_X(x) = e^{- {\left( \frac{x}{\theta} \right)}^\tau}
  \end{equation*}
  Then the hazard rate is
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{\tau}{\theta^\tau} x^{\tau - 1}.
  \end{equation*}
  Now if $\tau \geq 1$, then $h_X(x)$ is an increasing function, and so $X$ has an IFR, i.e. $X$ is a DMRL. if $0 < \tau \leq 1$, then $h_X(x)$ is a decreasing function, and so $X$ has a DFR, i.e. $X$ is an IMRL.
\end{solution}

\begin{eg}
  Consider a loss $X$ with $f_X(x) = (1 + 2x^2) e^{-2x}$ for $x > 0$.
  \begin{enumerate}
    \item Determine $h_X(x)$.
    \item Determine $e_X(x)$.
    \item Find $\lim\limits_{x \to \infty} h_X(x)$ and $\lim\limits_{x \to \infty} e_X(x)$.
    \item Show that $X$ is DMRL but not IFR.
  \end{enumerate}
\end{eg}

\begin{solution}
  Since both $h_X(x)$ and $e_X(x)$ require the survival function, we shall first derive that. Observe that
  \begin{align*}
    \bar{F}_X(x) &= 
  \end{align*}
  \begin{enumerate}
    \item 
  \end{enumerate}
\end{solution}

% subsection mean_excess_loss (end)

% section severity_distributions_tail_of_distributions_continued_2 (end)

\appendix

\chapter{Additional Material}%
\label{chp:additional_material}
% chapter additional_material

\section{Individual Risk Model: An Alternate View}%
\label{sec:individual_risk_model_an_alternate_view}
% section individual_risk_model_an_alternate_view

\textit{This appendix serves to explain why our note of $Z_i = I_i X_i$ is wrong with as mush rigour as we can go for now. There may be hand-wavy parts, but those will be indicated.} 

We mentioned, as shown by Klugman, Panjer and Willmot (2012)\cite{KlugmanPanjerWillmot2012}, that for the \hyperref[defn:individual_risk_model]{Individual Risk Model}, the aggregate claim is modeled by
\begin{equation*}
  S = \sum_{i=1}^{n} Z_i
\end{equation*}
where $Z_i$ is a random variable for the potential loss of the $i$\textsuperscript{th} insurance policy, while $n$ is fixed. It is claimed that we can also express each $Z_i$ as
\begin{equation*}
  Z_i = I_i X_i
\end{equation*}
where $I_i$ is an indicator function given by
\begin{equation*}
  I_i(x) = \begin{cases}
    1 & \text{ if a claim occurs } \\
    0 & \text{ if there are no claims }
  \end{cases},
\end{equation*}
while $X_i$ is the size of the claim(s) for the $i$\textsuperscript{th} policy provided that there is a claim.

\newthought{One problem} that arises is: are $X_i$ and $I_i$ independent? They should be if we wish to define $Z_i$ in such a way. In fact, according to \\
\noindent\textcolor{base16-eighties-magenta}{\underline{Klugman et. al. in page 177}},

\begin{quotebox}{base16-eighties-magenta}
  Let $X_j = I_j B_j$, where $I_1, ..., I_n, B_1, ..., B_n$ are independent.
\end{quotebox}

where $X_j$ is our $Z_i$, $I_j$ is our $I_i$, and $B_j$ is our $X_i$.

\paragraph{$\S \; Z_i$ is not well-defined} Let us be explicit about the definitions of $I_i$ and $X_i$; we have
\begin{gather*}
  I_i = \mathbb{1}_{\{ Z_i > 0 \}} \\
  X_i = Z_i \mid Z_i > 0
\end{gather*}
However, we observe that such a defintion of $X_i$ is undefined on $Z_i = 0$. So the equation
\begin{equation*}
  Z_i = I_i X_i
\end{equation*}
is note well-defined.

\paragraph{$\S$ Independence of $I_i$ and $X_i$} We cannot actually tell if $I_i$ and $X_i$ are independent from each other, as it is equivalent to comparing apples with oranges\sidenote{In fact, I think this analogy fits our case perfectly so.}. Recall from our earlier courses, in particular STAT330, of the following notion:

\begin{defnnonum}[Probability Space]
\label{defn:probability_space}
  Let $\Omega$ be a sample space, and $\mathcal{F}$ a $\sigma$-algebra defined on $\Omega$\sidenote{Note that $(\Omega, \mathcal{F})$ is called a \hlnotea{measurable space}.}. A \hlnoteb{probability space} is the measurable space $(\Omega, \mathcal{F})$ with a \textcolor{base16-eighties-blue}{probability measure}, $f: \mathcal{F} \to [0, 1]$, defined on the space. We denote a probability space as $(\Omega, \mathcal{F}, f)$.
\end{defnnonum}

As mentioned in an earlier $\S$, $X_i$ is not defined on $Z_i = 0$, while $I_i$ is defined on $Z_i = 0$ \sidenote{\faHandPaperO \enspace This statement is hand-wavy.}. So the sample space for $X_i$ and $I_i$ are not the same, and so their probability measures are not the same as well. Therefore, \hlimpo{it is meaningless to ask if $X_i$ and $I_i$ are independent}.

Our best attempt at fixing this is probably the following: let
\begin{equation*}
  Z_i = \sum_{i=1}^{I_i} X_i,
\end{equation*}
which we can then have $X_i$ to be independent from $I_i$. However, interestingly so, this is a very similar approach to a \hyperref[defn:collective_risk_model]{Collective Risk Model}.

% section individual_risk_model_an_alternate_view (end)

\section{Coherent Risk Measure}%
\label{sec:coherent_risk_measure}
% section coherent_risk_measure

An excerpt from Klugman et. al. (2012)\cite{KlugmanPanjerWillmot2012}:

\begin{quotebox}{base16-eighties-magenta}
  The study of risk measures and their properties has been carried out by authors such as Wang. Specific desirable properties of risk measures were proposed as axioms in connection with risk pricing by Wang, Young, and Panjer and more generally in risk measurement by Artzer et. al. The Artzner paper introduced the concept of \textbf{coherence} and is considered to be the groundbreaking paper in risk measurement.
\end{quotebox}

Often, we use the function $\rho(X)$ to denote risk measures. One may think of $\rho(X)$ as \hlnotec{the amount of assets required to protect against adverse outcomes of the risk $X$}.

\begin{defn}[Coherent Risk Measure]\index{Coherent Risk Measure}
\label{defn:coherent_risk_measure}
  A \hlnoteb{coherent risk measure} is a risk measure $\rho(X)$ that has the following four properties for any two loss rvs $X$ and $Y$:
  \begin{enumerate}
    \item (\hlnotea{Subadditivity}) $\rho(X + Y) \leq \rho(X) + \rho(Y)$.
    \item (\hlnotea{Monotonicity}) If $X \leq Y$ for all possible outcomes, then $\rho(X) \leq \rho(Y)$.
    \item (\hlnotea{Positive homogeneity}) $\forall c \in \mathbb{R}_{> 0}$, $\rho(cX) = c\rho(X)$.
    \item (\hlnotea{Translation invariance}) $\forall c \in \mathbb{R}_{> 0}$, $\rho(X + c) = \rho(X) + c$
  \end{enumerate}
\end{defn}

\paragraph{Interpretation of the conditions}

\begin{itemize}
  \item \textbf{Subadditivity}
    \begin{itemize}
      \item the risk measure (and in return, the capital required to cover for it) for two risks combined will not be greater than for the risks to be treated separately;
      \item reflects the fact that there shuld be some diversification benefit from combining risks;
      \item this requirement is disputed: e.g. the merger of several small companies into a larger one exposes each of the small companies to the \hlnotea{reputational risks} of the others.
    \end{itemize}

  \item \textbf{Monotonicity}
    \begin{itemize}
      \item if one risk always has greater losses than the other under all circumstances\sidenote{Probabilistically, this means $P(X > Y) = 0$}, then the risk measure of the greater risk should always be greater than the other.
    \end{itemize}

  \item \textbf{Positive homogeneity}
    \begin{itemize}
      \item the risk measure is independent of the currency used to measure it;
      \item doubling the exposure to a particular risk requires double the capital, which is sensible as doubling provides no diversification.
    \end{itemize}

  \item \textbf{Translation invariance}
    \begin{itemize}
      \item there is no additional risk for an additional risk which has no additional uncertainty.
    \end{itemize}
\end{itemize}

% section coherent_risk_measure (end)

% chapter additional_material (end)

\backmatter

\pagestyle{plain}

\nobibliography*
\bibliography{references}

\input{listofsymbols.tex}

\printindex

\end{document}

