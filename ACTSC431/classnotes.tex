\documentclass[notoc,notitlepage]{tufte-book}
% \nonstopmode % uncomment to enable nonstopmode

\usepackage{classnotetitle}

\title{ACTSC 431 --- Loss Model I}
\author{Johnson Ng}
\subtitle{Classnotes for Fall 2018}
\credentials{BMath (Hons), Pure Mathematics major, Actuarial Science Minor}
\institution{University of Waterloo}

\input{latex-classnotes-preamble.tex}
\input{probnotation.tex}

\setsidenotefont{\color{light}\footnotesize}
\setmarginnotefont{\color{light}\footnotesize}

\DeclareMathOperator{\VaR}{VaR}
\DeclareMathOperator{\TVaR}{TVaR}
\DeclareMathOperator{\LER}{LER }

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\begin{fullwidth}
\tableofcontents
\end{fullwidth}

\newpage
\begin{fullwidth}
  \renewcommand{\listtheoremname}{\faBook\ \slshape List of Definitions}
  \listoftheorems[ignoreall,show={defn,margindefn}]
  \addcontentsline{toc}{chapter}{List of Definitions}
\end{fullwidth}

\newpage 
\begin{fullwidth}
  \renewcommand{\listtheoremname}{\faCoffee\ \slshape List of Theorems}
  \listoftheorems[ignoreall,show={axiom,lemma,thm,crly,propo}]
  \addcontentsline{toc}{chapter}{List of Theorems}
\end{fullwidth}

\chapter{Lecture 1 Sep 06}%
\label{chp:lecture_1_sep_06}
% chapter lecture_1_sep_06

\section{Introduction and Overview}%
\label{sec:introduction_and_overview}
% section introduction_and_overview

\paragraph{Course Objective} In Loss Model I, the focus of our study is to learn the basic methods which are used by insurers to quantify risk from mathematical/statistical models, in order for insurers to make various decisions\sidenote{e.g.\ setting premiums, control expenses, deciding for reinsurance, etc.}. By quantifying risk, it helps us monitor underlying risks so that not only are we aware of them, but also so that we can take actions or preventive measures against them.

Our main interest of this course is:
\begin{itemize}
  \item to quantify and seek protection against the loss of funds due either to \hlnoteb{too many claims} or \hlnoteb{a few large claims};
  \item to reduce adverse financial impact of random events that prevent the realization of reasonable expectations.
\end{itemize}

\newthought{The main model that shall be the focus} of this course is \hlnoteb{models for liability risk}.

\begin{defn}[Liability Risk]\index{Liability Risk}\label{defn:liability_risk}
  A \hlnoteb{liability risk} is a risk that insurance companies assume by selling insurance contracts.
\end{defn}

In particular, the liability that we shall focus on is \hlnotea{insurance claims}.\marginnote{Many of the models that we shall see later in the course are also applied for other types of risks, e.g.\ investment risk, credit risk, liquidity risk, and operational risk.}

\newthought{We are interested} in modelling the total amount of claims, i.e.\ the \hldefn{aggregate claim amount}, of a group fo insurance policies over a given period of time. In the actuarial literature, there are two main approaches that have been proposed to model the aggrement claim amount of an insurance portfolio, namely:
\begin{itemize}
  \item individual risk model;
  \item collective risk model.
\end{itemize}

\subsection{Individual Risk Model}%
\label{sub:individual_risk_model}
% subsection individual_risk_model

\begin{defn}[Individual Risk Model]\index{Individual Risk Model}\label{defn:individual_risk_model}
  In an \hlnoteb{individual risk model}, the aggregate claim is modeled by
  \begin{equation*}
    S = \sum_{i=1}^{n} Z_i
  \end{equation*}
  where $n$ is a \hlnotea{deterministic}\sidenote{i.e.\ fixed} integer that represents the \hlnotec{total number of insurance policies}, and $Z_i$ is a random variable for the \hlnotec{potential loss of the $i$\textsuperscript{th} insurance policy.}
\end{defn}

\begin{note}
  Since a policy may or may not incur a loss\sidenote{Since a claim may or may not be made!}, we have that
  \begin{equation*}
    P(Z_i = 0) > 0.
  \end{equation*}
  Thus, in an individual risk model, we may also express the aggregate claim amount as
  \begin{equation*}
    S = \sum_{i=1}^{n} X_i I_i
  \end{equation*}
  where $I_i$ is the indicator function about the claimant of policy $i$, while $X_i$ represents the size of the claim(s) for the $i$\textsuperscript{th} policy provided that there is a claim.\sidenote{\hlimpo{This is actually incorrect, despite being in the recommended textbook. See \cref{sec:individual_risk_model_an_alternate_view}.}}
\end{note}

However, in an individual risk model, according to Dhaene and Vyncke (2010)\cite{DhaeneVyncke2010},

\begin{quotebox}{be-red}{light}
  A third type of error that may arise when computing aggregate claims follows from the fact that the assumption of mutual independency of the individual claim amounts may be violated in practice.
\end{quotebox}

Due to complications such as this, the individual risk model will not be the focus of our studies.

% subsection individual_risk_model (end)

\subsection{Collective Risk Model}%
\label{sub:collective_risk_model}
% subsection collective_risk_model

\begin{defn}[Collective Risk Model]\index{Collective Risk Model}\label{defn:collective_risk_model}
  In a \hlnoteb{collective risk model}, the aggregate claim is modeled by
  \begin{equation*}
    S = \sum_{i=1}^{N} X_i, \end{equation*}
  where $N$ is a non-negative integer-valued random variable that denotes \hlnotec{the number of claims among a given set of policies}, while $X_i$ denotes the \hlnotec{size of the $i$\textsuperscript{th} policy.}
\end{defn}

\begin{note}
  In a collective risk model, we need to determine:
  \begin{itemize}
    \item the distribution of the total number of claims for the entire portfolio, i.e.\ the distribution of $N$; and
    \item the distribution of the loss amount per claim, i.e.\ the distribution of $X_i$.
  \end{itemize}
\end{note}

% subsection collective_risk_model (end)

In this course, the primary focus of our studies will be on \hlnotea{collective risk models}.

\paragraph{Terminologies} To end today's lecture, the following terminologies are introduced:

\begin{defn}[Severity Distribution]\index{Severity Distribution}\label{defn:severity_distribution}
  The \hlnoteb{severity distribution} is the distribution of the loss amount of the amount paid by the insurer on a given loss/claim.
\end{defn}

\begin{defn}[Frequency Distribution]\index{Frequency Distribution}\label{defn:frequency_distribution}
  The \hlnoteb{frequency distribution} is the distributino fo the number of losses/claims paid by the insurer over a given period of time.
\end{defn}

\begin{note}
  The frequency distribution is typically a discrete distribution.
\end{note}

\begin{defn}[Aggrement Payment / Loss]\index{Aggrement Payment}\index{Aggregate Loss}\label{defn:aggrement_payment_loss}
  The \hlnoteb{aggregate payment (loss)} is the total amout of all claim payments (losses) over a given period of time.
\end{defn}

\begin{note}
  There is a distinction between an aggregate payment and an aggregate loss, since an aggregate payment is ``essentially'' an aggregate loss after certain claim adjustments, such as deductibles, limits, and coinsurance.
\end{note}

% section introduction_and_overview (end)

% chapter lecture_1_sep_06 (end)

\chapter{Lecture 2 Sep 11th}%
\label{chp:lecture_2_sep_11th}
% chapter lecture_2_sep_11th

\section{Review of Probability Theory}%
\label{sec:review_of_probability_theory}
% section review_of_probability_theory

Firstly, we shall review the definition of a random variable.

\begin{defn}[Random Variable]\index{Random Variable}\label{defn:random_variable}
  Let $\Omega$ be a sample space and $\mathcal{F}$ its $\sigma$-algebra\sidenote{For definitions of $\Omega$ and $\mathcal{F}$, see notes on STAT330.}. A \hlnoteb{random variable} (rv) $X : \Omega \to (\Omega, \mathcal{F})$ is a function from a possible set of outcomes to a measurable space $(\Omega, \mathcal{F})$. Within the context of our interest, $X$ is real-valued, i.e. $(\Omega, \mathcal{F}) = \mathbb{R}$.
\end{defn}

\subsection{Discrete Random Variables}%
\label{sub:discrete_random_variables}
% subsection discrete_random_variables

\begin{defn}[Discrete Random Variable]\index{Discrete Random Variable}\label{defn:discrete_random_variable}
  A \hlnoteb{discrete random variable} (drv) is an rv $X$ that takes only countable (finite) real values.
\end{defn}

\begin{note}
  Let $X$ be a drv.
  \begin{itemize}
    \item The \hlnotea{probability mass function} (pmf) of $X$ is: for $i \in \mathbb{N}$,
      \begin{equation*}
        p(x_i) = P(X = x_i)
      \end{equation*}

    \item The \hlnotea{cumulative distribution function} (cdf) of $X$ is
      \begin{equation*}
        F(x) = P(X \leq x) = \sum_{x_i \leq x} p(x_i).
      \end{equation*}

    \item The $k$th \hldefn{moment} of $X$ is\sidenote{This implicitly uses the \href{https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician}{Law of the Unconcious Statistician}.}
      \begin{equation*}
        E[X^k] = \sum_{i \in \mathbb{N}} x_i^k p(x_i)
      \end{equation*}
      if $E[X^k]$ is finite.

    \item Some commonly seen/introduced discrete distributions are: Poisson, Binomial, Negative Binomial
  \end{itemize}
\end{note}

\begin{eg}
  Let $X$ take values from $\{x_1, x_2, x_3, x_4\}$, and
  \begin{equation*}
    p(x_i) = P(X = x_i) \text{ for } i = 1, 2, 3, 4.
  \end{equation*}
  The cdf of $X$ is\marginnote{
    It is recommended to visualize the cdf first before putting it down in pencil.
    \resizebox{4.5cm}{!}{
    \begin{tikzpicture}
      % axes
      \draw[->] (0, 0) -- (0, 5) node[above] {$F(x)$};
      \draw[->] (0, 0) -- (5, 0) node[right] {$x$};
      \node[below=1.5mm] at (1, 0) {$x_1$};
      \node[below=1.5mm] at (2, 0) {$x_2$};
      \node[below=1.5mm] at (3, 0) {$x_3$};
      \node[below=1.5mm] at (4, 0) {$x_4$};

      % cdf
      \draw[-,line width=0.5mm] (0, 0) -- (1, 0);
      \draw[-,line width=0.5mm] (1, 1) -- (2, 1);
      \draw[-,line width=0.5mm] (2, 2) -- (3, 2);
      \draw[-,line width=0.5mm] (3, 3) -- (4, 3);
      \draw[->,line width=0.5mm] (4, 4) -- (5, 4);
      \node[circle,inner sep=2pt,draw] at (1, 0) {};
      \node[circle,inner sep=2pt,draw] at (2, 1) {};
      \node[circle,inner sep=2pt,draw] at (3, 2) {};
      \node[circle,inner sep=2pt,draw] at (4, 3) {};
      \node[circle,inner sep=2pt,fill] at (1, 1) {};
      \node[circle,inner sep=2pt,fill] at (2, 2) {};
      \node[circle,inner sep=2pt,fill] at (3, 3) {};
      \node[circle,inner sep=2pt,fill] at (4, 4) {};
      \draw[dotted] (4, 4) -- (0, 4) node[left] {$1$};

      % jumps
      \draw[dotted] (1, 1) -- (1, 0) node[midway,left] {$p(x_1)$};
      \draw[dotted] (2, 2) -- (2, 1) node[midway,left] {$p(x_2)$};
      \draw[dotted] (3, 3) -- (3, 2) node[midway,left] {$p(x_3)$};
      \draw[dotted] (4, 4) -- (4, 3) node[midway,left] {$p(x_4)$};
    \end{tikzpicture}
    }
  }
  \begin{equation*}
    F(x) = \begin{cases}
      0               & x < x_1 \\
      p(x_1)          & x_1 \leq x < x_2 \\
      p(x_1) + p(x_2) & x_2 \leq x < x_3 \\
      1 - p(x_4)      & x_3 \leq x < x_4 \\
      1               & x \geq x_4
    \end{cases}
  \end{equation*}
\end{eg}

\begin{note}
  \begin{itemize}
    \item It is important that we stress the need for showing \hlnotea{right continuity} in the graph.
    \item Note that the cdf always sums to $1$.
    \item The ``\hlnotea{jumps}'' at $x_i$ correspond to $p(x_i)$, for $i = 1, 2 ,3, 4$.
  \end{itemize}
\end{note}

\begin{defn}[Probability Generating Function]\index{Probability Generating Function}\label{defn:probability_generating_function}
  Suppose a drv $X$ only takes \hlimpo{non-negative integer values}. The \hlnoteb{probability generating function} (pgf) of $X$ is defined as
  \begin{equation*}
    G(z) = E\left[ z^X \right] = \sum_{k=1}^{\infty} z^k p(k)
  \end{equation*}
  where we note that if $\max X = n$, then $p(m) = 0$ for all $m > n$.
\end{defn}

\begin{note}
  \begin{itemize}
    \item The pgf uniquely identifies the distribution of the drv\sidenote{\faHandPaperO\ This was given as is without proof, and I cannot find any resources that proves this.}.
    \item To get the probability for $k \in \{0, 1, 2, \ldots\}$, we simply need to do
      \begin{equation*}
        p(k) = \frac{1}{k!} G^{(k)}(x) \at{x = 0}{}.
      \end{equation*}
  \end{itemize}
\end{note}

\begin{eg}[Lecture Slides: Example 1]
  Consider a drv $X$ with pmf
  \begin{equation*}
    p(x) = P(X = x) = \begin{cases}
      0.5 & x = 0 \\
      0.4 & x = 1 \\
      0.1 & x = 2
    \end{cases}
  \end{equation*}
  Its cdf is\marginnote{
    \resizebox{4.5cm}{!}{
    \begin{tikzpicture}
      % axes
      \draw[->] (0, 0) -- (0, 5) node[above] {$F(x)$};
      \draw[->] (0, 0) -- (3.5, 0) node[right] {$x$};
      \node[below=1.5mm] at (1, 0) {$0$};
      \node[below=1.5mm] at (2, 0) {$1$};
      \node[below=1.5mm] at (3, 0) {$2$};

      % cdf
      \draw[-,line width=0.5mm] (0, 2) -- (1, 2);
      \draw[-,line width=0.5mm] (1, 3.6) -- (2, 3.6);
      \draw[->,line width=0.5mm] (2, 4) -- (3.5, 4);
      \node[circle,inner sep=2pt,draw] at (1, 2) {};
      \node[circle,inner sep=2pt,draw] at (2, 3.6) {};
      \node[circle,inner sep=2pt,fill] at (0, 2) {};
      \node[circle,inner sep=2pt,fill] at (1, 3.6) {};
      \node[circle,inner sep=2pt,fill] at (2, 4) {};
      \draw[dotted] (3, 4) -- (0, 4) node[left] {$1$};

      % jumps
      \draw[dotted] (0, 0) -- (0, 2) node[midway,left] {$0.5$};
      \draw[dotted] (1, 2) -- (1, 3.6) node[midway,left] {$0.4$};
      \draw[dotted] (2, 3.6) -- (2, 4) node[midway,left] {$0.1$};
    \end{tikzpicture}
    }
  }
  \begin{equation*}
    F(x) = P(X \leq x) \begin{cases}
      0   & x < 0 \\
      0.5 & 0 \leq x < 1 \\
      0.9 & 1 \leq x < 2 \\
      1   & x \geq 2
    \end{cases}
  \end{equation*}
  and its pgf is
  \begin{equation*}
    G(z) = E\left[ z^X \right] = 0.5 + 0.4z + 0.1z^2.
  \end{equation*}
\end{eg}

% subsection discrete_random_variables (end)

\subsection{Continuous Random Variables}%
\label{sub:continuous_random_variables}
% subsection continuous_random_variables

\begin{defn}[Continuous Random Variable]\index{Continuous Random Variable}\label{defn:continuous_random_variable}
  A \hlnoteb{continuous random variable} (crv) takes on a continuum of values.
\end{defn}

\begin{note}
  Let $X$ be a crv.
  \begin{itemize}
    \item $\exists f : X \to \mathbb{R}$ called a \hlnotea{probability density function} (pdf) such that its cdf is
      \begin{equation*}
        F(x) = \int_{-\infty}^{x} f(y) \dif{y},
      \end{equation*}
      and consequently by the \hlnotea{Fundamental Theorem of Calculus}, we have
      \begin{equation*}
        f(x) = F'(x).
      \end{equation*}

    \item The $k$th moment of $X$ is
      \begin{equation*}
        E[X^k] = \int_{x} x^k f(x) \dif{x} 
      \end{equation*}
      so long that $E[X^k]$ is defined.
      
    \item Some commonly introduced distributions are: Uniform, Exponential, Gamma, Weibull, and Normal.
  \end{itemize}
\end{note}

\begin{defn}[Moment Generating Function]\index{Moment Generating Function}\label{defn:moment_generating_function}
Let $X$ be an rv. The \hlnoteb{moment generating function} (mgf)\marginnote{The mgf is also defined for drvs.} of $X$ is, for $t \in \mathbb{R}$ (appropriately so),
  \begin{equation*}
    M_X(t) = E\left[e^{tX}\right] = \int_{x} e^{tx} f(x) \dif{x}
  \end{equation*}
  provided that the integral is well-defined.
\end{defn}

\begin{note}
  \begin{itemize}
    \item The mgf uniquely determines the distribution of its rv\sidenote{\faHandPaperO\ This shall, also, not be proven in this course.}

    \item With the mgf, we can obtain the $k$th moment of an rv $X$ by
      \begin{equation*}
        E\left[X^k\right] = \frac{d^k}{dt^k} M_X(t) \at{t = 0}{}
      \end{equation*}
  \end{itemize}
\end{note}

\begin{eg}[Lecture Notes: Example 2]
  Consider an exponential rv $X$ with pdf\sidenote{When not explicitly stated, it shall be assumed that domains at which we did not specify $x$ shall have probability $0$.}
  \begin{equation*}
    f(x) = 0.1e^{-0.1x}, \; x > 0.
  \end{equation*}
  Its cdf is
  \begin{equation*}
    F(x) = \int_{-\infty}^{x} f(y) \dif{y} = \begin{cases}
      1 - e^{-0.1 x} & x \geq 0 \\
      0              & \text{otherwise}
    \end{cases}
  \end{equation*}
  and its mgf is
  \begin{align*}
    M_X(t) &= E\left[ e^{tX} \right] = \int_{0}^{\infty} e^{tx} 0.1 e^{-0.1x} \dif{x} \\
           &= 0.1 \int_{0}^{\infty} e^{( t - 0.1 )x} \dif{x} \\
           &= \frac{0.1}{0.1 - t}, \enspace t < 0.1,
  \end{align*}
  where we note that we must have $t < 0.1$, for otherwise the value of the exponent would render the integral undefined.
\end{eg}

\begin{defn}[Hazard Rate Function]\index{Hazard Rate Function}\label{defn:hazard_rate_function}
  For a crv $X$, the \hlnoteb{hazard rate function} (aka \hldefn{failure rate}) of $X$ is defined as
  \begin{equation*}
    h(x) = \frac{f(x)}{\bar{F}(x)} = - \frac{d}{dx} \ln \bar{F}(x),
  \end{equation*}
  where $\bar{F}(x) = 1 - F(x)$ is the \hlnotea{survival function}\sidenote{You should be familiar with this if you have studied for Exam P.}
\end{defn}

\begin{note}
  \begin{itemize}
    \item We may also express the survival function in terms of the hazard rate by
      \begin{equation*}
        \bar{F}(x) = e^{- \int_{-\infty}^{x} h(y) \dif{y}}.
      \end{equation*}

    \item In terms of limits, we can express the hazard rate function, for small enough $\delta > 0$, as
      \begin{align*}
        h(x) &= \frac{f(x)}{\bar{F}(x)} = \frac{F'(x)}{\bar{F}(x)} \\
             &\approx \frac{F(x + \delta) - F(x)}{\delta \bar{F}(x)} \\
             &= \frac{P(x < X \leq x + \delta)}{\delta F(X > x)} \\
             &= \frac{1}{\delta} P(x < X \leq x + \delta \mid X > x).
      \end{align*}
      We can make sense of this expression by recalling the notion of the probability of survival from Exam MLC\sidenote{This also tells us that the hazard rate gets its name from life insurance.}, where if a life has survived over $x$, the hazard rate is the probability that the life does not survive beyond another $\delta$ \sidenote{From the perspective of life insurance, the greater the probability, the more likely the claim is going to happen.}.
  \end{itemize}
\end{note}

% subsection continuous_random_variables (end)

% section review_of_probability_theory (end)

% chapter lecture_2_sep_11th (end)

\chapter{Lecture 3 Sep 13th}%
\label{chp:lecture_3_sep_13th}
% chapter lecture_3_sep_13th

\section{Review of Probability Theory (Continued)}%
\label{sec:review_of_probability_theory_continued}
% section review_of_probability_theory_continued

\subsection{Continuous Random Variables (Continued)}%
\label{sub:continuous_random_variables_continued}
% subsection continuous_random_variables_continued

\begin{eg}[Lecture Notes: Example 3 --- Hazard Rate of Weibull Distribution]\label{eg:weibull_hazard_rate}
  Suppose $X \sim \Wei(\theta, \tau)$ with pdf
  \begin{equation*}
    f(x) = \frac{\tau {\left( \frac{x}{\theta} \right)}^\tau e^{-{\left( \frac{x}{\theta} \right)}^\tau}}{x}, \quad x > 0,
  \end{equation*}
  where $\theta, \tau > 0$. Find its hazard rate function.
\end{eg}

\begin{solution}
  We first require the survival function\sidenote{\hlnotea{Weibull Survival Function}\label{note:weibull_survival_function}}:
  \begin{align*}
    \bar{F}(x) &= \int_{x}^{\infty} \frac{1}{y} \tau {\left( \frac{y}{\theta} \right)}^\tau e^{-{\left( \frac{y}{\theta} \right)}^\tau} \dif{y} \\
               &= \int_{\frac{x}{\theta}}^{\infty} \frac{1}{u} \tau u^\tau e^{-u^\tau} \dif{u} \qquad \text{ where } u = \frac{y}{\theta} \\
               &= \int_{\frac{x}{\theta}}^{\infty} \tau u^{\tau - 1} e^{-u^\tau} \dif{u} \\
               &= -e^{-u^\tau} \at{\frac{x}{\theta}}{\infty} = e^{-{\left( \frac{x}{\theta} \right)}^\tau}
  \end{align*}
  The hazard rate is therefore
  \begin{equation*}
    h(x) = \frac{f(x)}{\bar{F}(x)} = \frac{\tau}{x} {\left( \frac{x}{\theta} \right)}^\tau
  \end{equation*}
\end{solution}

% subsection continuous_random_variables_continued (end)

\subsection{Mixed Random Variable}%
\label{sub:mixed_random_variable}
% subsection mixed_random_variable

\begin{defn}[Mixed Random Variable]\index{Mixed Random Variable}\label{defn:mixed_random_variable}
  We call $X$ a \hlnoteb{mixed random variable} (mixed rv) if it has both discrete and continuous components.
\end{defn}

\begin{note}
  \begin{itemize}
    \item Mixed rvs are important in modeling insurance claims, e.g., the loss amount is usually a continuous random variable with a probability mass at $0$.
  \end{itemize}
\end{note}

The following is a type of mixed random variable:

\begin{defn}[Deductibles]\index{Deductibles}\label{defn:deductibles}
  Let $X$ be an rv and $d$ be a fixed value.
  \begin{equation*}
    {[ X - d ]}_+ = \begin{cases}
      X - d & x \geq d \\
      0     & \text{ otherwise }
    \end{cases}
  \end{equation*}
\end{defn}

\begin{note}
  If $X$ be an rv and $d$ a fixed value, the deductible ${[X - d]}_+$ has a mass point at $0$ since
  \begin{equation*}
    P( {[ X - d ]}_+ = 0 ) = P(X < d) > 0
  \end{equation*}
\end{note}

\begin{note}
  Let $\{ x_1, x_2, \ldots \}$ be a sequence of real numbers in an increasing order. Suppose $X$ is a rv that takes on values on the real, and has a \hlnotea{density function} $f$ on each interval $(x_i, x_{i + 1})$, and has \hlnotea{discrete mass points} at the boundaries of these intervals, i.e.\marginnote{In other words, we treat the discrete and continuous part of a mixed rv separately.}
  \begin{equation*}
    P(X = x_i) = p(x_i) > 0 \quad i \in \mathbb{N}.
  \end{equation*}
  Since $X$ is an rv, it must be the case that
  \begin{equation*}
    \sum_{i \in \mathbb{N}} p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} f(x) \dif{x}  = 1.
  \end{equation*}
  The cdf of a mixed rv $X$ is
  \begin{equation*}
    F(x) = P(X \leq x) = \sum_{i \in \mathbb{N}} p(x_i) \mathbb{1}_{\{x_i \leq x\}} + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} f(y) \mathbb{1}_{\{ y \leq x \} } \dif{y}.
  \end{equation*}
  The $k$th moment of $X$ is
  \begin{equation*}
    E\left[X^k\right] = \sum_{i \in \mathbb{N}} {(x_i)}^k p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} x^k f(x) \dif{x}.
  \end{equation*}
  The mgf of $X$ is
  \begin{equation*}
    M_X(t) = E\left[ e^{tX} \right] = \sum_{i \in \mathbb{N}} e^{tx_i} p(x_i) + \sum_{i \in \mathbb{N}} \int_{x_i}^{x_{i + 1}} e^{tx} f(x) \dif{x}.
  \end{equation*}
\end{note}

\begin{eg}[Lecture Notes: Example 4]
  Assume a claim amount of an insurance policy is modeled by a non-negative rv $X$ which has probability mass of $p$ and $0$, and otherwise continuous with a pdf $f$ over $(0, \infty)$. Find its cdf, $k$th moment, and mgf.
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      p + \int_{0}^{x} f(y) \dif{y} & x \geq 0 \\
      0                             & \text{ otherwise }
    \end{cases}
  \end{equation*}
  The $k$th moment of $X$ is
  \begin{equation*}
    E\left[ X^k \right] = \int_{0}^{\infty} x^k f(x) \dif{x}.
  \end{equation*}
  The mgf of $X$ is
  \begin{equation*}
    M_X(t) = p + \int_{0}^{\infty} e^{tx} f(x) \dif{x}.
  \end{equation*}
\end{solution}

% subsection mixed_random_variable (end)

% section review_of_probability_theory_continued (end)

\section{Distributional Quantities and Risk Measures}%
\label{sec:distributional_quantities_and_risk_measures}
% section distributional_quantities_and_risk_measures

\newthought{This chapter} introduces us to some \hlnotea{distributional quantities} for a given rv $X$. These distributional quantities are informative values to describe the characteristics of a risk.

\subsection{Distributional Quantities}%
\label{sub:distributional_quantities}
% subsection distributional_quantities

\begin{defn}[Central Moment]\index{Central Moment}\label{defn:central_moment}
  The \hlnoteb{$k$th central moment} of an rv $X$ is defined as
  \begin{equation*}
    E\left[ {(X - E(X))}^k \right].
  \end{equation*}
\end{defn}

\begin{note}
  The second central moment is the \hldefn{variance}. The square root of the variance is the \hldefn{standard deviation}.
\end{note}

\begin{eg}[Lecture Notes: Example 5]
  Consider an rv $Y = \begin{cases} Y_1 & U = 1 \\ Y_2 & U = 2 \end{cases} \; $\sidenote{This notation is just syntatic sugar for saying $Y_1 = Y \mid ( U = 1 )$ and $Y_2 = Y \mid ( U = 2 )$.}, where $Y_1 = 0$, $Y_2 \sim \Exp(10)$, and $P(U = 1) = P(U = 2) = 0.5$.
  \begin{enumerate}
    \item Find the cdf of $Y$.
    \item Find the mean and variance of $Y$.
    \item Let $Z = \frac{1}{2} Y_1 + \frac{1}{2} Y_2$. Does $Z$ have the same distribution as $Y$? Answer this by solving the mean and variance of $Z$.
  \end{enumerate}
\end{eg}

\begin{solution}
  \begin{enumerate}
    \item Note that
      \begin{equation*}
        F(y) = P( Y_1 \leq y \mid U = 1 ) P(U = 1) + P( Y_2 \leq y \mid U = 2 ) P(U = 2).
      \end{equation*}
      Observe that
      \begin{equation*}
        P(Y_1 \leq y \mid U = 1) = \begin{cases}
          1 & y \geq 0 \\
          0 & y < 0
        \end{cases}
      \end{equation*}
      and
      \begin{equation*}
        P(Y_2 \leq y \mid U = 2) = \begin{cases}
          1 - e^{-10y} & y \geq 0 \\
          0            & y < 0
        \end{cases}
      \end{equation*}
      Therefore
      \begin{equation*}
        F(y) = \begin{cases}
          1 - \frac{1}{2} e^{-10 y} & y \geq 0 \\
          0                         & y < 0
        \end{cases}
      \end{equation*}

    \item The mean of $Y$ is
      \begin{equation*}
        E(Y) = E(Y \mid U = 1) P(U = 1) + E(Y \mid U = 2) P(U = 2) = 10 \cdot \frac{1}{2} = 5.
      \end{equation*}
      To calculate the variance of $Y$, we require
      \begin{align*}
        E\left[Y^2\right] &= E\left[Y^2 \mid U = 1\right] P(U = 1) + E\left[Y^2 \mid U = 2\right] P(U = 2) \\
                          &= ( \Var(Y_2) + E{(Y_2)}^2 ) \cdot \frac{1}{2} = 100.
      \end{align*}
      Therefore
      \begin{equation*}
        \Var(Y) = 100 - 5^2 = 75.
      \end{equation*}

    \item The mean of $Z$ is
      \begin{equation*}
        E[Z] = E[ \frac{1}{2} Y_1 + \frac{1}{2} Y_2 ] = 5.
      \end{equation*}
      The variance of $Z$ is
      \begin{equation*}
        \Var(Z) = \frac{1}{4} \Var(Y_1) + \frac{1}{4} \Var(Y_2) = 25.
      \end{equation*}
      Therefore, $Z$ does not have the same distribution as $Y$.
  \end{enumerate}
\end{solution}

\begin{defn}[Quantiles]\index{Quantiles}\label{defn:quantiles}
  The \hlnoteb{$100p\%$ quantile} (or \hldefn{percentile}) of an rv $X$ is a set $\pi_p$ such that\marginnote{This definition may also be presented as: any number $\pi_p$ such that
  \begin{equation*}
    P(X < \pi_p) \leq p \leq P(X \leq \pi_p).
  \end{equation*}}
  \begin{equation*}
    \pi_p = \{ x \in X \mid P(X < x) \leq p \leq P(X \leq x) \}.
  \end{equation*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item If $X$ is a continuous random variable, we have that $P(X < \pi_p) = P(X \leq \pi_p)$ and so we have to define the quantile as
      \begin{equation*}
        \pi_p = F^{-1} (p)
      \end{equation*}
      where $F^{-1}$ is the inverse function of $F$, the cdf of $X$.

    \item A quantile \hlimpo{can be a set of numbers}.
    \item $\pi_{0.5}$ is called the \hldefn{median} of $X$.
  \end{itemize}
\end{note}

\marginnote{Graphical method to interpret this notion will be included.}

\begin{eg}[Lecture Notes: Example 1]
  Find the $100p\%$ quantile of the loss distribution $F(x) = 1 - e^{-\frac{x}{\theta}}$, $x > 0$.
\end{eg}

\begin{solution}
  Note that $F$ is the cdf of an exponential distribution, which is a continuous distribution. Therefore,
  \begin{equation*}
    F(\pi_p) = 1 - e^{-\frac{\pi_p}{\theta}} = p \implies \pi_p = - \theta \ln (1 - p).
  \end{equation*}
\end{solution}

\begin{eg}[Lecture Notes: Example 2]
  Find the median $\pi_{0.5}$ for the following cdf
  \begin{equation*}
    F(x) = \begin{cases}
      0                                & x < 0 \\
      0.6 + 0.4 (1 - e^{-\frac{x}{3}}) & x \geq 0
    \end{cases}
  \end{equation*}
\end{eg}

\begin{solution}
  Since $F(0) = 0.6$ and $F$ is an increasing function, we have that $F(x) = 0$ for all $x < 0$. Therefore
  \begin{equation*}
    \pi_{0.5} = 0.
  \end{equation*}
\end{solution}

\begin{eg}[Lecture Notes: Example 3]
  Find the median $\pi_{0.5}$ for a loss $X$ with pmf
  \begin{equation*}
    p(0) = 0.25, \, p(1) = 0.25, \, p(2) = 0.5.
  \end{equation*}
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      0    & x < 0 \\
      0.25 & 0 \leq x < 1 \\
      0.5  & 1 \leq x < 2 \\
      1    & x \geq 2
    \end{cases}
  \end{equation*}
  since $F(x) = 0.5$ when $1 \leq x < 2$, we have that
  \begin{equation*}
    \pi_{0.5} = [1, 2].
  \end{equation*}
\end{solution}

% subsection distributional_quantities (end)

% section distributional_quantities_and_risk_measures (end)

% chapter lecture_3_sep_13th (end)

\chapter{Lecture 4 Sep 18th}%
\label{chp:lecture_4_sep_18th}
% chapter lecture_4_sep_18th

\section{Distributional Quantities and Risk Measures (Continued)}%
\label{sec:distributional_quantities_and_risk_measures_continued}
% section distributional_quantities_and_risk_measures_continued

\subsection{Risk Measures}%
\label{sub:risk_measures}
% subsection risk_measures

\begin{defn}[Risk Measure]\index{Risk Measure}\label{defn:risk_measure}
  A \hlnoteb{risk measure} is a mapping from the loss rv to the real line $\mathbb{R}$.
\end{defn}

Klugman, Panjer \& Wilmot (2012)~\cite{KlugmanPanjerWillmot2012} on risk measure:

\begin{quotebox}{be-yellow}{light}
  The level of exposure to risk is often described by one number, or at least a small set of numbers. These numbers are necessarily functions of the model and are often called ‘key risk indicators’. Such key risk indicators indicate to risk managers the degree to which the company is subject to particular aspects of risk.
\end{quotebox}

To ensure its solvency, insurers will have to charge on these risks, i.e.\ we have to \hlnotea{price these exposures to risks}.

\begin{defn}[Premium Principle]\index{Premium Principle}\label{defn:premium_principle}
  A \hlnoteb{premium principle} (or \hldefn{insurance pricing}) is a rule for assigning a premium to an insurance risk.
\end{defn}

\begin{note}
  The following are some of the common principles used by insurers:
  \begin{itemize}
    \item \hldefn{Expectation Principle}
      \begin{equation*}
        \Pi(X) = ( 1 + \theta ) E(X), \quad \theta > 0
      \end{equation*}
    \item \hldefn{Standard Deviation Principle}
      \begin{equation*}
        \Pi(X) = E(X) + \theta \sqrt{\Var(X)}, \quad \theta > 0
      \end{equation*}
    \item \hldefn{Dutch Principle}
      \begin{equation*}
        \Pi(X) = E(X) + \theta E( {[ X - E(X) ]}_+ ), \quad \theta > 0
      \end{equation*}
  \end{itemize}
\end{note}

One particular measure is known as the \hlnotea{Value-at-Risk} (VaR).

\subsubsection{Value-At-Risk}\label{ssub:Value-At-Risk}

\begin{defn}[Value-at-Risk (VaR)]\index{Value-at-Risk}\index{VaR}\label{defn:value_at_risk}
The \hlnoteb{Value-at-Risk (VaR)} is a \hlnotea{quantile} of the distribution of aggregate losses, i.e.\ the $VaR$ of a risk $X$ at the $100\%p$ level is defined as\sidenote{I must find out why we define using $\inf$ instead of $\min$ (see following remark), and I will not take ``safe definition'' as an answer without full justification.}
  \begin{align*}
    \VaR_p (X) &= \inf \{ x \in \mathbb{R} : P (X > x) \leq 1 - p \} \\
               &= \inf \{ x \in \mathbb{R} : P (X \leq x) \geq p \}.
  \end{align*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item $\VaR$ is often called a \hldefn{quantile risk measure}.
    \item $\VaR$ is the standard risk measure used to evaluate exposure to risks.
    \item $\VaR$ measures the amount of capital required by the insurer to remain solvent, with high certainty, in the face of large claims.
    \item In practice, $p$ is generally high: $99.95\%$ or as low as $95\%$.
  \end{itemize}
\end{note}

\begin{remark}
  Observe that\marginnote{This remark basically points out that the left endpoint of the interval $B$ is always included, which should be quite clear by right-continuity of $F$.}
  \begin{equation*}
    B = \{ x \in \mathbb{R} \mid F_X(x) \geq p \} = (A, \infty) \text{ or } [A, \infty)
  \end{equation*}
  for some $A \in \mathbb{R}$, since $F$ is an increasing function. Now let $x_0 \in B$ such that
  \begin{equation*}
    F(x_0) = P(X \leq x_0) \geq p \quad \land \quad F(x_0-) = P(X < x_0) \leq p,
  \end{equation*}
  i.e.\ it is not necessary that $P(X = x_0) = p$ (see the two example graphs on the margin).
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (4, 0) node[right] {$x$};
      \draw[->] (0, 0) -- (0, 4) node[above] {$F(x)$};
      \draw (0, 1) -- (1, 1);
      \draw (1, 2) -- (2, 2);
      \draw[->] (2, 3) -- (4, 3);
      \node[circle,fill,inner sep=1pt] at (1, 2) {};
      \node[circle,fill,inner sep=1pt] at (2, 3) {};
      \node[circle,draw,inner sep=1pt] at (1, 1) {};
      \node[circle,draw,inner sep=1pt] at (2, 2) {};
      \draw[dotted] (4, 1.5) -- (0, 1.5) node[left] {$p$};
      \draw[dotted] (1, 2) -- (1, 0) node[below] {$x_0$};
    \end{tikzpicture}
    \caption{Discrete cdf}
  \end{marginfigure}
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (0, 0) -- (4, 0) node[right] {$x$};
      \draw[->] (0, 0) -- (0, 4) node[above] {$F(x)$};
      \draw[->,smooth,domain=0:4] plot (\x,{sqrt(\x)});
      \draw[dotted] (4, 1.5) -- (0, 1.5) node[left] {$p$};
      \draw[dotted] (2.25, 2) -- (2.25, 0) node[below] {$x_0$};
      \node[circle,fill,inner sep=1pt] at (2.25,1.5) {};
    \end{tikzpicture}
    \caption{Continuous cdf}
  \end{marginfigure}
  Let ${\{ x_n \}}_{n \in \mathbb{N}}$ be a decreasing sequence of points on $\mathbb{R}$ such that $x_n \to x_0$ as $n \to \infty$. Since $F$ is right-continuous, we have that $F(x_n) \to F(x_0)$ as $n \to \infty$. Therefore,
  \begin{equation*}
    B = [ x_0 , \infty )
  \end{equation*}\marginnote{The lecturer asserts that we can really define $\VaR$ using $\min$ instead of $\inf$, but even with this, I am not completely satisfied or convinced.}
  This justifies the definition of $\pi_p$.
\end{remark}

\begin{note}
  \begin{itemize}
    \item Note that by definition, we have
      \begin{equation*}
        P(X < \pi_p) \leq p \leq P(X \leq \pi_p)
      \end{equation*}
    \item If $X$ is a crv whose cdf is strictly increasing, i.e.\ no constant points, then
      \begin{equation*}
        \pi_p = F^{-1}(p)
      \end{equation*}
      since $P(X < \pi_p) = P(X \leq \pi_p)$.
  \end{itemize}
\end{note}

\begin{warning}[Shortcomings of $\VaR$]
  \begin{itemize}
    \item $\VaR$ cannot tell us the size of the potential loss in the $100(1 - p)\%$ cases, making it difficult for us to prepare the right amount in order to safeguard against insolvency.
    \item $\VaR$ actually fails to satisfy properties to be a \hlnotea{coherent risk measure}\sidenote{See \cref{sec:coherent_risk_measure}.}, for example, \hlnotea{subadditivity}.
    \item $\VaR$ is extensively used in financial risk management of trading risk over a fixed (usually short) time period, which are usually normally distributed, and $\VaR$ satisfies all coherency requirements.
    \item In insurance losses, instead of normal distributions, in general, skewed distributions are used, and in this cases, $\VaR$ is flawed as it lacks subadditivity.
  \end{itemize}
\end{warning}

\begin{eg}\label{eg:varp_pareto}
  Suppose that $X$ has a Pareto distribution with cdf
  \begin{equation*}
    F(x) = 1 - {\left( \frac{\theta}{x + \theta} \right)}^\alpha , \quad x > 0
  \end{equation*}
  where $\alpha, \theta > 0$. Find $\VaR_p(X)$.
\end{eg}

\begin{solution}
  Since $F$ is continuous and strictly increasing, we have that
  \begin{equation*}
    \pi_p = F^{-1}(p) = \theta \left[ {(1 - p)}^{-\frac{1}{\alpha}} - 1 \right]
  \end{equation*}
\end{solution}

\begin{eg}
  Find $\VaR_{0.95}(X)$, $\VaR_{0.5}(X)$, and $\VaR_{0.3}(X)$ for a random loss with pmf
  \begin{equation*}
    p(0) = 0.25, \, p(1) = 0.25, \, \text{ and } p(2) = 0.5.
  \end{equation*}
\end{eg}

\begin{solution}
  Note that the cdf of $X$ is
  \begin{equation*}
    F(x) = \begin{cases}
      0    & x < 0 \\
      0.25 & 0 \leq x < 1 \\
      0.5  & 1 \leq x < 2 \\
      1    & x \geq 2
    \end{cases}.
  \end{equation*}
  Therefore,
  \begin{equation*}
    \VaR_{0.95}(X) = 2, \, \VaR_{0.5}(X) = 1, \, \text{ and } \VaR_{0.3}(X) = 1.
  \end{equation*}
\end{solution}

\subsubsection{Tail-Value-at-Risk}\label{ssub:Tail-Value-at-Risk}

To compensate for the weakness of $\VaR$ at giving us the size of the loss $X$ of which we cannot measure, we use the \hlnotea{Tail-Value-at-Risk}.

\begin{defn}[Tail-Value-at-Risk (TVaR)]\index{Tail-Value-at-Risk}\label{defn:tail_value_at_risk}
Let $X$ be an rv. The \hlnoteb{Tail-Value-at-Risk (TVaR)} of $X$ at the $100p\%$ level, denoted as $\TVaR_p(X)$, is defined as the average of all $\VaR$ values above the level $p$, and expressed as\marginnote{TVaR also has the following names, used by different regions:
\begin{itemize}
  \item \hlnotea{Conditional Tail Expectation} (CTE) --- NA
  \item \hlnotea{Tail Conditional Expectation} (TCE)
  \item \hlnotea{Expected Shortfall} (ES) --- EU
\end{itemize}}
  \begin{equation*}
    \TVaR_p(X) = \frac{1}{1 - p} \int_{p}^{1} \VaR_\alpha(X) \dif{\alpha} = \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha}
  \end{equation*}
\end{defn}

\begin{remark}
  By considering the average of $\VaR$ from $p$'s going up to $1$, we take into account even the extreme cases of which $\VaR$ fails to account for.
\end{remark}

Perhaps a clearer definition would be the following, although the expression is only sensible if $X$ is a crv:

\begin{defn}[Tail-Value-at-Risk (TVaR)]\index{Tail-Value-at-Risk}\label{defn:tail_value_at_risk_v2}
  Let $X$ be an rv. The \hlnoteb{Tail-Value-at-Risk (TVAR)} of $X$ at the $100p\%$ level, denoted $\TVaR_p(X)$, is the expected loss given that the loss exceeds the $100p$ percentile (or quantile) of the distribution of $X$, expressible as
  \begin{equation*}
    \TVaR_p(X) = E[ X \mid X > \pi_p ] = \frac{1}{\bar{F}(\pi_p)} \int_{\pi_p}^{\infty} x f(x) \dif{x}.
  \end{equation*}
\end{defn}

Note that the two definitions agree with one another:

\begin{align*}
  \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} &= \frac{1}{1 - F(\pi_p)} \int_{p}^{1} F^{-1}(\alpha) \dif{\alpha} \\
                                                       &= \frac{1}{\bar{F}(\pi_p)} \int_{\pi_p}^{1} x f(x) \dif{x}
\end{align*}
where we let $\alpha = F(x)$ as substitution.

\begin{note}
  While it is not difficult to notice that
  \begin{equation*}
    \TVaR_p(X) \geq \VaR_p(X),
  \end{equation*}
  the proof is also simple:
  \begin{align*}
    \TVaR_p(X) &= \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} \\
               &\geq \frac{1}{1 - p} \pi_p \int_{p}^{1} \dif{\alpha} = \pi_p = \VaR_p(X).
  \end{align*}
\end{note}

\begin{eg}
  Find $\TVaR_p(X)$ for $X \sim \Exp(\theta)$.
\end{eg}

\begin{solution}
  Since $X$ is a crv, and $F(x) = 1 - e^{- \frac{x}{\theta}}$, we have that
  \begin{equation*}
    \pi_p = F^{-1}(p) = - \theta \ln (1 - p).
  \end{equation*}
  Therefore,
  \begin{align*}
    \TVaR_p(X) &= \frac{1}{1 - p} \int_{p}^{1} \pi_\alpha \dif{\alpha} = \frac{- \theta}{1 - p} \int_{p}^{1} \ln (1 - \alpha) \dif{\alpha} \\
               &= \frac{- \theta}{1 - p} \int_{-\infty}^{\ln (1 - p)} ue^u \dif{u} \quad \text{ let } u = \ln ( 1 - \alpha ) \\
               &= \frac{-\theta}{1 - p} \left[ ue^u \at{-\infty}{\ln (1 - p)} - \int_{-\infty}^{\ln(1-p)} e^u \dif{u} \right] \text{ by IBP } \\
               &= \frac{-\theta}{1 - p} \left[ (1 - p) \ln (1 - p) - ( 1 - p ) \right]\\
               &= \theta [ 1 - \ln (1 - p) ]
  \end{align*}
\end{solution}

\begin{note}
  From the last example, by the memoryless property of $\Exp(\theta)$, notice that we may also do
  \begin{align}
    \TVaR_p(X) &= E[ X \mid X > \pi_p ] = E [ X - \pi_p + \pi_p \mid X > \pi_p ] \nonumber \\
               &= E[ X - \pi_p \mid X > \pi_p ] + E[ \pi_p \mid X > \pi_p ] \label{eq:tvar_memoryless_exp}\\
               &= E[ X ] + \pi_p \nonumber
  \end{align}
\end{note}

% subsection risk_measures (end)

% section distributional_quantities_and_risk_measures_continued (end)

% chapter lecture_4_sep_18th (end)

\chapter{Lecture 5 Sep 20th}%
\label{chp:lecture_5_sep_20th}
% chapter lecture_5_sep_20th

\section{Distrbutional Quantities and Risk Measures (Continued 2)}%
\label{sec:distrbutional_quantities_and_risk_measures_continued_2}
% section distrbutional_quantities_and_risk_measures_continued_2

\subsection{Risk Measures (Continued)}%
\label{sub:risk_measures_continued}
% subsection risk_measures_continued

Before ending this section, we introduce a notion that is related to $\TVaR$.

\begin{defn}[Mean Excess Loss]\index{Mean Excess Loss}\label{defn:mean_excess_loss}
  Let $X$ be an rv, and $d \in \mathbb{R}$. The \hlnoteb{mean excess loss}, denoted $e_X(d)$, is defined as
  \begin{equation*}
    e_X(d) = E[ X - d \mid X > d ]
  \end{equation*}
  and $e_X(d) = 0$ for those $d$ such that $P(X > d) = 0$.
\end{defn}

\begin{propo}[Relation of $\TVaR_p(X)$ and $e_X(d)$]\label{propo:relation_of_tvar_p_x_and_e_x_d_}
  For a crv $X$, we have
  \begin{equation*}
    \TVaR_p(X) = e_X(\pi_p) + \VaR_p(X)
  \end{equation*}
\end{propo}

\begin{proof}
  By \cref{eq:tvar_memoryless_exp}, we have that
  \begin{equation*}
    \TVaR_p(X) = E [ X - \pi_p \mid X > \pi_p ] + \pi_p = e_X(\pi_p) + \pi_p.
  \end{equation*}\qed\
\end{proof}

\begin{propo}[Expection from Survival Function]\label{propo:expection_from_survival_function}
  Let $X$ be a non-negative rv such that $E[X^k] < \infty$, for any $k \in \mathbb{N} \setminus \{ 0 \}$. Then\sidenote{Note that this works for the discrete case as well, by replacing $\int$ with $\sum$.}
  \begin{equation*}
    E\left[X^k\right] = k \int_{0}^{\infty} x^{k - 1} \bar{F}(x) \dif{x}
  \end{equation*}
\end{propo}

\begin{proof}
  Firstly, note that since $E[X^k] < \infty$ for all $k \in \mathbb{N} \setminus \{0\}$, we have that $\bar{F}(x)$ decays faster than $x^k$ as $x \to \infty$. Now
  \begin{align*}
    E\left[ X^k \right] &= \int_{0}^{\infty} x^k f(x) \dif{x} \quad \because \text{ Law of the Unconscious Statistician} \\
                        &= \int_{0}^{\infty} x^k \dif{F(x)} \quad \because \dif{F(x)} = f(x) \dif{x} \\
                        &= - \int_{0}^{\infty} x^k \dif{\bar{F}(x)} \\
                        &= - \left[ x^k \bar{F}(x) \at{0}{\infty} - \int_{0}^{\infty} kx^{k - 1} \bar{F}(x) \dif{x} \right] \quad \because \text{ IBP } \\
                        &= k \int_{0}^{\infty} x^{k - 1} \bar{F}(x) \dif{x}
  \end{align*}\qed\
\end{proof}

\begin{eg}
  Calculate $e_X(d)$ and $\TVaR_p(X)$ for a Pareto distribution $X$ with cdf
  \begin{equation*}
    F(x) = 1 - {\left( \frac{\theta}{x + \theta} \right)}^\alpha, \quad x > 0,
  \end{equation*}
  where $\alpha > 1$ and $\theta > 0$.
\end{eg}

\begin{solution}
  Using \cref{propo:expection_from_survival_function},
  \begin{align*}
    e_X(d) &= \int_{0}^{\infty} P(X - d > x \mid X > d) \dif{x} = \int_{0}^{\infty} \frac{P(X - d > x, X > d)}{P(X > d)} \dif{x} \\
           &= \int_{0}^{\infty} \frac{P(X > x + d)}{P(X > d)} \dif{x} = \int_{0}^{\infty} \frac{\bar{F}(x + d)}{\bar{F}(d)} \dif{x} \\
           &= \int_{0}^{\infty} {\left( \frac{d + \theta}{x + d + \theta} \right)}^\alpha \dif{x} = \frac{{( d + \theta )}^\alpha}{1 - \alpha} {\left( \frac{1}{x + d + \theta} \right)}^{ \alpha - 1 } \at{0}{\infty} \\
           &= \frac{d + \theta}{\alpha - 1}
  \end{align*}
  By \cref{eg:varp_pareto}, we have
  \begin{equation*}
    \pi_p = \theta \left[ {( 1 - p )}^{-\frac{1}{\alpha}} - 1 \right]
  \end{equation*}
  and so
  \begin{align*}
    \TVaR_p(X) &= e_X(\pi_p) + \pi_p \\
               &= \frac{\theta\left[ {(1 - p)}^{-\frac{1}{\alpha}} - 1 \right] + \theta}{\alpha - 1} + \theta\left[ {( 1 - p )}^{-\frac{1}{\alpha}} - 1 \right] \\
               &= \frac{\theta{(1 - p)}^{-\frac{1}{\alpha}}}{\alpha - 1} + \frac{\theta(\alpha - 1){(1 - p)}^{-\frac{1}{\alpha}}}{\alpha - 1} - \theta \\
               &= \frac{\theta \alpha {( 1 - p )}^{-\frac{1}{\alpha}}}{\alpha - 1} - \theta
  \end{align*}
\end{solution}

\begin{propo}[Expected Deductible]\label{propo:expected_deductible}
  We have
  \begin{equation*}
    E( {[ X - d ]}_+ ) = \int_{d}^{\infty} \bar{F}(x) \dif{x}
  \end{equation*}
\end{propo}

\begin{proof}
  By the Law of the Unconscious Statistician and IBP on the last step,
  \begin{equation*}
    E( {[ X - d ]}_+ ) = \int_{d}^{\infty} ( x - d ) \dif{F(x)} = - \int_{d}^{\infty} (x - d) \dif{\bar{F}(x)} = \int_{d}^{\infty} \bar{F}(x) \dif{x}
  \end{equation*}\qed\
\end{proof}

\begin{propo}[An Expression for Mean Excess Value]\label{propo:an_expression_for_mean_excess_value}
  If $\bar{F}(d) > 0$, we have
  \begin{equation*}
    e_X(d) = \frac{\int_{d}^{\infty} \bar{F}(x) \dif{x}}{\bar{F}(d)}
  \end{equation*}
\end{propo}

\begin{proof}
  Observe that by \cref{propo:expected_deductible}, we have
  \begin{align*}
    e_X(d) &= E[ X - d \mid X > d ] = \frac{E[ ( X - d ) \mathbb{1}_{X > d} ]}{P(X > d)} \\
           &= \frac{E( {[ X - d ]}_+ )}{\bar{F}(d)} = \frac{\int_{d}^{\infty} \bar{F}(x) \dif{x}}{\bar{F}(d)}
  \end{align*}\qed\
\end{proof}

% subsection risk_measures_continued (end)

% section distrbutional_quantities_and_risk_measures_continued_2 (end)

\section{Severity Distributions --- Creating Severity Distributions}%
\label{sec:severity_distributions_creating_severity_distributions}
% section severity_distributions_creating_severity_distributions

Recall the definition of a severity distribution.

\begin{defnnonum}[Severity Distribution]\index{Severity Distribution}
  A \hlnoteb{severity distribution} is a distribution used to describe single random losses in an insurance portfolio.
\end{defnnonum}

When a loss occurs, the full amount of the loss is not necessarily the amount paid by the insurer, since an insurance policy typically involves some form of adjustment (e.g. \hlnotea{deductible, limit, coinsurance}). A distinction needs to be made between the actual loss prior to any of the adjustments (aka \hldefn{ground-up loss}) and the amount ultimately paid by the insurer.

Our goal is to find a reasonable model for the \hlnotea{ground-up loss} rv $X$. The following are two desirable properties for $X$:
\begin{itemize}
  \item $\text{Im}(X) = \mathbb{R}_{> 0}$, since losses are positive;
  \item pf of $X$ is right-skewed, since we want the ``tail'' of the distribution to be not heavy.
    \begin{itemize}
      \item The motivation for this property is due to the \hlnotea{20-80 rule}: 20\% of the largest claims accountn for 80\% of the total claim amount.
    \end{itemize}
\end{itemize}

\newthought{There are} two approaches to constructing a severity distribution:
\begin{itemize}
  \item \hlnotea{Parametric approach}\sidenote{This approach shall be the focus of this course.}: specify a ``form'' for the distribution with a finite number of parameters.
  \item Nonparametric approach: no form is specified; the distribution is constructed directly from the empirical data.
\end{itemize}

A weakness of the \textbf{Nonparametric approach} is, if there is not enough data, such as in catasthropic risks, is becomes difficult to obtain reliable information. We shall look at one such example in this approach.

\begin{defn}[Empirical Distribution Function]\index{Empirical Distribution Function}\label{defn:empirical_distribution_function}
  Let $\{ X_1, \ldots, X_n \}$ be an iid sample of a risk $X$. Then its \hlnoteb{empricial distribution function (edf)} is defined as
  \begin{equation*}
    \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^{n}  \mathbb{1}_{\{X_i \leq x\}}, \quad x \in \mathbb{R}.
  \end{equation*}
\end{defn}

\begin{remark}
  Simply put, the edf assigns a probability of $\frac{1}{n}$ to each sample point $X_i$.
\end{remark}

\begin{eg}
  Consider a random sample of a risk with size $5$: $\{ 30, 80, 150, 150, 200 \}$. Find the edf of the risk.
\end{eg}

\begin{solution}
  The edf is given by
  \begin{equation*}
    \hat{F}_n(x) = \frac{1}{5} \sum_{i=1}^{5} \mathbb{1}_{\{ X_i \leq x_i \}} = \begin{cases}
      0           & x < 30 \\
      \frac{1}{5} & 30 \leq x < 80 \\
      \frac{2}{5} & 80 \leq x < 150 \\
      \frac{4}{5} & 150 \leq x < 200 \\
      1           & x \geq 200
    \end{cases}
  \end{equation*}
\end{solution}

% section severity_distributions_creating_severity_distributions (end)

% chapter lecture_5_sep_20th (end)

\chapter{Lecture 6 Sep 25th}%
\label{chp:lecture_6_sep_25th} % chapter lecture_6_sep_25th

\section{Severity Distributions --- Creating Severity Distributions (Continued)}%
\label{sec:severity_distributions_creating_severity_distributions_continued}
% section severity_distributions_creating_severity_distributions_continued

\paragraph{The Parametric Approach} The following is a graph showing the process of a parametric approach:

\begin{figure}[h]
  \begin{tikzpicture}[
    block/.style = {rectangle, draw, rounded corners,
               text width =15em, align=center,
               color=dark,fill=be-blue},
    sblock/.style = {rectangle, draw, rounded corners,
               text width =4em, align=center},
    ]
    \node [block] (selection) {\textbf{Model Selection}\\
      select a model based on prior knowledge of historical datasets};
    \node [block, below=of selection] (estimation) {\textbf{Model Estimation}\\
      estimate parameter values based on data};
    \node [block, below=of estimation] (validation) {\textbf{Model Validation} \\
      test for goodness-of-fit};
    \node [block, below=of validation] (decision) {Is model acceptable?};
    \node [sblock, left=of decision,color=dark,fill=be-red] (no) {\textbf{No}};
    \node [sblock, right=of decision,color=dark,fill=be-green] (yes) {\textbf{DONE!}};
    
    \draw[-latex'] (selection) -- (estimation);
    \draw[-latex'] (estimation) -- (validation);
    \draw[-latex'] (validation) -- (decision);
    \draw[-latex'] (decision) -- (no);
    \draw[-latex'] (no) |- (selection.west);
    \draw[-latex'] (decision) -- (yes);
  \end{tikzpicture}
  \caption{Process of a Parametric Approach}\label{fig:process_of_a_parametric_approach}
\end{figure}

\paragraph{Common Techniques in Creating New Parametric Distributions} Before diving into the topic, first, a definition:

\begin{defn}[Parametric Distribution]\index{Parametric Distribution}\label{defn:parametric_distribution}
  A \hlnoteb{parametric distribution} is a set of distribution functions, of which each member is determined by specifying one or more parameters.
\end{defn}

Some common techniques are the following:
\begin{itemize}
  \item Multiplication by a constant
  \item Raising to a power
  \item Exponentiation
  \item Mixture of distributions
\end{itemize}

\subsection{Multiplication By A Constant}%
\label{sub:multiplication_by_a_constant}
% subsection multiplication_by_a_constant

This transformation is equivalent to applying inflation uniformly across all loss levels, and is known as a change of scale.

\begin{propo}[Multiplication by a Constant]\label{propo:multiplication_by_a_constant}
  Let $X$ be a crv with cdf $F_X$ and pdf $f_X$. Let $Y = cX$ for some $c > 0$. Then
  \begin{equation*}
    F_Y(y) = F_X\left( \frac{y}{c} \right), \quad f_Y(y) = \frac{1}{c}f_X\left( \frac{y}{c} \right).
  \end{equation*}
\end{propo}

\begin{proof}
  \begin{gather*}
    F_Y(y) = P(Y \leq y) = P( cX \leq y ) = P\left( X \leq \frac{y}{c} \right) = F_X\left( \frac{y}{c} \right) \\
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X\left( \frac{y}{c} \right) = \frac{1}{c}f_X\left( \frac{y}{c} \right)
  \end{gather*}\qed\
\end{proof}

\begin{defn}[Scale Distribution]\index{Scale Distribution}\label{defn:scale_distribution}
  We say that a parametric distribution is a \hlnoteb{scale distribution} if $Y = cY$ for any positive constant $c$ is from the same set of distributions as $X$.
\end{defn}

It is clear that we have the following result:

\begin{crly}\label{crly:constant_multiplication_crly}
  The parameter $c$ in \cref{propo:multiplication_by_a_constant} is a scale parameter, and $Y$ is a scale distribution.
\end{crly}

\begin{eg}\label{eg:scale_distn_exp}
  Let $X \sim \Exp(\theta)$ with pdf
  \begin{equation*}
    f_X(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}, \quad x > 0.
  \end{equation*}
  Let $y = cX$ with $c > 0$, it follows that
  \begin{equation*}
    f_Y(y) = \frac{1}{c}f_X\left(\frac{y}{c}\right) = \frac{1}{c \theta} e^{- \frac{y}{c \theta}}, \quad y > 0.
  \end{equation*}
  Thus $Y \sim \Exp(c\theta)$ and so $Y$ is a scale distribution. In particular, the exponential distribution belongs to a family of scale distributions.
\end{eg}

\begin{defn}[Scale Parameter]\index{Scale Parameter}\label{defn:scale_parameter}
  A parameter $\theta$ is called a \hlnoteb{scale paramter} of a parametric distribution $X$ if it satisfies the following condition: the parametric value of $cX$ is $c \theta$ for any positive constant $c$, and other parameters (if any) remain unchanged.
\end{defn}

\begin{eg}
  From \cref{eg:scale_distn_exp}, we had that
  \begin{equation*}
    f_X(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}, \quad x > 0.
  \end{equation*}
  We showed that $Y = cX \sim \Exp(c\theta)$. Therefore, the parameter $\theta$ is a scale parameter.
\end{eg}

\begin{eg}
  Determine whether the lognormal distribution $X \sim \LogN(\mu, \sigma^2)$, i.e. $\ln(X) \sim \Nor(\mu, \sigma^2)$, is a scale distribution or not. If yes, determine whether it has any scale parameter.
\end{eg}

\begin{solution}
  Let $Y = cX$ for some $c > 0$. Observe that
  \begin{equation*}
    \ln Y = \ln cX = \ln c + \ln X \sim \Nor(\mu + \ln c, \sigma^2).
  \end{equation*}
  For the last equation, note that if we let $Z = \ln X \sim \Nor(\mu, \sigma^2)$
  \begin{align*}
    E\left[ e^{t( Z + \ln c )} \right] &= e^{t \ln c} e^{\mu t + \frac{\sigma^2 t^2}{2}} = e^{t ( \mu + \ln c ) + \frac{\sigma^2 t^2}{2}}
  \end{align*}
  we see that the above is the mgf of $\Nor(\mu + \ln c, \sigma^2)$. Thus we have that $Y$ has the same distribution as $X$ and so it is a scale distribution. However, we also see that it has no scale parameters.
\end{solution}

% subsection multiplication_by_a_constant (end)

\subsection{Raising to a Power}%
\label{sub:raising_to_a_power}
% subsection raising_to_a_power

\begin{propo}[Raising to a Power]\label{propo:raising_to_a_power}
  Let $X$ be a crv with pdf $f_X$ and cdf $F_X$ with $F_X(0) = 0$. Let $Y = X^{\frac{1}{\tau}}$. If $\tau > 0$, then
  \begin{equation*}
    F_Y(y) = F_X(y^\tau), \quad f_Y(y) = \tau y^{\tau - 1} f_X(y^\tau), \quad y > 0,
  \end{equation*}
  while if $\tau < 0$, then
  \begin{equation*}
    F_Y(y) = 1 - F_X(y^\tau), \quad f_Y(y) = - \tau y^{\tau - 1} f_X(y^\tau), \quad y > 0.
  \end{equation*}
\end{propo}

\begin{proof}
  When $\tau > 0$,
  \begin{equation*}
    F_Y(y) = P(Y \leq y) = P\left(X^{\frac{1}{\tau}}\right) = P\left(X \leq y^\tau\right) = F_X\left(y^\tau\right)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy}f_X\left(y^\tau\right) = \tau y^{\tau - 1} f_X(y^\tau).
  \end{equation*}
  When $\tau < 0$,
  \begin{equation*}
    F_Y(y) = P(Y \leq y) = P\left(X^{\frac{1}{\tau}} \leq y\right) = P\left( X \geq y^\tau \right) = \bar{F}_X(y^\tau)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} (1 - F_X\left(y^\tau\right)) = - \tau y^{\tau - 1} f_X\left(y^\tau\right).
  \end{equation*}\qed\
\end{proof}

\begin{eg}
  Let $X \sim \Exp(\theta)$ and $Y = X^{\frac{1}{\tau}}$ for $\tau > 0$, we have
  \begin{equation*}
    F_Y(y) = F_X\left(t^{\tau}\right) = 1 - e^{ \frac{-y^\tau}{\theta} } = 1 - e^{-{\left( \frac{y}{\alpha} \right)}^\tau},
  \end{equation*}
  where $\alpha = \theta^{\frac{1}{\tau}}$. In particular, we have that $Y \sim \Wei(\alpha, \tau)$.
\end{eg}

% subsection raising_to_a_power (end)

\subsection{Exponentiation}%
\label{sub:exponentiation}
% subsection exponentiation

\begin{propo}[Exponentiation Method]\label{propo:exponentiation_method}
  Let $X$ be a crv with pdf $f_X$ and cdf $F_X$. Let $Y = e^X$. Then
  \begin{equation*}
    F_Y(y) = F_X(\ln y), \quad f_Y(y) = \frac{1}{y} f_X(\ln y).
  \end{equation*}
\end{propo}

\begin{proof}
  We have
  \begin{equation*}
    F_Y(y) = P\left( e^X \leq y \right) = P( X \leq \ln y ) = F_X(\ln y)
  \end{equation*}
  and
  \begin{equation*}
    f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X(\ln y) = \frac{1}{y} f_X(\ln y).
  \end{equation*}\qed\
\end{proof}

\begin{ex}[Lognormal Distribution]\label{eg:lognormal_distribution}
  Let $X \sim \Nor(\mu, \sigma^2)$. The cdf and pdf of $Y = e^X$ is
  \begin{gather*}
    F_Y(y) = F_X(\ln y) = \Phi\left( \frac{\ln y - \mu}{\sigma} \right) \\
    f_Y(y) = \frac{1}{y} f_X(\ln y) = \frac{1}{y} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \cdot {\left( \frac{\ln y - \mu}{\sigma} \right)}^2}
  \end{gather*}
\end{ex}

% subsection exponentiation (end)

\subsection{Mixing Distributions}%
\label{sub:mixing_distributions}
% subsection mixing_distributions

The rationale behind mixing distributions is to define an rv $X$ conditional on a second rv, say $\Theta$ (aka \hldefn{mixing rv}). The mixing rv $\Theta$ can either be discrete or be continuous, which leads to two types of mixtures:
\begin{itemize}
  \item \hlnotea{discrete mixture}: when $\Theta$ is discrete; and
  \item \hlnotea{continuous mixture}: when $\Theta$ is continuous.
\end{itemize}

\begin{defn}[Discrete Mixed Distribution]\index{Discrete Mixed Distribution}\label{defn:discrete_mixed_distribution}
  Let $\Theta$ be a drv taking values on $\{ \theta_1, \theta_2, \ldots, \theta_n \}$ with
  \begin{equation*}
    P(\Theta = \theta_i) = p_i > 0, \quad i = 1, \ldots, n,
  \end{equation*}
  and the rv $Y_i := X \mid \Theta = \theta_i$ has cdf
  \begin{equation*}
    F_{Y_i}(x) = P(X \leq x \mid \Theta = \theta_i), x \in \mathbb{R}.
  \end{equation*}
  Then $X$ is called a \hlnoteb{discrete mixed distribution} with cdf
  \begin{equation*}
  F_X(x) = \sum_{i=1}^{n} P(X \leq x \mid \Theta = \theta_i) P(\Theta = \theta_i) = \sum_{i=1}^{n} p_i F_{Y_i}(x).
  \end{equation*}
\end{defn}

Following the above definition, by the Law of the Unconscious Statistician, we have
\begin{equation*}
  E[g(X)] = \sum^{n}_{i=1} E[g(X) \mid \Theta = \theta_i] P(\Theta = \theta_i) = \sum_{i=1}^{n} p_i E[g(Y_i)],
\end{equation*}
for any function $g$ such that the expectation exists. In particular, we have
\begin{equation*}
  E[X] = \sum_{i=1}^{n} p_i E[Y_i] \text{ and } E\left[ X^2 \right] = \sum_{i=1}^{n} p_i E\left[Y_i^2\right].
\end{equation*}

\begin{eg}
  Let $Y_i \sim \Exp(i)$ for $i = 1, 2, 3$. Define $X$ to be an equal mixture of these three exponential rvs. Fidn the cdf, pdf, and mean of $X$.
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{align*}
    F_X(x) &= \sum_{i=1}^{3} \frac{1}{3}F_{Y_i}(x) = \frac{(1 - e^{-x}) + (1 - e^{- x / 2}) + (1 - e^{-x / 3})}{3} \\
           &= 1 - \frac{1}{3} \left( e^{-x} + e^{-\frac{x}{2}} + e^{-\frac{x}{3}} \right), x > 0.
  \end{align*}
  The pdf of $X$ is
  \begin{equation*}
    f_X(x) = \frac{1}{3} \left( e^{-x} + \frac{1}{2} e^{-\frac{x}{2}} + \frac{1}{3} e^{-\frac{x}{3}} \right), x > 0.
  \end{equation*}
  The mean of $X$ is therefore
  \begin{equation*}
    E[X] = \sum_{i=1}^{3} E[ Y_i ] = \frac{1}{3} ( 1 + 2 + 3 ) = 2.
  \end{equation*}
\end{solution}

% subsection mixing_distributions (end)

% section severity_distributions_creating_severity_distributions_continued (end)

% chapter lecture_6_sep_25th (end)

\chapter{Lecture 7 Sep 27th}%
\label{chp:lecture_7_sep_27th}
% chapter lecture_7_sep_27th

\section{Severity Distributions --- Creating Severity Distributions (Continued 2)}%
\label{sec:severity_distributions_creating_severity_distributions_continued_2}
% section severity_distributions_creating_severity_distributions_continued_2

\subsection{Mixing Distributions (Continued)}%
\label{sub:mixing_distributions_continued}
% subsection mixing_distributions_continued

\begin{defn}[Continuous Mixture]\index{Continuous Mixture}\label{defn:continuous_mixture}
  Let $\Theta$ be a crv with density $f_\Theta$, and the cdf and pdf of $X \mid \Theta = \theta$ are given by
  \begin{equation*}
    F_{X \mid \Theta}(x \mid \theta) = P(X \leq x \mid \Theta = \theta) \text{ and } f_{X \mid \Theta}(x \mid \theta) = P(X = x \mid \Theta = \theta).
  \end{equation*}
  The unconditional distribution of $X$ is said to be a \hlnoteb{continuous mixed distribution} with cdf and pdf
  \begin{align*}
    F_X(x) &= \int_{-\infty}^{\infty} F_{X \mid \Theta}(x\mid\theta) f_{\Theta}(\theta) \dif{\theta} \\
    f_X(x) &= \int_{-\infty}^{\infty} f_{X \mid \Theta}(x\mid\theta) f_{\Theta}(\theta) \dif{\theta}.
  \end{align*}
  Furthermore, for any function $H$,
  \begin{equation*}
    E[H(X)] = \int_{-\infty}^{\infty} E[H(X) \mid \Theta = \theta] f_{\Theta}(\theta) \dif{\theta}.
  \end{equation*}
\end{defn}

\begin{eg}
  Suppose that $X \mid \Lambda = \lambda$ is exponentially distributed with mean $\frac{1}{\lambda}$, and let $\Lambda$ be a gamma distributed rv with mean $\alpha / \theta$ and variance $\alpha / \theta^2$, i.e.
  \begin{equation*}
    f_{\Lambda}(\lambda) = \frac{\theta^\alpha \lambda^{\alpha - 1} e^{-\theta \lambda}}{\Gamma(\alpha)}, \lambda > 0,
  \end{equation*}
  where $\Gamma(\alpha) = \int_{0}^{\infty} t^{\alpha - 1} e^{-t} \dif{t}$ is the gamma function. Determine the conditional pdf of $X$.
\end{eg}

\begin{solution}
  We have
  \begin{align*}
    f_X(x) &= \int_{0}^{\infty} f_{X \mid \Lambda}(x \mid \lambda) f_{\Lambda}(\lambda) \dif{\lambda} \\
           &= \int_{0}^{\infty} \lambda e^{-x \lambda} \frac{\theta^{\alpha} \lambda^{\alpha - 1} e^{-\theta \lambda}}{\Gamma(\alpha)} \dif{\lambda} \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha)} \int_{0}^{\infty} \lambda^\alpha e^{-\lambda ( x + \theta )} \dif{\lambda} \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha) (x + \theta)} \int_{0}^{\infty} {\left( \frac{y}{x + \theta} \right)}^\alpha e^{-y} \dif{y} \enspace \text{ where } y = \lambda ( x + \theta ) \\
           &= \frac{\theta^\alpha}{\Gamma(\alpha) {( x + \theta )}^{\alpha + 1}} \int_{0}^{\infty} y^\alpha e^{-y} \dif{y} \\
           &= \frac{\theta^\alpha \Gamma(\alpha + 1)}{\Gamma(\alpha) {( x + \theta )}^{\alpha + 1}} = \frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}.
  \end{align*}
\end{solution}

\begin{propo}[Total Expectation and Total Variance]\index{Total Expectation}\index{Total Variance}\label{propo:total_expectation_and_total_variance}
  For any rvs $X$ and $\Theta$, provided that the repsective expectation and variance exist, we have
  \begin{gather*}
    E[X] = E[ E[ X \mid \Theta ] ] \\
    \Var(X) = E[ \Var(X \mid \Theta) ] + \Var( E[ X \mid \Theta ] )
  \end{gather*}
\end{propo}

\begin{proof}
  \begin{align*}
    E[X] &= E\left( \int_{X} xf_{X \mid \Theta}(x \mid \Theta) \dif{x} \right) \\
         &= \int_{\Theta} \int_{X} xf_{X \mid \Theta}( x \mid \theta ) f_{\Theta}(\theta) \dif{x} \dif{\theta} \\
         &= \int_{X} x \int_{\Theta} f_{X, \Theta}(x, \theta) \dif{\theta} \dif{x} \enspace \because \text{ Fubini's Theorem } \\
         &= \int_{X} xf_X(x) \dif{x} = E[X].
  \end{align*}
  Note that
  \begin{equation*}
    \Var(X \mid \Theta) = E[ X^2 \mid \Theta ] + E{[ X \mid \Theta ]}^2.
  \end{equation*}
  And so
  \begin{align*}
    &E[\Var(X \mid \Theta)] + \Var( E[ X \mid \Theta ] ) \\
    &= E[ E[ X^2 \mid \Theta ] ] - E \left[ E{[ X \mid \Theta ]}^2 \right] + E \left[ E {[ X \mid \Theta ]}^2 \right] - E{[ E [ X \mid \Theta ] ]}^2 \\
    &= E\left[X^2\right] - E{[X]}^2 = \Var(X)
  \end{align*}\qed\
\end{proof}

\begin{eg}
  Suppose that $X \mid \Theta = \theta \sim \Exp(\theta)$ and $p_{\Theta}(\theta) = \frac{1}{3}$ for $\theta = 1, 2, 3$. Find the mean and variance of $X$.
\end{eg}

\begin{solution}
  The mean of $X$ is
  \begin{equation*}
    E[X] = EE[X \mid \Theta] = E[\Theta] = \frac{1}{3} (1 + 2 + 3) = 2.
  \end{equation*}
  The variance of $X$ is
  \begin{align*}
    \Var(X) &= E[\Var(X \mid \Theta)] + \Var( E[X \mid \Theta] ) \\
            &= E[\Theta^2] + \Var(\Theta) = 2E[\Theta^2] - E{[\Theta]}^2 \\
            &= \frac{2}{3}(1 + 4 + 9) - 4 = \frac{28}{3} - \frac{12}{3} = \frac{16}{3}
  \end{align*}
\end{solution}

\begin{eg}
  Suppose that $X \mid \Lambda = \lambda \sim \Exp(\lambda)$ and $\Lambda \sim \Gam(\alpha, \theta)$ with mean $\alpha \theta$ and variance $\alpha \theta^2$. Find the mean and variance of $X$.
\end{eg}

\begin{solution}
  The mean of $X$ is
  \begin{equation*}
    E[X] = EE[X \mid \Lambda] = E[ \Lambda ] = \alpha\theta.
  \end{equation*}
  The variance of $X$ is
  \begin{align*}
    \Var(X) &= E[\Var(X \mid \Lambda)] + \Var( E[ X \mid \Lambda ] ) \\
            &= E[\Lambda^2] + \Var(\Lambda) = 2\Var(\Lambda) + E{[\Lambda]}^2 \\
            &= 2 \alpha \theta^2 + \alpha^2 \theta^2.
  \end{align*}
\end{solution}

% subsection mixing_distributions_continued (end)

% section severity_distributions_creating_severity_distributions_continued_2 (end)

\section{Severity Distributions --- Tail of Distributions}%
\label{sec:severity_distributions_tail_of_distributions}
% section severity_distributions_tail_of_distributions

\begin{defn}[Tail]\index{Tail}\label{defn:tail}
  The \hlnoteb{tail} of a distribution (usually the right tail) is the portion of the distribution corresponding to large values of the random variable.
\end{defn}

It is important that we understand large possible loss values as they have the greatest impact on the total losses that we may have to endure. In general, a loss rv is said to be \hldefn{heavy-tailed} if it has a large probability to take large values.

Two measurements of tail weight:
\begin{itemize}
  \item \textbf{relative}: comparing ``sizes'' of the tails of two distributions;
  \item \textbf{absolute}: classifying distributions as heavy or light-tailed.
\end{itemize}

The following is a set of criteria to measure or compare the heaviness of the tails of loss distributions:
\begin{itemize}
  \item Existence of moments
  \item Limiting ratios
  \item Hazard rate function
  \item Mean excess loss function
\end{itemize}

\subsection{Existence of Moments}%
\label{sub:existence_of_moments}
% subsection existence_of_moments

Recall that the $k$th moment of a loss $X$ is
\begin{equation*}
  E\left[X^k\right] = \int_{0}^{\infty} x^k f_X(x) \dif{x}.
\end{equation*}
Now if $f_X$ takes on large values for large $x$, we may have $E\left[X^k\right]$ blow up to infinity, and so it is desirable to find/use some distribution with a \hlnotea{decaying} probability function, one at which its rate of decay is faster than the growth of $x^{-(k + 1)}$.

% subsection existence_of_moments (end)

% section severity_distributions_tail_of_distributions (end)

% chapter lecture_7_sep_27th (end)

\chapter{Lecture 8 Oct 02nd}%
\label{chp:lecture_8_oct_02nd}
% chapter lecture_8_oct_02nd

\section{Severity Distributions --- Tail of Distributions (Continued)}%
\label{sec:severity_distributions_tail_of_distributions_continued}
% section severity_distributions_tail_of_distributions_continued

\subsection{Existence of Moments (Continued)}%
\label{sub:existence_of_moments_continued}
% subsection existence_of_moments_continued

\begin{eg}
  For a Pareto distribution, as $x \to \infty$, we have that $f_X(x) \sim x^{-(\alpha + 1)}$, so its moments are finite if and only if $k < \alpha$.

  We say that the Pareto distribution has a \hlnotea{power tail}.
\end{eg}

\begin{eg}
  Given the transformed Gamma distribution, with pdf
  \begin{equation*}
    f_X(x) = \frac{{\left( \frac{x}{\theta} \right)}^\alpha e^{- \frac{x}{\theta}}}{x \Gamma(\alpha)}.
  \end{equation*}
  Now as $x \to \infty$, we have
  \begin{equation*}
    f_X(x) \sim x^{\alpha - 1} e^{-\frac{x}{\theta}}
  \end{equation*}
  We see that the exponential term decays faster than the rate of growth of $x^{\alpha - 1}$ for any $\alpha > 0$. Thus all moments of the Gamma distribution exists.

  We say that the Gamma distribution has a \hlnotea{exponential tail}.
\end{eg}

\begin{ex}
  The Normal distribution has an exponential tail.
\end{ex}

\begin{defn}[Heavy-Tails Light-Tails]\index{Heavy-Tailed Distribution}\index{Light-Tailed Distribution}\label{defn:heavy_tails_light_tails}
  We say that a distribution is a \hlnoteb{heavy-tailed distribution} if \hlimpo{its moments only exist up to some $k \in \mathbb{N} \setminus \{ 0 \}$}.\marginnote{The actual definition, or should I say notion, of tail-heaviness comes from talking about the boundedness of the tail of the distribution, with reference to the exponential distribution. If a distribution has a tail that has greater value than the tail of the exponential distribution, then we say that the distribution has a heavy-tail.}

  We say that a distribution is a \hlnoteb{light-tail distribution} if \hlimpo{its moments exist for all $k \in \mathbb{N} \setminus \{ 0 \}$}.
\end{defn}

\begin{note}
  We may also use the mgf to determine if a distribution has a heavy or light tail; the inexistence of the $k$th moment implies the inexistence of the mgf, i.e.\ if the mgf does not exist, then the moments of the distribution is only finite up to some $k \in \mathbb{N} \setminus \{0\}$.
\end{note}

\subsubsection{Limiting Ratio: Survival Functions}%
\label{ssub:limiting_ratio_survival_functions}
% subsubsection limiting_ratio_survival_functions

\begin{defn}[Limiting Ratio]\index{Limiting Ratio}\label{defn:limiting_ratio}
  The \hlnoteb{limiting ratio} of \hlnoteb{two survival functions} is used to compare the heaviness of tails of the two losses. Consider two losses $X$ and $Y$, and consider the limit of the ratio
  \begin{equation*}
    \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)}.
  \end{equation*}
  If the limit does not exist, we say that the comparison is inconclusive. Otherwise, we have 3 cases:
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}[yscale=2]
      \draw[->] (-0.5,-0.1) -- (3, -0.1) node[right] {$x$};
      \draw[->] (0,-0.5) -- (0, 1.5) node[above] {$y$};
      \draw[-,domain=0:3,color=be-blue,thick] plot ({\x},{ 0.5 * exp( -0.5 * \x )});
      \draw[-,domain=0:3,color=be-red,thick] plot ({\x},{ 1 * exp( -1 * \x )});
    \end{tikzpicture}
    \caption{Limiting Ratio}\label{fig:limiting_ratio}
  \end{marginfigure}
  \begin{itemize}
    \item If $c = 0$, then $\bar{F}_X(x)$ decays faster than $\bar{F}_Y(x)$ as $x \to \infty$, i.e. $Y$ has a heavier tail than $X$;
    \item If $0 < c < \infty$, then $\bar{F}_X(x)$ and $\bar{F}_Y(x)$ decays at the smae rate, as $x \to \infty$, i.e. $X$ and $Y$ have similar tails;
    \item If $c = \infty$, then $\bar{F}_X(x)$ decays slower than $\bar{F}_Y(x)$ as $x \to \infty$, i.e. $X$ has a heavier tail than $Y$;
  \end{itemize}
  where we let
  \begin{equation*}
    c := \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)}
  \end{equation*}
\end{defn}

\begin{note}
  Not all distributions have an explicit survival function, but they will always have a pdf/pmf. Fortunately, by \hlnotea{L'H\^{o}pital's Rule}, the above definition can be applied to the pdfs of $X$ and $Y$, i.e.
  \begin{equation*}
    c = \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_Y(x)} = \lim_{x \to \infty} \frac{-f_X(x)}{-f_Y(x)} = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(x)}
  \end{equation*}
\end{note}

\begin{eg}
  Show that the Pareto distribution has a heavier tail than the Gamma distribution using limiting ratio.
\end{eg}

\begin{solution}
  Let $X \sim \Pareto(\alpha, \theta)$ and $Y \sim \Gam(\tau, \lambda)$. We have
  \begin{align*}
    c = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(x)} &= \lim_{x \to \infty} \frac{\frac{\alpha \theta^{\alpha}}{{( x + \theta )}^{\alpha + 1}}}{\frac{x^{\tau - 1} e^{-\frac{x}{\lambda}}}{\lambda^\tau \Gamma(\tau)}} = \alpha \theta^\alpha \lambda^\tau \Gamma(\tau) \lim_{x \to \infty} \frac{e^{\frac{x}{\lambda}}}{x^{\tau - 1} {(x + \theta)}^{\alpha + 1}}
  \end{align*}
  Since the exponential term grows faster than the term in the denominator, we have $c = \infty$, i.e. $X$ has a heavier tail than $Y$, as required.
\end{solution}

\begin{eg}
  For two losses $X$ and $Y$, suppose that $f_X(x) = \frac{2}{\pi (1 + x^2)}$ and $f_Y(x) = \frac{1}{(1 + x^2)}$ for $x > 0$. Compare the tail heaviness of the two losses.
\end{eg}

\begin{solution}
  Notice that
  \begin{equation*}
    c = \lim_{x \to \infty} \frac{f_X(x)}{f_Y(y)} = \lim_{x \to \infty} = \frac{2}{\pi} < \infty,
  \end{equation*}
  i.e. $X$ and $Y$ have similar tails.
\end{solution}

% subsubsection limiting_ratio_survival_functions (end)

\subsubsection{Hazard Rate}%
\label{ssub:hazard_rate}
% subsubsection hazard_rate

\newthought{Recall} \cref{defn:hazard_rate_function}. We had
\begin{gather*}
  h(x) = \frac{f(x)}{\bar{F}(x)} = - \frac{d}{dx} \ln \bar{F}(x), \\
  h_X(x) \Delta x \approx P(X \leq x + \Delta x \mid X > x)
\end{gather*}
and the hazard rate function relates to the survival function as
\begin{equation*}
  \bar{F}(x) = e^{-\int_{-\infty}^{x} h(y) \dif{y} }.
\end{equation*}

Notice that
\begin{itemize}
  \item if the hazard rate function is a \hlnotea{decreasing} function, that implies that the probability of the occurrence of $X \leq x + \Delta x$ decreases given $X > x$, as $x$ increases, i.e. it is more likely that we have $X > x + \Delta x \mid X > x$. So $X$ has a \hlnotea{heavy tail}.
  \item if the hazard rate function is a \hlnotea{increasing} function, that implies that the probability of the occurrence of $X \leq x + \Delta x$ increases given $X > x$, as $x$ increases, i.e. it is less likely that $X > x + \Delta x \mid X > x$. So $X$ has a \hlnotea{light tail}.
\end{itemize}

\begin{defn}[Decreasing and Increasing Failure Rates]\index{Decreasing Failure Rate}\index{Increasing Failure Rate}\label{defn:decreasing_and_increasing_failure_rates}
  Let $X$ be a loss with hazard rate function $h_X$. We say that\sidenote{The following source claims that the \hlnotea{failure rate} and hazard rate are, in fact, not always interchangable terms: \url{https://nomtbf.com/2013/11/difference-hazard-failure-rate/}. Perhaps this is worth looking into.}
  \begin{itemize}
    \item $X$ or $F_X$ has a \hlnoteb{decreasing failure rate (DFR)} if $h_X$ is decreasing;
    \item $X$ or $F_X$ has a \hlnoteb{increasing failure rate (IFR)} if $h_X$ is increasing.
  \end{itemize}
\end{defn}

\begin{note}
  Consequently,
  \begin{itemize}
    \item Distributions that have a DFR are heavy-tailed;
    \item Distributions that have an IFR are light-tailed.
  \end{itemize}
\end{note}

\begin{propo}[Exponential has Constant Hazard Rate]\label{propo:exponential_has_constant_hazard_rate}
  The exponential distribution has a constant hazard rate.
\end{propo}

\begin{proof}
  The pdf and survival function of $X \sim \Exp(\lambda)$ is
  \begin{equation*}
    f_X(x) = \lambda e^{-\lambda x} \text{ and } \bar{F}_X(x) = e^{-\lambda x},
  \end{equation*}
  respectively. Thus the hazard rate of $X$ is
  \begin{equation*}
    h(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \lambda,
  \end{equation*}
  which is a fixed value.\qed\
\end{proof}

\begin{note}
  We say that the exponential distribution is the only distribution which is said to have both DFR and IFR.\sidenote{\hlwarn{Why?}}
\end{note}

\begin{eg}
  Let $X \sim \Pareto(\alpha, \theta)$ with $f_X(x) = \frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}$ and $\bar{F}_X(x) = \frac{\theta^\alpha}{{(x + \theta)}^\alpha}$. Determine whether $X$ has a DFR or IFR.
\end{eg}

\begin{solution}
  The hazard rate function of $X$ is
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{\frac{\alpha \theta^\alpha}{{(x + \theta)}^{\alpha + 1}}}{\frac{\theta^\alpha}{{(x + \theta)}^\alpha}} = \frac{\alpha}{x + \theta}.
  \end{equation*}
  It is clear that $h_X$ is a decreasing function, and so $X \sim \Pareto(\alpha, \theta)$ has a DFR, i.e.\ it is heavy-tailed.
\end{solution}

It is not always easy to get the survival function. The following is an alternative approach to finding out if the hazard rate function is increasing or decreasing.

\begin{propo}[Ratio Comparison for DFR/IFR]\label{propo:ratio_comparison_for_dfr_ifr}
  Let $X$ be an rv, and\sidenote{\hlwarn{Any bounds on $y$?}}
  \begin{equation*}
    s(x) = \frac{f_X(x + y)}{f_X(x)}.
  \end{equation*}
  \begin{enumerate}
    \item If $s(x)$ is increasing in $x$ for every $y$, then $X$ has a DFR;
    \item If $s(x)$ is decreasing in $x$ for every $y$, then $X$ has an IFR.
  \end{enumerate}
\end{propo}

\begin{proof}
  We shall prove for one case as the other will follow analogously. Notice that
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{f_X(x)}{\int_{x}^{\infty} f_X(y) \dif{y}} = \frac{1}{\int_{0}^{\infty} \frac{f_X(x + y)}{f_X(x)}\dif{y} }
  \end{equation*}
  by a change of variable in the last equality. We notice that if $\frac{f_X(x + y)}{f_X(x)}$ is increasing, then $h_X(x)$ will be decreasing, and so $X$ has a DFR.\qed\
\end{proof}

\begin{eg}
  Let $X \sim \Gam(\alpha, \theta)$ with $\alpha > 1$. Determine whether $X$ is a DFR or IFR distribution.
\end{eg}

\begin{solution}
  The cdf of $X$ is
  \begin{equation*}
    f_X(x) = \frac{x^{\alpha - 1} e^{-\frac{x}{\theta}}}{\theta^\alpha \Gamma(\alpha)}.
  \end{equation*}
  The survival function of $X$ is not explicit, and so we should use \cref{propo:ratio_comparison_for_dfr_ifr}. We have
  \begin{align*}
    \frac{f_X(x + y)}{f_X(x)} = = \frac{ \frac{(x + y)^{\alpha - 1} e^{- \frac{x + y}{\theta}}}{\theta^{\alpha} \Gamma(\alpha)} }{\frac{ x^{\alpha - 1} e^{-\frac{x}{\theta}} }{\theta^\alpha \Gamma(\alpha)}} = \left( \frac{x + y}{x} \right)^{\alpha - 1} e^{-\frac{y}{\theta}} = \left( 1 + \frac{y}{x} \right)^{\alpha - 1} e^{-\frac{y}{\theta}}.
  \end{align*}
  To try to determine if it is increasing or decreasing, we calculate the second derivative of the ratio:
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \draw[->] (-0.5,0) -- (4, 0) node[right] {$x$};
      \draw[->] (0,-0.5) -- (0, 4) node[above] {$f(x)$};
      \draw[->,domain=0.2:4] plot ({\x},{(1 + ( 1 / \x )) * exp(-0.5)});
    \end{tikzpicture}
    \caption{Graph of $\left( 1 + \frac{y}{x} \right)^{\alpha - 1} e^{-\frac{y}{\theta}}$ for $y > -x$ and $x > 0$.}\label{fig:graph_of_left_1_y_x_right_alpha_1_e_y_theta_}
  \end{marginfigure}
  \begin{equation*}
    \frac{d}{dx} \left( 1 + \frac{y}{x} \right)^{\alpha - 1} e^{-\frac{y}{\theta}} = y (\alpha - 1)\left( 1 + \frac{y}{x} \right)^{\alpha - 2} e^{ -\frac{y}{\theta} }.
  \end{equation*}
  It is important to note that $y$ is not completely free: it is bounded below by $-x$, as if $y < -x$, then $x + y < 0$, and $f$ is undefined at these values. Also, if $y = -x$, then the ratio is simply a constant, and we cannot use \cref{propo:ratio_comparison_for_dfr_ifr} to reach a conclusion. To be able to use \cref{propo:ratio_comparison_for_dfr_ifr}, we must have $y > -x$. In this case, it is clear that the ratio is increasing as $x$ increases. Thus $X$ has an IFR.
\end{solution}

% subsubsection hazard_rate (end)

% subsection existence_of_moments_continued (end)

% section severity_distributions_tail_of_distributions_continued (end)

% chapter lecture_8_oct_02nd (end)

\chapter{Lecture 9 Oct 11th}%
\label{chp:lecture_9_oct_11th}
% chapter lecture_9_oct_11th

\section{Severity Distributions --- Tail of Distributions (Continued 2)}%
\label{sec:severity_distributions_tail_of_distributions_continued_2}
% section severity_distributions_tail_of_distributions_continued_2

\subsection{Mean Excess Loss}%
\label{sub:mean_excess_loss}
% subsection mean_excess_loss

\begin{defn}[Excess Loss Random Variable]\index{Excess Loss Random Variable}\label{defn:excess_loss_random_variable}
  For a loss rv $X$, we define the \hlnoteb{excess loss rv} as
  \begin{equation*}
    T_d = X - d \mid X > d, \quad d > 0.
  \end{equation*}
  The survival function of $T_d$ is
  \begin{align*}
    \bar{F}_{T_d}(x) &= P(T_d > x) = P(X - d > x \mid X > d) \\
                     &= \frac{P(X > x + d)}{P(X > d)} = \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)}.
  \end{align*}
\end{defn}

As defined before in \cref{defn:mean_excess_loss},

\begin{defnnonum}[Mean Excess Loss]\index{Mean Excess Loss}
  The \hlnoteb{mean excess loss} (or \hldefn{mean residual life}) function is defined as\marginnote{Essentially, the mean excess loss is the average payment in excess of the threshold $d$, given that the loss exceeds the threshold. }
  \begin{equation*}
    e_X(d) = E[T_d] = \int_{0}^{\infty} \bar{F}_{T_d}(x) \dif{x} = \frac{\int_{0}^{\infty} \bar{F}_X(x + d) \dif{x}}{\bar{F}_X(d)} = \frac{\int_{d}^{\infty} \bar{F}_X(y) \dif{y} }{\bar{F}_X(d)}
  \end{equation*}
\end{defnnonum}

\begin{defn}[Increasing and Decreasing Mean Residual Lifetime]\index{Increasing Mean Residual Lifetime}\index{Decreasing Mean Residual Lifetime}\index{IMRL}\index{DMRL}\label{defn:increasing_and_decreasing_mean_residual_lifetime}
  Given a loss rv $X$,
  \begin{enumerate}
    \item we say $X$ or $F_X$ is an \hlnoteb{increasing mean residual lifetime (IMRL)} if $e_X(x)$ is increasing in $x$;
    \item we say $X$ or $F_X$ is an \hlnoteb{decreasing mean residual lifetime (DMRL)} if $e_X(x)$ is decreasing in $x$.
  \end{enumerate}
\end{defn}

\begin{note}
  \begin{itemize}
    \item IMRL distributions are \hlnotec{heavy-tailed};
    \item DMRL distributions are \hlnotec{light-tailed}.
  \end{itemize}

  The reason of this claim should be rather clear from the context of $e_X(x)$: if $e_X(x)$ is increasing with $x$, then we expect that the survival probability of $T_d$ to be greater, and so the tail should be a heavy one. The following proposition clarifies this notion.
\end{note}

\begin{propo}[Relation between DFR/IFR and IMRL/DMRL]\label{propo:relation_between_dfr_ifr_and_imrl_dmrl}
  A DFR rv is IMRL, and an IFR rv is a DMRL.
\end{propo}

\begin{proof}
  Suppose $X$ has a DFR. The mean excess loss of $X$ is
  \begin{equation*}
    e_X(d) = \frac{\int_{0}^{\infty} \bar{F}_X(x + d) \dif{x} }{\bar{F}_X(d)} = \int_{0}^{\infty} \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)} \dif{x}.
  \end{equation*}
  Note that by the relationship between the survival function and the hazard rate\sidenote{We use the hazard rate here because it is provided by the assumption.},
  \begin{equation*}
    \frac{\bar{F}_X(x + d)}{\bar{F}_X(d)} = \frac{e^{-\int_{0}^{x + d} h_X(y) \dif{y} }}{e^{-\int_{0}^{d} h_X(y) \dif{y} }} = e^{-\int_{d}^{x + d} h_X(y) \dif{y} } = e^{-\int_{0}^{x} h_X(z + d) \dif{z} }.
  \end{equation*}
  Since $X$ has a DFR, $h_X$ is decreasing, and thus $\frac{\bar{F}_X(x + d)}{\bar{F}_X(d)}$ is increasing. Thus $e_X(d)$ is increasing and so $X$ is a IMRL, as required. THe argument is similar for $IFL$ being a $DMRL$.\qed\
\end{proof}

\begin{eg}
  Let $X \sim \Wei(\theta, \tau)$. Determine whether $X$ is DMRL or IMRL.
\end{eg}

\begin{solution}
  Since
  \begin{equation*}
    f_X(x) = \frac{\tau x^{\tau - 1} e^{-{\left( \frac{x}{\theta} \right)}^\tau}}{\theta^\tau}
  \end{equation*}
  and from \hyperref[note:weibull_survival_function]{an earlier example}, we have
  \begin{equation*}
    \bar{F}_X(x) = e^{- {\left( \frac{x}{\theta} \right)}^\tau}
  \end{equation*}
  Then the hazard rate is
  \begin{equation*}
    h_X(x) = \frac{f_X(x)}{\bar{F}_X(x)} = \frac{\tau}{\theta^\tau} x^{\tau - 1}.
  \end{equation*}
  Now if $\tau \geq 1$, then $h_X(x)$ is an increasing function, and so $X$ has an IFR, i.e. $X$ is a DMRL. if $0 < \tau \leq 1$, then $h_X(x)$ is a decreasing function, and so $X$ has a DFR, i.e. $X$ is an IMRL.
\end{solution}

\begin{eg}
  Consider a loss $X$ with $f_X(x) = (1 + 2x^2) e^{-2x}$ for $x > 0$.
  \begin{enumerate}
    \item Determine $h_X(x)$.
    \item Determine $e_X(x)$.
    \item Find $\lim\limits_{x \to \infty} h_X(x)$ and $\lim\limits_{x \to \infty} e_X(x)$.
    \item Show that $X$ is DMRL but not IFR.
  \end{enumerate}
\end{eg}

\begin{solution}
  Since both $h_X(x)$ and $e_X(x)$ require the survival function, we shall first derive that. Observe that\sidenote{It is highly recommended that one gets really used to using integration by parts, to the point that you do not have to repeatedly write down what the $u$ and $dv$ are explicitly every time.}
  \begin{align*}
    \bar{F}_X(x) &= \int_{x}^{\infty} (1 + 2y^2) e^{-2y} \dif{y} = \frac{1}{2} e^{-2x} + 2 \left[ \int_{x}^{\infty} y^2 e^{-2y} \dif{y} \right] \\
                 &= \frac{1}{2}e^{-2x} + 2 \left[ -\frac{1}{2} y^2 e^{-2y} \at{x}{\infty} + \int_{x}^{\infty} ye^{-2y} \dif{y} \right] \\
                 &= \frac{1}{2}e^{-2x} + x^2 e^{-2x} + 2 \left[ -\frac{1}{2}ye^{-2y} \at{x}{\infty} + \frac{1}{2} \int_{x}^{\infty} e^{-2y} \dif{y} \right] \\
                 &= \frac{1}{2}e^{-2x} + x^2 e^{-2x} + xe^{-2x} + \frac{1}{2}e^{-2x} \\
                 &= (x^2 + x + 1) e^{-2x}.
  \end{align*}
  \begin{enumerate}
    \item It is clear that
      \begin{align*}
        h_X(x) &= \frac{1 + 2x^2}{1 + x + x^2}
      \end{align*}

    \item By its definition, we have that
      \begin{equation*}
        e_X(x) = \frac{\int_{x}^{\infty} \bar{F}_X(y) \dif{y} }{\bar{F}_X(x)},
      \end{equation*}
      and so we need to solve for the integral in the numerator. Using pieces from our derivation of $\bar{F}_X(x)$, we obtain
      \begin{align*}
        &\int_{x}^{\infty} (1 + y + y^2) e^{-2y} \dif{y} \\
        &= \frac{1}{2}e^{-2x} + \frac{1}{2}xe^{-2x} + \frac{1}{4}e^{-2x} + \frac{1}{2}x^2 e^{-2x} + \frac{1}{2}xe^{-2x} + \frac{1}{4}e^{-2x} \\
        &= \left( 1 + x + \frac{1}{2}x^2 \right) e^{-2x}.
      \end{align*}
      Thus
      \begin{equation*}
        e_X(x) = \frac{1 + x + \frac{1}{2}x^2}{1 + x + x^2}.
      \end{equation*}

    \item The answers are straightforward\sidenote{\hlwarn{Find out why did we calculate these values.}}
      \begin{gather*}
        \lim_{x \to \infty} h_X(x) = \lim_{x \to \infty} \frac{\frac{1}{x^2} + 2}{1 + \frac{1}{x} + \frac{1}{x^2}} = 2 \\
        \lim_{x \to \infty} e_X(x) = \lim_{x \to \infty} \frac{\frac{1}{x^2} + \frac{1}{x} + \frac{1}{2}}{\frac{1}{x^2} + \frac{1}{x} + 1} = \frac{1}{2}
      \end{gather*}

    \item First, observe that
      \begin{align*}
        e_X'(x) &= \frac{(1 + x)\left( 1 + x + x^2 \right) - (1 + 2x)\left( 1 + x + \frac{1}{2}x^2 \right)}{(1 + x + x^2)} \\
                &= - \frac{x + \frac{1}{2}x^2}{(1 + x + x^2)^2},
      \end{align*}
      and we see that $e_X'(x) < 0$ for $x > 0$. Thus $X$ has a DMRL. For $h_X(x)$,
      \begin{align*}
        h_X'(x) &= \frac{4x \left( 1 + x + x^2 \right) - (1 + 2x) \left( 1 + 2x^2 \right)}{\left(1 + x + x^2\right)^2} \\
                &= \frac{2x^2 + 2x - 1}{x^4 + 2x^3 + 3x^3 + 2x + 1}.
      \end{align*}
      It may appear as if $h_X'(x)$ is positive, seeing that $x^4$ should dominate. However, notice that the \href{https://en.wikipedia.org/wiki/Quadratic_equation\#Discriminant}{discriminant} is positive:\sidenote{Lecture notes simply threw the values $1$ and $\frac{1}{2}$ for $x$ almost out of nowhere. While the result seems harmless, firstly, $x \neq 0$, since $x > 0$. In fact, since the critical point is $\sqrt{\frac{3}{4}} - \frac{1}{2} \approx 0.366$, $\frac{1}{2}$ is a value that comes after the critical point, so we would not have been able to verify without trying and failing numerous times, especially since the critical point is an irrational value.
      
      Here, we are smart and equipped with the knowledge that by solving the first derivative for $x$ by equating to $0$ allows us to find these critical points, which is indicative of a change from positive to negative, or vice versa, slope for $h_X(x)$.}
      \begin{equation*}
        2^2 - 4(2)(-1) = 12 > 0,
      \end{equation*}
      and so the numerator has a root, i.e. there are critical points on $h_X(x)$. In fact, equating the said numerator to $0$, we can obtain that $x = - \frac{1}{2} + \sqrt{\frac{3}{4}}$ (the other case is ruled out as $x > 0$). Since $h_X'(x)$ looks as if it is increasing, let's try out some values of $x$ for $0 < x < \sqrt{\frac{3}{4}} - \frac{1}{2}$. In particular, notice that
      \begin{gather*}
        h_X \left( \frac{1}{10} \right) = \frac{102}{111} \approx 0.9198 \\
        h_X \left( \frac{1}{5} \right) = \frac{27}{31} \approx 0.8710
      \end{gather*}
      but $\frac{1}{10} < \frac{1}{5}$, and so we notice that $X$ is not IFR.
  \end{enumerate}
\end{solution}

% subsection mean_excess_loss (end)

% section severity_distributions_tail_of_distributions_continued_2 (end)

% chapter lecture_9_oct_11th (end)

\chapter{Lecture 10 Oct 16th}%
\label{chp:lecture_10_oct_16th}
% chapter lecture_10_oct_16th

\section{Severity Distributions --- Policy Adjustments (Continued)}%
\label{sec:severity_distributions_policy_adjustments_continued}
% section severity_distributions_policy_adjustments_continued

Insurance policies contain various \hlnotea{adjustments} to soften the amount that insurers have to pay, to minimize moral hazards, and for various other reasons. In this section, we shall introduce some common policy adjustments.

In the following definitions, suppose that $X$ is our ground-up loss rv, and $H$ a function incurred by the adjustment.

\begin{defn}[Policy Limit]\index{Policy Limit}\label{defn:policy_limit}
  A fixed level $u > 0$ is called a \hlnoteb{policy limit} if, provided that there are no other adjustments, the insurer shall pay\sidenote{Now that this definition uses the symbol $\land$ for denoting a policy limit, I shall refrain from using the same symbol in proofs, unless if the context is clear.}
  \begin{equation*}
    H(X) = \min \{ X, u \} := X \land u = \begin{cases}
      X & X \leq u \\
      u & X \geq u
    \end{cases}.
  \end{equation*}
  \begin{marginfigure}
    \centering
    \begin{tikzpicture}
      \draw[->] (-0.5,0) -- (4,0) node[right] {$x$};
      \draw[->] (0,-0.5) -- (0,4) node[above] {$F(x)$};
      \draw[->,thick] (0, 0) -- (2, 3) -- (4, 3);
      \draw[dashed] (2, 3) -- (0, 3) node[left] {$u$};
    \end{tikzpicture}
    \caption{Typical graph of a policy limit, without other adjustments.}\label{fig:typical_graph_of_a_policy_limit_}
  \end{marginfigure}
\end{defn}

\begin{note}
  \begin{itemize}
    \item A policy limit protects the insurer from overly large losses.
    \item This is \hlimpo{noteworthy}: in practice, a policy limit may refer to \hlnotea{the maximum amount paid} by the insurer, but in this course, it is the \hlnotea{maximum loss} coverred by the insurer.
  \end{itemize}
\end{note}

\begin{defn}[Ordinary Deductible]\index{Ordinary Deductible}\label{defn:ordinary_deductible}
  A fixed level $d > 0$ is called an \hlnoteb{ordinary deductible} if, given that there are no other adjustments, the insurer pays
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (-0.5,0) -- (3,0) node[right] {$x$};
      \draw[->] (0,-0.5) -- (0,3) node[above] {$F(x)$};
      \draw[->,thick] (-0.5,0) -- (1, 0) -- (3, 2) node[right] {$H(X)$};
      \node[below] at (1, 0) {$d$};
    \end{tikzpicture}
    \caption{Graph of a policy with ordinary deductible without any other adjustments.}\label{fig:graph_of_a_policy_with_ordinary_deductible_without_any_other_adjustments}
  \end{marginfigure}
  \begin{equation*}
    Y = H(X) = {(X - d)}_+ = X \lor d = \begin{cases}
      0     & X < d \\
      X - d & X \geq d
    \end{cases}
  \end{equation*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item For any given loss, the first $d$ dollars falls on the insured.
    \item It is a protection against frequent small claims.
  \end{itemize}
\end{note}

\begin{defn}[Franchise Deductible]\index{Franchise Deductible}\label{defn:franchise_deductible}
  A fixed level $d > 0$ is called a \hlnoteb{Franchise Deductible} if, given that there are no other adjustments, the insurer pays
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (-0.5,0) -- (3,0) node[right] {$X$};
      \draw[->] (0,-0.5) -- (0,3) node[above] {$Y$};
      \draw[-,thick] (-0.5,0) -- (1, 0);
      \draw[->,thick] (1, 1) -- (2.7, 2.7) node[right] {$H(X)$};
      \draw[dotted] (1, 1) -- (1, 0) node[below] {$d$};
    \end{tikzpicture}
    \caption{Graph of a policy with Franchise deductible without any other adjustments.}\label{fig:graph_of_a_policy_with_franchise_deductible_without_any_other_adjustments}
  \end{marginfigure}
  \begin{align*}
    H(X) &= X \cdot \mathbb{1}_{\{ X > d \}} = \begin{cases}
      0 & X \leq d \\
      X & X > d
    \end{cases} \\
    &= (X - d) \mathbb{1}_{\{X > d\}} + d \cdot \mathbb{1}_{\{ X > d\}} \\
    &= {(X - d)}_+ + d \cdot \mathbb{1}_{\{X > d\}}
  \end{align*}
\end{defn}

\begin{note}
  \begin{itemize}
    \item This differs from the ordinary deductible in that twhen the loss exceeds $d$, the deductible is waived and the \hlnotec{full loss is paid} by the insurer.
    \item We are not concerned with whether the payment goes out or not at $X = d$ in this course.\sidenote{In the event that a problem of such a nature comes out in either exercises or exams, the point will be explicitly stated.}
  \end{itemize}
\end{note}

\begin{remark}
  This is not a good adjustment as it is prone to \hlnotea{moral hazard}.
\end{remark}

\begin{defn}[Coinsurance]\index{Coinsurance}\label{defn:coinsurance}
  A fixed rate $\alpha \in [0, 1]$ is called a \hlnoteb{coinsurance factor} if, given that there are no other adjustments, the insurer pays
  \begin{marginfigure}
    \begin{tikzpicture}
      \draw[->] (-0.5,0) -- (3,0) node[right] {$X$};
      \draw[->] (0,-0.5) -- (0,3) node[above] {$Y$};
      \draw[->,thick] (0, 0) -- (3, 3) node[right] {$H(X)$};
      \draw[latex'-] (1.2,1.5) -- (0.7,2) node[above] {slope $= \alpha$};
    \end{tikzpicture}
    \caption{Graph of a policy with coinsurance without any other adjustments.}\label{fig:graph_of_a_policy_with_coinsurance_without_any_other_adjustments}
  \end{marginfigure}
  \begin{equation*}
    H(X) = \alpha X.
  \end{equation*}
  For any given loss, the \hlnotec{insurer pays a proportion $100\alpha\%$} of the loss amount the remaining $100(1 - \alpha)\%$ falls on the insured.
\end{defn}

\subsection{Application Order for Multiple Adjustments}%
\label{sub:application_order_for_multiple_adjustments}
% subsection application_order_for_multiple_adjustments

\newthought{If an insurance policy} has more than one adjustment, we assume the adjustments in the following order:
\begin{itemize}
  \item Policy limit (if any)
  \item Policy/ordinary deductible (if any)
  \item Coninsurance (if any)
\end{itemize}

\begin{note}
  \begin{itemize}
    \item These transformations are not necessarily commutative, so the order must be obeyed.
    \item This ordering is optimal, i.e.\ it covers for all possible combinations, i.e.\ any other ways of adjustment can be expressed in this form.\sidenote{Claimed by lecturer. Require example.}
    \item If $d$ is a deductible and $u$ the policy limit, we must have that $d < u$, since if $u < d$, then the insurer will only pay the maximum amount $u$ if the loss exeeds $d$, which is absurd. Therefore, for all of the cases that we shall consider, we will always assume, and safely so, that $d < u$.
  \end{itemize}
\end{note}

Applying the ordering, we have
\begin{equation*}
  X \to X \land u \to {[ (X \land u) - d ]}_+ \to \alpha {[ ( X \land u ) - d ]}_+
\end{equation*}
\begin{marginfigure}
  \begin{tikzpicture}
    \draw[->] (-0.5,0) -- (3,0) node[right] {$X$};
    \draw[->] (0,-0.5) -- (0,3) node[above] {$Y$};
    \draw[-latex',thick] (-0.5,0) -- (1, 0) -- (2, 2) -- (3, 2) node[right] {$H(X)$};
    \draw[dotted] (2, 2) -- (0, 2) node[left] {$\alpha(u - d)$};
    \node[left] at (1.5,1) {slope $\alpha$};
    \node[below] at (1, 0) {$d$};
    \draw[dotted] (2, 2) -- (2, 0) node[below] {$u$};
  \end{tikzpicture}
  \caption{Graph of $H(X) = \alpha{[ (X \land u) - d ]}_+$.}\label{fig:graph_of_mutiple_adjustments}
\end{marginfigure}
and so
\begin{equation*}
  H(X) = \alpha {[ (X \land u) - d ]}_+ = \begin{cases}
    0               & X < d \\
    \alpha( X - d ) & d \leq X < u \\
    \alpha( u - d ) & X \geq u
  \end{cases}
\end{equation*}

For the case of applying \hlnotea{Franchise deductible} instead of ordinary deductible, we have
\begin{equation*}
  X \to X \land u \to ( X \land u ) \mathbb{1}_{\{X > d\}} \to \alpha(X \land u) \cdot \mathbb{1}_{\{X > d\}}
\end{equation*}
\begin{marginfigure}
  \begin{tikzpicture} \draw[->] (-0.5,0) -- (3,0) node[right] {$X$};
    \draw[->] (0,-0.5) -- (0,3) node[above] {$Y$};
    \draw[-latex',thick] (1,1) -- (2, 2) -- (3, 2) node[right] {$H(X)$};
    \draw[dotted] (0,0) -- (1,1);
    \draw[dotted] (2, 2) -- (0, 2) node[left] {$\alpha(u - d)$};
    \node[left] at (1.5,1.5) {slope $\alpha$};
    \draw[dotted] (1, 1) -- (1, 0) node[below] {$d$};
    \draw[dotted] (2, 2) -- (2, 0) node[below] {$u$};
  \end{tikzpicture}
  \caption{Graph of $H(X) = \alpha( X \land u ) \cdot \mathbb{1}_{\{X > d\}}$.}\label{fig:graph_of_mutiple_adjustments_with_franchise_deductible}
\end{marginfigure}
Notice that $X \land u \to (X \land u) \mathbb{1}_{\{X > d\}}$, since $X \land u > d$ is simply $X > d$ as $u > d$ by assumption. We have that for the case where we consider the Franchise deductible instead of an ordinary deductible,
\begin{equation*}\label{eq}
  H(X) = \alpha( X \land u ) \cdot \mathbb{1}_{\{X > d\}} = \begin{cases}
    0        & X < d \\
    \alpha X & d \leq X < u \\
    \alpha u & X \geq u
  \end{cases}
\end{equation*}

% subsection application_order_for_multiple_adjustments (end)

\subsection{\imponote\ Reporting Methods}%
\label{sub:reporting_methods}
% subsection reporting_methods

When we consider the amount paid by the insurer, we typically consider (and distinguish) between two types of reporting methods.
\begin{itemize}
  \item \hldefn{Loss basis}: $Y_L =$ amount paid per loss
  \item \hldefn{Payment basis}: $Y_P =$ amount paid per payment
\end{itemize}
It is sensible that
\begin{equation}\label{eq:relationship_of_YP_and_YL}
  Y_P = Y_L \mid Y_L > 0.
\end{equation}

The above relationship shows that we can retrieve $Y_P$ from $Y_L$, i.e. $Y_L$ holds more information than $Y_P$. This makes sense; a loss may occur, but the insurer may not have to pay for the loss. For example, if the incurred loss is below a given deductible level.

\subsubsection{Loss Basis}%
\label{ssub:loss_basis}
% subsubsection loss_basis

\begin{itemize}
  \item Each loss is recorded, i.e. each loss has an entry, even if the amount paid is $0$.
  \item For a policy limit $u$, ordinary deductible $d$ with $d < u$ \sidenote{It would be silly if $d > u$.}, and coinsurance factor $\alpha$, we have
    \begin{equation*}
      Y_L = \alpha [ ( X \land u ) - d ]_+.
    \end{equation*}
  \item For a policy with limit $u$, Franchise deductible $d$ with $d < u$, and coinsurance factor $\alpha$, we have
    \begin{equation*}
      Y_L = \alpha (X \land u) \mathbb{1}_{\{ X > d \}}.
    \end{equation*}
  \item Note that in the presence of a deductible $d$, it is \hlnotec{usually the case} that $Y_L$ has a probability mass at $0$, i.e.
    \begin{equation*}
      P(Y_L = 0) = P(X \leq d) = F_X(d) > 0.
    \end{equation*}
    In this case, $Y_P > 0$ almost surely.
\end{itemize}

% subsubsection loss_basis (end)

\subsubsection{Payment Basis}%
\label{ssub:payment_basis}
% subsubsection payment_basis

\begin{itemize}
  \item Only \hlnotea{non-zero} payments of the insurer are included, and so not every loss will have an entry. Here, we see that $Y_P$ leaves that information behind.\sidenote{It is still useful to reporting purely on the financial effects of the claims.}
  \item $Y_P$ does not have a probability mass at $0$ (\hlwarn{Why}?), i.e.
    \begin{equation*}
      P(Y_P = 0) = 0,
    \end{equation*}
    or equivalently,
    \begin{equation*}
      Y_P > 0.
    \end{equation*}
\end{itemize}

% subsubsection payment_basis (end)

\begin{eg}
  Let $X$ be the ground-up loss rv and assume that there is an ordinary deductible of $5$ applied to the loss. The following table is a typical example illustrating how $Y_L$ and $Y_P$ works.
  \begin{table}[ht]
    \centering
    \caption{Example illustrating the relationship between $Y_P$ and $Y_L$.}
    \label{tab:example_for_yp_and_yp}
    \begin{tabular}{c | c c c c c c}
    \toprule
    $X$   & 3  & 2  & 5  & 7 & 9 & 10 \\
    \midrule
    $Y_L$ & 0  & 0  & 0  & 2 & 4 & 5 \\
    $Y_P$ & NA & NA & NA & 2 & 4 & 5 \\
    \bottomrule
    \end{tabular}
  \end{table}
\end{eg}

% subsection reporting_methods (end)

% section severity_distributions_policy_adjustments_continued (end)

% chapter lecture_10_oct_16th (end)

\chapter{Lecture 11 Oct 18th}%
\label{chp:lecture_11_oct_18th}
% chapter lecture_11_oct_18th

\section{Severity Distribution --- Policy Adjustments (Continued 2)}%
\label{sec:severity_distribution_policy_adjustments_continued_2}
% section severity_distribution_policy_adjustments_continued_2

\subsection{Distribution \& Moments of $Y_P$ and $Y_L$}%
\label{sub:distribution_n_moments_of_y_p_and_y_l_}
% subsection distribution_n_moments_of_y_p_and_y_l_

It suffices for us to closely study $Y_L$ due to the following proposition:

\begin{propo}[$Y_P$ is completely determined by $Y_L$]\label{propo:_y_p_is_completely_determined_by_y_l_}
  The survival function and moments of $Y_P$ are given by
  \begin{equation*}
    \bar{F}_{Y_P}(y) = \begin{cases}
      1                                         & y < 0 \\
      \frac{\bar{F}_{Y_L}(y)}{\bar{F}_{Y_L}(0)} & y \geq 0
    \end{cases}
  \end{equation*}
  and
  \begin{equation*}
    E \left[ Y_P^k \right] = \frac{E \left[ Y_L^k \right]}{\bar{F}_{Y_L}(0)}, \quad k = 1, 2, \ldots .
  \end{equation*}
\end{propo}

\begin{proof}
  Using the definition of a survival function, we have
  \begin{align*}
    \bar{F}_{Y_P}(x) = P(Y_P > x) = P(Y_L > x \mid Y_L > 0) = \begin{cases}
      1                                         & x < 0 \\
      \frac{\bar{F}_{Y_L}(x)}{\bar{F}_{Y_L}(0)} & x \geq 0
    \end{cases}.
  \end{align*}
  Consequently,
  \begin{equation*}
    e \left[ Y_P^k \right] = k \int_{0}^{\infty} x^{k - 1} \bar{F}_{Y_P}(x) \dif{x} = k \int_{0}^{\infty} x^{k - 1} \frac{\bar{F}_{Y_L}(x)}{\bar{F}_{Y_L}(0)} \dif{x} = \frac{E \left[ Y_L^k \right]}{\bar{F}_{Y_L}(0)}.
  \end{equation*}\qed\
\end{proof}

\begin{note}
  \cref{propo:_y_p_is_completely_determined_by_y_l_} tells us that it suffices to discover the distribution of $Y_L$, since it completely determines $Y_P$,
\end{note}

\begin{remark}
  \begin{itemize}
    \item If there is a deductible $d > 0$, then the distributions of $Y_P$ and $Y_L$ are \hlnotea{usually}\sidenote{This depends on the distribution of $X$ and $Y_L$.} different.
    \item If $Y_L$ has no mass point at $0$, i.e. $\bar{F}_{Y_L}(0) = 1$, then $Y_P$ nand $Y_L$ have the same distribution.
  \end{itemize}
\end{remark}

% subsection distribution_n_moments_of_y_p_and_y_l_ (end)

\subsection{Some Important Identities}%
\label{sub:some_important_identities}
% subsection some_important_identities

The following proposition is important for us to venture forward.

\begin{propo}[\vimponote\ Expected Value of the Policy Adjustments]\label{propo:expected_value_of_the_policy_adjustments}
  Consider a non-negative rv $X$ and $d > 0$. Then
  \begin{enumerate}
    \item We have
      \begin{equation*}
        E[X] = E[ [ X - d ]_+ ] + E[ X \land d ].
      \end{equation*}
    \item For $k = 1, 2, \ldots$,\label{item:expected_value_of_policy_adjustment_base}
      \begin{equation*}
        E \left[ X^k \right] = \int_{0}^{\infty} x^{k - 1} \bar{F}_X(x) \dif{x}.
      \end{equation*}
    \item For $k = 1, 2, \ldots$,
      \begin{equation*}
        E\left[ ( X \land d )^k \right] = \int_{0}^{d} kx^{k - 1}\bar{F}_X(x) \dif{x} .
      \end{equation*}
    \item For $k = 1, 2, \ldots$,
      \begin{equation*}
        E \left[ [ X - d ]_+^k \right] = \int_{d}^{\infty} k ( x - d )^{k - 1} \bar{F}_X(x) \dif{x}.
      \end{equation*}
    \item For $k = 1, 2, \ldots$, and $\bar{F}_X(d) > 0$,
      \begin{equation*}
        E \left[ ( X - d )^k \mid X > d \right] = \frac{E \left[ [ X - d ]_+^k \right]}{\bar{F}_X(d)} = \frac{\int_{d}^{\infty} k( x -  d )^{k - 1} \bar{F}_X(x) \dif{x} }{\bar{F}_X(d)}.
      \end{equation*}
      In particular, we have an alternate way to derive the mean excess value
      \begin{equation*}
        e_X(d) = E \left[ X - d \mid X > d \right] = \frac{\int_{d}^{\infty} \bar{F}_X(x) \dif{x}}{\bar{F}_X(d)}.
      \end{equation*}
  \end{enumerate}
\end{propo}

\begin{proof}
  \begin{enumerate}
    \item For this identity, notice that
      \begin{equation*}
        [ X - d ]_+ + ( X \land d ) = \begin{cases}
          0 + X     & X \leq d \\
          X - d + d & X > d
        \end{cases} = X.
      \end{equation*}
      The result follows from linearity of $E$.

    \item We have proved this earlier on, but it shall be re-proved for exercise, variety, and ease of reference.\sidenote{\hlimpo{Important}: There are two rules that you must use, and it does not depend on any of your existing knowledge as an undergrad whatsoever.
      \begin{enumerate}
        \item Notice that $\frac{d}{dx}F_X(x) = f_X(x) \implies - d\bar{F}_X(x) = f_X(x) \dif{x}$;
        \item While using integration by parts, let $dv = d\bar{F}_X(x)$ so that $v = \bar{F}_X(x)$.
      \end{enumerate}
      Forget any one of these are prepare to be screwed over.}
      \begin{align*}
        E \left[ X^k \right] &= \int_{0}^{\infty} x^k f_X(x) \dif{x} = \int_{0}^{\infty} x^k \frac{d}{dx} F_X(x) \dif{x} \\
                             &= \int_{0}^{\infty} x^k d F_X(x) = \int_{0}^{\infty} x^k d( 1 - \bar{F}_X(x) ) \\
                             &= - \int_{0}^{\infty} x^k d \bar{F}_X(x) \\
                             &= - x^k \bar{F}_X(x) \at{0}{\infty} + k \int_{0}^{\infty} x^{k - 1} \bar{F}_X(x) \dif{x} \quad \because \text{ IBP } \\
                             &= k \int_{0}^{\infty} x^{k - 1} \bar{F}_X(x) \dif{x},
      \end{align*}
      under the assumption that $\bar{F}_X(x)$ decays faster than $x^k$ \sidenote{How unlikely is this, I do not know.}.

    \item Using a similar argument as in the earlier part of the last proof, and by the Law of the Unconscious Statistician, we can arrive at
      \begin{equation*}
        E \left[ ( X \land u )^k \right] = - \int_{0}^{\infty} (x \land u)^k \dif{\bar{F}_X(x)}.
      \end{equation*}
      To proceed, use integration by parts as follows\sidenote{This is hopeless. \hlimpo{If you can't remember this}, or somehow make some sense of this monster (without going through a few lectures on Lesbesgue or Riemann-Stieljes integration), \hlimpo{you're screwed.} }:
      \begin{gather*}
        u = ( x \land u )^k \quad v = \bar{F}_X(x) \\
        du = d(x \land u)^k \quad dv = d\bar{F}_X(x)
      \end{gather*}
      We get
      \begin{equation*}
        E \left[ ( X \land u )^k \right] = - (x \land u)^k \bar{F}_X(x) \at{0}{\infty} + \int_{0}^{\infty} \bar{F}_X(x) \dif{( x \land u )^k},
      \end{equation*}
      and $(x \land u)^k \bar{F}_X(x) \at{0}{\infty} = 0$. Next, it is a ``fact'' that
      \begin{equation*}
        d ( x \land u )^k = \begin{cases}
          k x^{k - 1} \dif{x} & x < u \\
          0                   & x \geq u
        \end{cases}
      \end{equation*}
      \sidenote{This is yet another monstrosity that makes no sense whatsoever for one that has only taken basic Calculus classes and introductory Analysis. \hlimpo{Remember this ``fact''} or \hlimpo{get screwed over}. The only ``sense'' that I can come up with right now is to consider the cases of when $x < u$ and when $x \geq u$, and determine what $x \land u$ should be in these cases, and provide some baseless rationalization.}Since $x > u$ gives us a $0$ term, we are left with
      \begin{equation*}
        E \left[ (X \land u)^k \right] = \int_{0}^{u} k x^{k-1} \bar{F}_X(x) \dif{x}.
      \end{equation*}
      
    \item Using the Law of the Unconscious Statistician and \cref{item:expected_value_of_policy_adjustment_base}, we have
      \begin{equation*}
        E \left[ [ X - d ]_+^k \right] = k \int_{0}^{\infty} (x - d)_+^k \bar{F}_X(x) \dif{x} = k \int_{d}^{\infty} (x - d)^k \bar{F}_X(x) \dif{x}.
      \end{equation*}

    \item Notice once and for all that
      \begin{align*}
        E \left[ (X - d)^k \mid X > d \right] &= \frac{E \left[ (X - d)^k \mathbb{1}_{\{ X > d \}} \right]}{\bar{F}_X(d)} \\
                                              &= \frac{E \left[ [ X - d ]_+^k \right]}{\bar{F}_X(d)} = \frac{\int_{d}^{\infty} k (x - d)^k \bar{F}_X(x) \dif{x}}{\bar{F}_X(d)}.
      \end{align*}
  \end{enumerate}\qed\
\end{proof}

\begin{eg}
  Consider a ground-up loss $X$ with pdf
  \begin{equation*}
    f_X(x) = 0.0005, \quad 0 \leq x \leq 20.
  \end{equation*}
  Solve for
  \begin{enumerate}
    \item $\bar{F}_X(x)$;
    \item $E [ X \land 10 ]$ and $E[ X \land 25 ]$;
    \item $\Var(X \land 10)$;
    \item $E \left[ [ X - 10 ]_+ \right]$;
    \item $E \left[ [ X - 10 ]_+^2 \right]$; and
    \item $e_X(10)$.
  \end{enumerate}
\end{eg}

\begin{solution}
  \begin{enumerate}
    \item We have
      \begin{equation*}
        F_X(x) = \int_{0}^{x} 0.005y \dif{y} = 0.0025x^2
      \end{equation*}
      and so
      \begin{equation*}
        \bar{F}_X(x) = \begin{cases}
          1             & x < 0 \\
          1 - 0.0025x^2 & 0 \leq x \leq 20 \\
          0             & x > 20
        \end{cases}.
      \end{equation*}

    \item Using our identities,
      \begin{equation*}
        E [ X \land 10 ] = \int_{0}^{10} \bar{F}_X(x) \dif{x} = 10 - \frac{5}{6000} (10)^3 = \frac{55}{6},
      \end{equation*}
      and
      \begin{equation*}
        E [ X \land 25 ] = \int_{0}^{20} \bar{F}_X(x) \dif{x} = 25 - \frac{5}{6000} (25)^3 = \frac{40}{3}.
      \end{equation*}
    \item To get $\Var(X \land 10)$, we first need the 2nd moment of $X \land 10$:
      \begin{align*}
        E \left[ \left( X \land 10 \right)^2 \right] &= 2 \int_{0}^{10} x \bar{F}_X(x) \dif{x} = 2 \left[ \frac{1}{2} x^2 - \frac{5}{8000} x^4 \right]_{0}^{10} \\
                                                     &= 100 - \frac{25}{2} = \frac{175}{2}.
      \end{align*}
      Thus
      \begin{equation*}
        \Var \left( X \land 10 \right) = \frac{175}{2} - \left( \frac{55}{6} \right)^2 = \frac{125}{36}.
      \end{equation*}

    \item We have that
      \begin{align*}
        E \left[ [ X - 10 ]_+ \right] &= \int_{10}^{20} 1 - \frac{1}{400}x^2 \dif{x} \\
                                      &= 10 - \frac{1}{1200} (8000 - 1000) = \frac{25}{6}.
      \end{align*}

    \item We have that
      \begin{align*}
        E \left[ [ X - 10 ]_+^2 \right] &= 2 \int_{10}^{20} ( x - 10 ) \left( 1 - \frac{1}{400}x^2 \right) \dif{x} \\
                                        &= 2 \int_{10}^{20} \left( -10 + x + \frac{1}{40} x^2 - \frac{1}{400} x^3 \right) \dif{x} \\
                                        &= 2 \left[ -100 + \frac{300}{2} + \frac{7000}{120} - \frac{150000}{1600} \right] \\
                                        &= \frac{175}{6}
      \end{align*}

    \item We have
      \begin{equation*}
        e_X(10) = \frac{\int_{10}^{20} \left( 1 - \frac{1}{400} x^2 \right) \dif{x}}{1 - \frac{1}{400} (10)^2} = \frac{10 - \frac{7000}{1200}}{\frac{3}{4}} = \frac{50}{9}
      \end{equation*}
  \end{enumerate}
\end{solution}

\subsubsection{Application of \cref{propo:expected_value_of_the_policy_adjustments}}%
\label{ssub:application_of_expected_value_of_the_policy_adjustments}
% subsubsection application_of_expected_value_of_the_policy_adjustments

\paragraph{Policy Limit} If a policy limit $u$ is the only adjustment in a contract, then
\begin{equation*}
  Y_L = X \land u \quad \text{ and } \quad Y_P = X \land u \mid X > 0.
\end{equation*}
Since in most cases $X > 0$, we have that $\bar{F}_X(0) = 1$, and so $Y_L = Y_P = X \land u$.
\begin{itemize}
  \item The survival function is
    \begin{equation*}
      \bar{F}_{Y_P} (y) = \bar{F}_{Y_L} (y) = P ( X \land u > y ) = \begin{cases}
        1            & y < 0 \\
        \bar{F}_X(y) & 0 \leq y < u \\
        0            & y \geq u
      \end{cases}
    \end{equation*}
  \item The expected value is
    \begin{equation*}
      E \left[ Y_P \right] = E \left[ Y_L \right] = E [ X \land u ] = \int_{0}^{u} \bar{F}_X(x) \dif{x}.
    \end{equation*}
  \item The second moment is
    \begin{equation*}
      E \left[ Y_P^2 \right] = E \left[ Y_L^2 \right] = E \left[ (X \land u)^2 \right] = 2 \int_{0}^{u} x \bar{F}_X(x) \dif{x}
    \end{equation*}
\end{itemize}

\paragraph{Ordinary Deductible} If an ordinary deductible $d$ is the only adjustmnet in a contract, then
\begin{equation*}
  Y_L = [ X - d ]_+ = (X - d) \mathbb{1}_{\{X > d\}}
\end{equation*}
and
\begin{equation*}
  Y_P = [ X - d ]_+ \mid [ X - d ]_+ > 0 = ( X - d ) \mathbb{1}_{\{X > d\}} \mid X > d = X - d \mid X > d
\end{equation*}
In most cases, since $\bar{F}_X(d) = P(X > d) < 1$ as it is $P(X < d) \neq 0$, the distribution of $Y_L$ and $Y_P$ differs.
\begin{itemize}
  \item The survival function is
    \begin{equation*}
      \bar{F}_{Y_L}(y) = P( [ X - d ]_+ > y ) = \begin{cases}
        1 & y < 0 \\
        \bar{F}_X(y + d) & y \geq 0
      \end{cases}
    \end{equation*}
    and
    \begin{equation*}
      \bar{F}_{Y_P}(y) = \frac{\bar{F}_{Y_L}(y)}{\bar{F}_{Y_L}(0)} = \begin{cases}
        1                                       & y < 0 \\
        \frac{\bar{F}_{X}(d + y)}{\bar{F}_X(d)} & y \geq 0
      \end{cases}
    \end{equation*}
    for $Y_L$ and $Y_P$ respectively.

  \item The mean is
    \begin{equation*}
      E \left[ Y_L \right] = E \left[ [ X - d ]_+ \right] = \int_{d}^{\infty} \bar{F}_X(x) \dif{x}
    \end{equation*}
    and
    \begin{equation*}
      E \left[ Y_P \right] = E \left[ X - d \mid X > d \right] = \frac{E [ (X - d) \mathbb{1}_{\{X > d\}} ]}{\bar{F}_X(d)} = \frac{\int_{d}^{\infty} \bar{F}_X(x) \dif{x}}{\bar{F}_X(d)}
    \end{equation*}
    for $Y_L$ and $Y_P$ respectively.

  \item The second moment is
    \begin{equation*}
      E \left[ Y_L^2 \right] = E \left[ [ X - d ]_+^2 \right] = 2 \int_{d}^{\infty} x \bar{F}_X(x) \dif{x}
    \end{equation*}
    and
    \begin{equation*}
      E \left[ Y_P^2 \right] = \frac{E \left[ Y_L^2 \right]}{\bar{F}_{Y_L}(0)} = \frac{2 \int_{d}^{\infty} x \bar{F}_X(x) \dif{x}}{\bar{F}_X(d)}
    \end{equation*}
    for $Y_L$ and $Y_P$ respectively.
\end{itemize}

% subsubsection application_of_expected_value_of_the_policy_adjustments (end)

% subsection some_important_identities (end)

% section severity_distribution_policy_adjustments_continued_2 (end)

% chapter lecture_11_oct_18th (end)

\chapter{Lecture 12 Oct 23rd}%
\label{chp:lecture_12_oct_23rd}
% chapter lecture_12_oct_23rd

\section{Severity Distribution --- Policy Adjustments (Continued 3)}%
\label{sec:severity_distribution_policy_adjustments_continued_3}
% section severity_distribution_policy_adjustments_continued_3

\subsection{Some Important Identities (Continued)}%
\label{sub:some_important_identities_continued}
% subsection some_important_identities_continued

\subsubsection{Application of \cref{propo:expected_value_of_the_policy_adjustments} (Continued)}%
\label{ssub:application_of_expected_value_of_the_policy_adjustments_continued}
% subsubsection application_of_expected_value_of_the_policy_adjustments_continued

\paragraph{Policy Limit + Ordinary Deductible} If there is a policy limit $u$ and an ordinary deductible $d$ with $u > d$, then
\begin{equation*}
  Y_L = [ ( X \land u ) - d ]_+ = \begin{cases}
    0 & X < d \\
    X - d & d \leq X < u \\
    u - d & X \geq u
  \end{cases}
\end{equation*}
and so its survival function is\sidenote{\hlimpo{Important}: Pay attention to the notion here: we are using the actual definition of a deductible to arrive at an explicit solution.}
\begin{align*}
  \bar{F}_{Y_L}(y) & = P ( Y_L > y ) = P ( [ (X \land u) - d ]_+ > y ) \\
                   & = P ( \max\{ 0, (X \land u) - d \} > y ) \\
                   & = \begin{cases}
                     1                        & y < 0 \\
                     P( (X \land u) - d > y ) & y \geq 0
                   \end{cases} \\
                   &= \begin{cases}
                     1            & y < 0 \\
                     P(X > y + d) & 0 \leq y < u - d \\
                     P(u > y + d) & y \geq u - d
                   \end{cases} \\
                   &= \begin{cases}
                     1                & y < 0 \\
                     \bar{F}_X(y + d) & 0 \leq y < u - d \\
                     0                & y \geq u - d
                   \end{cases}
\end{align*}
Consequently, its moments are
\begin{align*}
  E \left[ Y_L^k \right] &= \int_{0}^{\infty} k y^{k - 1} \bar{F}_{Y_L}(y) \dif{y} \\
                         &= \int_{0}^{u - d} k y^{k - 1} \bar{F}_X(y + d) \dif{y} \\
                         &= \int_{d}^{u} k ( y - d )^{k - 1} \bar{F}_X(y) \dif{y} 
\end{align*}
For $Y_P$, we have
\begin{align*}
  Y_P &= Y_L \mid Y_L > 0 = [ (X \land u) - d ]_+ \mid [ (X \land u) - d ]_+ > 0 \\
      &= (X \land u) - d \mid X > d.
\end{align*}
Thus by \cref{propo:_y_p_is_completely_determined_by_y_l_},
\begin{equation*}
  \bar{F}_{Y_P}(y) = \begin{cases}
      1 & y < 0 \\
      \frac{\bar{F}_{Y_L}(y)}{\bar{F}_{Y_L}(0)} & 0 \leq y < u - d \\
      0 & y \geq u - d
    \end{cases}
\end{equation*}
Using the same proposition, the moments of $Y_P$ are
\begin{equation*}
  E \left[ Y_P^k \right] = \frac{E \left[ Y_L^k \right]}{\bar{F}_X(d)} = \frac{\int_{d}^{u} k ( y - d )^{k - 1} \bar{F}_X(y) \dif{y}}{\bar{F}_X(d)}.
\end{equation*}

\paragraph{Franchise Deductible} Given a Franchise Deductible $d > 0$, we have
\begin{equation*}
  Y_L = X \cdot \mathbb{1}_{\{X > d\}}.
\end{equation*}
Its survival function is
\begin{align*}
  \bar{F}_{Y_L}(y) &= P ( X \cdot \mathbb{1}_{\{X > d\}} > y ) = \begin{cases}
                     1 & y < 0 \\
                     P ( X \cdot \mathbb{1}_{\{X > d\}} > y ) & y \geq 0
                   \end{cases} \\
                   &= \begin{cases}
                     1 & y < 0 \\
                     \bar{F}_X(d) & 0 \leq y < d \\
                     \bar{F}_X(y) & y \geq d
                   \end{cases}.
\end{align*}
For the case when $0 \leq y < d$, it is clear that in order for $X \cdot \mathbb{1}_{\{X > d\}} > y$, we first need $X > d$. Now the moments of $Y_L$ are
\begin{align*}
  E \left[ Y_L^k \right] &= \int_{0}^{\infty} ky^{k - 1} \bar{F}_{Y_L}(y) \dif{y} \\
                         &= \bar{F}_X(d) \int_{0}^{d} ky^{k - 1} \dif{y} + \int_{d}^{\infty} k y^{k - 1} \bar{F}_X(y) \dif{y} \\
                         &= d^k \bar{F}_X(d) + \int_{d}^{\infty} ky^{k - 1} \bar{F}_X(y) \dif{y}.
\end{align*}
Now observe that
\begin{equation*}
  Y_P = Y_L \mid Y_L > 0 = X \cdot \mathbb{1}_{\{X > d\}} \mid X \cdot \mathbb{1}_{\{X > d\}} > 0 = X \mid X > d
\end{equation*}
Again, using \cref{propo:_y_p_is_completely_determined_by_y_l_}, 
\begin{equation*}
  \bar{F}_{Y_P}(y) = \frac{\bar{F}_{Y_L}(y)}{\bar{F}_{Y_L}(0)} = \begin{cases}
    1                                 & y < d \\
    \frac{\bar{F}_X(y)}{\bar{F}_X(d)} & y \geq d
  \end{cases}
\end{equation*}
and its moments are
\begin{align*}
  E \left[ Y_P^k \right] &= \frac{E \left[ Y_L^k \right]}{\bar{F}_X(d)} = \frac{d^k \bar{F}_X(d) + \int_{d}^{\infty} ky^{k - 1} \bar{F}_X(y) \dif{y}}{\bar{F}_X(d)} \\
                         &= d^k + \frac{\int_{d}^{\infty} ky^{k - 1} \bar{F}_X(y) \dif{y}}{\bar{F}_X(d)}.
\end{align*}

\paragraph{Coinsurance} Let $\hat{Y_L}$ be the amount to-be-paid per loss without coinsurance. Let $\alpha \in (0, 1)$ be a coinsurance factor. Applying coinsurance for adjustment, we have that
\begin{equation*}
  Y_L = \alpha \hat{Y_L}.
\end{equation*}
The survival function of $Y_L$ is
\begin{equation*}
  \bar{F}_{Y_L}(y) = P\left( \alpha \hat{Y_L} > y \right) = P\left( \hat{Y_L} > \frac{y}{\alpha} \right) = \begin{cases}
    1 & y < 0 \\
    \bar{F}_{\hat{Y_L}} \left( \frac{y}{\alpha} \right) & y \geq 0
  \end{cases}
\end{equation*}
and its moments are
\begin{equation*}
  E \left[ Y_L^k \right] = E \left[ \alpha^k \hat{Y_L}^k \right] = \alpha^k E \left[ \hat{Y_L}^k \right].
\end{equation*}
Then for $Y_P$, we have that its survival function is
\begin{equation*}
  \bar{F}_{Y_P}(y) = \frac{\bar{F}_{Y_L}(y)}{\bar{F}_{Y_L}(0)} = \frac{\bar{F}_{\hat{Y_L}}\left( \frac{y}{\alpha} \right)}{\bar{F}_{\hat{Y_L}}(0)}, \quad y \geq 0,
\end{equation*}
and its moments
\begin{equation*}
  E \left[ Y_P^k \right] = \frac{E \left[ Y_L^k \right]}{\bar{F}_{\hat{Y_L}}(0)} = \frac{\alpha^k E \left[ \hat{Y_L}^k \right]}{\bar{F}_{\hat{Y_L}}(0)}.
\end{equation*}

\begin{eg}
  The cdf of a ground-up loss $X$ is given by
  \begin{equation*}
    F_X(x) = 1 - \left( 1 - \frac{x}{800} \right)^2, \quad 0 \leq x \leq 800.
  \end{equation*}
  Assuming a policy limit of $600$, an ordinary deductible of $200$, and a coinsurance facotr of $0.8$, determine the cdf of $Y_L$ and the expected amount paid per loss $E\left[ Y_L \right]$.
\end{eg}

\begin{solution}
  We are given
  \begin{equation*}
    Y_L = 0.8 [ (X \land 600) - 200 ]_+.
  \end{equation*}
  The survival function of $Y_L$ is
  \begin{align*}
    \bar{F}_{Y_L}(y) &= P \left( \max \{ 0, (X \land 600) - 200\} > \frac{y}{0.8} \right) \\
      &= \begin{cases}
        1 & y < 0 \\
        P \left( X \land 600 > \frac{5y}{4} + 200 \right) & y \geq 0
      \end{cases} \\
      &= \begin{cases}
        1 & y < 0 \\
        P \left( X > \frac{5y}{4} + 200 \right) & 0 \leq y < 0.8(600 - 200) = 320 \\
        0 & y \geq 320
      \end{cases} \\
      &= \begin{cases}
        1 & y < 0 \\
        \bar{F}_X \left( \frac{5y}{4} + 200 \right) & 0 \leq y < 320 \\
        0 & y \geq 320
      \end{cases}
  \end{align*}
  Thus the cdf of $Y_L$ is
  \begin{equation*}
    \bar{F}_{Y_L}(y) = \begin{cases}
      0 & y < 0 \\
      1 - \bar{F}_X \left( \frac{5y}{4} + 200 \right) & 0 \leq y < 320 \\
      1 & y \geq 320
    \end{cases}.
  \end{equation*}
  The expected amount paid per loss is
  \begin{equation*}
    E \left[ Y_L \right] = 0.8 \int_{200}^{600} \left( 1 - \frac{x}{800} \right)^2 \dif{x} = \frac{260}{3}
  \end{equation*}
\end{solution}

\begin{eg}
  Consider a ground-up loss $X$ with a Franchise deductible $d$. You are given that
  \begin{itemize}
    \item $15\%$ of the losses are below the Franchise deductible $d$,
    \item the mean excess loss $e_X(d) = 50$,
    \item the expected amount paid per loss is $51$.
  \end{itemize}
  Determine the value of $d$.
\end{eg}

\begin{solution}
  We are given
  \begin{gather*}
    Y_L = X \cdot \mathbb{1}_{\{X > d\}} \\
    P( X < d ) = 0.15
  \end{gather*}
  Thus $P(X > d) = 0.85$. We have
  \begin{equation*}
    \bar{F}_{Y_L}(y) = \begin{cases}
      1 & y < 0 \\
      \bar{F}_X(d) & 0 \leq y < d \\
      \bar{F}_X(y) & y \geq d
    \end{cases} = \begin{cases}
      1 & y < 0 \\
      0.85 & 0 \leq y < d \\
      \bar{F}_X(y) & y \geq d
    \end{cases}.
  \end{equation*}
  We are given
  \begin{equation*}
    50 = e_X(d) = \frac{\int_{d}^{\infty} \bar{F}_X(y) \dif{y}}{\bar{F}_X(d)},
  \end{equation*}
  and so
  \begin{equation*}
    \int_{d}^{\infty} \bar{F}_X(y) \dif{y} = 50 * 0.85 = 42.5.
  \end{equation*}
  We are given
  \begin{align*}
    51 &= E [ Y_L ] = \int_{0}^{\infty} \bar{F}_X(y) \dif{y} = d\bar{F}_X(d) + \int_{d}^{\infty} \bar{F}_X(y) \dif{y} \\
       &= 0.85d + 42.5.
  \end{align*}
  Thus
  \begin{equation*}
    d = \frac{8.5}{0.85} = 10.
  \end{equation*}
\end{solution}

% subsubsection application_of_expected_value_of_the_policy_adjustments_continued (end)

% subsection some_important_identities_continued (end)

% section severity_distribution_policy_adjustments_continued_3 (end)

% chapter lecture_12_oct_23rd (end)

\chapter{Lecture 13 Oct 25th}%
\label{chp:lecture_13_oct_25th}
% chapter lecture_13_oct_25th

\section{Severity Distribution --- Policy Adjustments (Continued 4)}%
\label{sec:severity_distribution_policy_adjustments_continued_4}
% section severity_distribution_policy_adjustments_continued_4

By introducing policy adjustments, it is within our interest to determine if the introduced adjustments have helped to eliminate the expected propotion of loss.

\begin{defn}[Loss Elimination Ratio]\index{Loss Elimination Ratio}\label{defn:loss_elimination_ratio}
  The \hlnoteb{loss elimination ratio}, denoted as \hldefn{$\LER$}, is the ratio of which loss has been mitigated, or eliminated, as a result of policy adjustments, and it is given by
  \begin{equation*}
    \LER = \frac{E[ X - Y_L ]}{E[X]} = 1 - \frac{E[Y_L]}{E[X]},
  \end{equation*}
  where $\frac{E[Y_L]}{E[X]}$ corresponds to the percentage of loss retained by the insurer.
\end{defn}

\begin{eg}
  For a policy that has only an ordinary deductible, i.e. $Y_L = {[X - d]}_+$, we have
  \begin{equation*}
    \LER = 1 - \frac{E( {[X - d]}_+ )}{E[X]} = 1 - \frac{E[X] - E[X \land d]}{E[X]} = \frac{E[X \land d]}{E[X]}.
  \end{equation*}
\end{eg}

\begin{eg}
  Consider a ground-up loss $X \sim \Pareto(\alpha, \theta)$ with $\alpha = 2$ and $\theta = 1000$.
  \begin{enumerate}
    \item Calculate the $\LER$ if an ordinary deductible of $500$ is applied.
    \item What is the required value of $d$ to eliminate $20\%$ of the loss?
  \end{enumerate}

  \begin{solution}
    \begin{enumerate}
      \item Note that
        \begin{equation*}
          \bar{F}_X(x) = \frac{\theta^\alpha}{(x + \theta)^\alpha}.
        \end{equation*}
        Now
        \begin{align*}
          E[X] &= \int_{0}^{\infty} \bar{F}_X(x) \dif{x} = \theta^\alpha \int_{0}^{\infty} \frac{1}{(x + \theta)^\alpha} \dif{x} \\
               &= \frac{\theta^\alpha}{1 - \alpha} \cdot \frac{1}{( x + \theta )^{ \alpha - 1 }} \at{0}{\infty} = \frac{\theta}{\alpha - 1}.
        \end{align*}
        and
        \begin{align*}
          E[X \land d] &= \int_{0}^{d} \bar{F}_X(x) \dif{x} = \frac{\theta^\alpha}{1 - \alpha}\cdot \frac{1}{(x + \theta)^{ \alpha - 1 }} \at{0}{d} \\
                       &= \frac{\theta^\alpha}{1 - \alpha} \left( \frac{1}{(d + \theta)^{ \alpha - 1 }} - \frac{1}{\theta^{\alpha - 1}} \right) \\
                       &= \frac{\theta}{\alpha - 1} \left( 1 - \left( \frac{\theta}{d + \theta} \right)^{\alpha - 1} \right).
        \end{align*}
        Thus
        \begin{equation*}
          \LER = \frac{E[X \land d]}{E[X]} = \left( 1 - \left( \frac{\theta}{d + \theta} \right)^{\alpha - 1} \right) = \frac{1}{3}.
        \end{equation*}

        In other words, $\frac{1}{3}$ is mitigated by setting an ordinary deductible of $500$.

      \item In this case, let $\LER = 0.2 = \frac{1}{5}$. Then
        \begin{equation*}
          \frac{1}{5} = 1 - \frac{1000}{d + 1000} \iff \frac{1000}{d + 1000} = \frac{4}{5} \iff d = 250
        \end{equation*}
    \end{enumerate}
  \end{solution}
\end{eg}

% section severity_distribution_policy_adjustments_continued_4 (end)

\section{Frequency Distributions --- Basic Frequency Distributions}%
\label{sec:frequency_distributions_basic_frequency_distributions}
% section frequency_distributions_basic_frequency_distributions

Recall from our \hyperref[defn:collective_risk_model]{Collective Risk Model} that
\begin{equation*}
  S = \sum_{i=1}^{N} X_i,
\end{equation*}
where
\begin{align*}
  X_i &\equiv \text{ size of the } i\text{\textsuperscript{th} claim, modelled by severity distributions} \\
N &\equiv \text{ a nonnegative integer-valued rv that represents the number of } \\
  &\quad\; \text{ claims, modelled by frequency distributions }
\end{align*}

\begin{defn}[Counting Distributions and RVs]\index{Counting Distribution}\index{Counting Random Variables}\label{defn:counting_distributions_and_rvs}
  A nonnegative rv, usually represented by $N$, is called a \hlnoteb{counting rv} and its distribution is called a \hlnoteb{counting distirbution}.
\end{defn}

\begin{note}
  For this section, the \hyperref[defn:probability_generating_function]{pgf} is important.
\end{note}

\paragraph{Importance of PGF} Given $G(t) = E\left[ t^N \right] = \sum_{k=0}^{\infty} t^K p_k$, provided that the moments exist, we have\marginnote{Also for $(*)$: The derivation is
\begin{align*}
  &G_N^{(n)}(t) = \frac{d^n}{dt^n} G_N(t) \\
  &= \sum_{k=0}^{\infty} \frac{d^n}{dt^n} t^k p_k \\
  &= \sum_{k=0}^{\infty} k(k - 1) \hdots (k - n + 1) t^{k - n} p_k \\
  &= E \left[ \prod_{k=1}^{n} ( N - k + 1 ) t^{N - n} \right]
\end{align*}}
\begin{align*}
  G^{(n)}(t) &= \frac{d^n}{dt^n} G(t) \overset{(*)}{=} E\left[ \prod_{i=1}^{n} (N - i + 1)t^{N - n} \right] \\
             &= \sum_{k=0}^{\infty} \prod_{i=1}^{n} (k - i + 1) t^{k - n} p_k \\
             &\overset{(**)}{=} \sum_{k=n}^{\infty} \prod_{i=1}^{n} (k - i + 1) t^{k - n} p_k
\end{align*}
where $(*)$ is because the moments exist, and $(**)$ is because for $k = 0, 1, \ldots, n - 1$, the product $\prod_{i=1}^{n} (k - i + 1) = 0$.

We can obtain the pmf of $N$ from the pgf by
\begin{align}
  G^{(n)}(0) &= \sum_{k=n}^{\infty} \prod_{i=1}^{n} (k - i + 1) t^{k - n} p_k \at{t = 0}{} \nonumber \\
             &= \prod_{i=1}^{n} ( n - i + 1 ) p_n = n! p_n \label{eq:pmf_from_pgf}
\end{align}
where we notice in \cref{eq:pmf_from_pgf} that only the $n$\textsuperscript{th} term survives as $t^{n - n} = 1$.

\hlnoteb{Factorial Moments}\sidenote{
  \begin{margindefn}[Factorial Moments from PGF]\label{defn:factorial_moments_from_PGF}
    We can obtain the \hlnoteb{factorial moments} of an rv $X$ from its pgf. In particular,
    \begin{equation*}
      G^{(n)}(1) = E \left[ \prod_{i=1}^{n} (X - i + 1) \right]
    \end{equation*}
    where $n \in \mathbb{N} \setminus \{ 0 \}$.
  \end{margindefn}
} can be obtained by
\begin{equation*}
  G^{(n)}(1) = E\left[ \prod_{i=1}^{n} ( N - i + 1 ) \right], \quad n = 1, 2, 3, \ldots.
\end{equation*}
In particular, we have that
\begin{equation*}
  G'(1) = E[N] \text{ and } G''(1) = E[N(N - 1)] = E(N^2) - E(N)
\end{equation*}
and so
\begin{equation*}
  \Var(N) = G''(1) + G'(1) - G'(1)^2.
\end{equation*}

\subsection{Frequency Distributions}%
\label{sub:frequency_distributions}
% subsection frequency_distributions

\subsubsection{Poisson Distribution}%
\label{ssub:poisson_distribution}
% subsubsection poisson_distribution

\begin{defn}[Poisson Distribution]\index{Poisson Distribution}\label{defn:poisson_distribution}
  A counting rv $N$ is said to have a \hlnoteb{Poisson distribution} with parameter $\lambda$, and denote $N \sim \Poi(\lambda)$, if it has the pmf
  \begin{equation*}
    p_k = P(N = k) = \frac{e^{-\lambda} \lambda^k}{k!}, \quad k = 0, 1, 2, \ldots.
  \end{equation*}
\end{defn}

\begin{remark}
  We can easily verify that the Poisson distribution is indeed a probability distribution, by noticing that
  \begin{equation*}
    \sum_{k=0}^{\infty} p_k = \sum_{k=0}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} = e^{-\lambda} e^{\lambda} = 1
  \end{equation*}
  where we used the \hlnotea{Taylor expansion} $e^{x} = \sum_{k=0}^{\infty} \frac{x^k}{k!}$.
\end{remark}

\begin{propo}[PGF, Mean, and Variance of Poisson Distribution]\label{propo:pgf_mean_and_variance_of_poisson_distribution}
  For $N \sim \Poi(\lambda)$, its pgf is
  \begin{equation*}
    G(t) = e^{\lambda (t - 1)},
  \end{equation*}
  and its mean and variance are
  \begin{equation*}
    E(N) = \Var(N) = \lambda.
  \end{equation*}
\end{propo}

\begin{proof}
  Notice that
  \begin{equation*}
    G(t) = E\left[ t^N \right] = \sum_{k=0}^{\infty} t^k p_k = \sum_{k=0}^{\infty} \frac{e^{-\lambda} (t \lambda)^k}{k!} = e^{-\lambda} e^{t \lambda} = e^{\lambda( t - 1 )}.
  \end{equation*}
  Thus
  \begin{equation*}
    E[N] = G'(1) = \lambda \text{ and } G''(1) = \lambda^2,
  \end{equation*}
  and so
  \begin{equation*}
    \Var(N) = \lambda^2 + \lambda - \lambda^2 = \lambda.
  \end{equation*}\qed\
\end{proof}

\begin{propo}[Sum of Independent Poisson RVs]\label{propo:sum_of_independent_poisson_rvs}
  If $N_1, N_2, \ldots, N_m$ are \hlnotea{independent} Poisson rvs with parameters $\lambda_1, \lambda_2,$ $\ldots, \lambda_m$ respectively, then
  \begin{equation*}
    N = \sum_{i=1}^{m} N_i \sim \Poi \left( \sum_{i=1}^{m} \lambda_i \right)
  \end{equation*}
\end{propo}

\begin{proof}
  Using the pgf method for $N$, we see that
  \begin{align*}
    G(t) &= e\left[ t^N \right] = E\left[ t^{\sum_{i=1}^{m} N_i} \right] = \prod_{i=1}^{m} E\left[t^{N_i}\right] \\
         &= \prod_{i=1}^{m} e^{\lambda_i ( t - 1 )} = e^{( t - 1 ) \sum_{i=1}^{m} \lambda_i}
  \end{align*}
  which is the pgf of $\Poi\left( \sum_{i=1}^{m} \lambda_i \right)$ as required.\qed\
\end{proof}

% subsubsection poisson_distribution (end)

% subsection frequency_distributions (end)

% section frequency_distributions_basic_frequency_distributions (end)

% chapter lecture_13_oct_25th (end)

\chapter{Lecture 14 Oct 30th}%
\label{chp:lecture_14_oct_30th}
% chapter lecture_14_oct_30th

\section{Frequency Distribution --- Basic Frequency Distributions (Continued)}%
\label{sec:frequency_distribution_basic_frequency_distributions_continued}
% section frequency_distribution_basic_frequency_distributions_continued

\subsection{Frequency Distributions (Continued)}%
\label{sub:frequency_distributions_continued}
% subsection frequency_distributions_continued

\subsubsection{Poisson Distribution (Continued)}%
\label{ssub:poisson_distribution_continued}
% subsubsection poisson_distribution_continued

\begin{propo}[Splitting a Poisson Distribution]\label{propo:splitting_a_poisson_distribution}
  Suppose that the total number of claim arrivals follows $N \sim \Poi(\lambda)$. There are $m$ distinct types of claims. Given a claim occurs, it is of type $i$ with probability $p_i$ such that
  \begin{equation*}
    p_1 + \hdots + p_m = 1.
  \end{equation*}
  Then, for each fixed $i = 1, \ldots, m$, the number of claims of type $i$, $N_i \sim \Poi(\lambda p_i)$. Furthermore, $N_1, N_2, \ldots, N_m$ are independent.
\end{propo}

\begin{note}
  The above proposition can be visualized using a tree.
  \begin{figure}[h]
    \centering
    \begin{tikzpicture}
      \node[label={180:{$N \sim \Poi(\lambda)$}}] at (0, 0) {};
      \node[draw,circle,fill,inner sep=1pt,label={180:{A claim}}] (A) at (0, 0.5) {};
      \node[draw,circle,fill,inner sep=1pt,label={0:{Type 1}}] (one) at (1, 2) {};
      \node[draw,circle,fill,inner sep=1pt,label={0:{Type 2}}] (two) at (1, 1) {};
      \node (vdots) at (1, 0) {$\vdots$};
      \node[draw,circle,fill,inner sep=1pt,label={0:{Type m}}] (m) at (1, -1) {};
      \draw[-latex'] (A) -- (one);
      \draw[-latex'] (A) -- (two);
      \draw[-latex'] (A) -- (vdots);
      \draw[-latex'] (A) -- (m);
      \node[label={0:{w/ $p_1 \quad \to N_1 \sim \Poi(\lambda p_1)$}}] at (3, 2) {};
      \node[label={0:{w/ $p_2 \quad \to N_2 \sim \Poi(\lambda p_2)$}}] at (3, 1) {};
      \node[label={0:{w/ $p_m \quad \to N_m \sim \Poi(\lambda p_m)$}}] at (3, -1) {};
    \end{tikzpicture}
    \caption{Visualization of \cref{propo:splitting_a_poisson_distribution}}
    \label{fig:visualization_of_splitting_a_poisson_distribution}
  \end{figure}
\end{note}

\begin{proof}
  We shall use \hlnotea{mathematical induction} on $m$, for the statement ``$N_1, N_2, \ldots, N_m$ are independent''. $N_i \sim \Poi(\lambda p_i)$ will follow from the induction step.

  For $m = 1$, there is nothing to prove. It suffices to prove for $m = 2$, since we may think of the problem as
  \begin{equation*}
    \text{Type 1} \quad \underbrace{\text{Type 2} \quad \text{Type 3} \hdots \text{Type m}}_{\text{Type 2'}}.
  \end{equation*}

  \noindent\hlbnotea{Case $m = 2$} Suppose $N = N_1 + N_2 \sim \Poi(\lambda)$. To show that $N_1$ and $N_2$ are independent, a relation which we denote as $N_1 \bot N_2$, we need to show
  \begin{equation}\label{eq:splitting_poisson_independence}
    P(N_1 = k_1, N_2 = k_2) = P(N_1 = k_1) P(N_2 = k_2),
  \end{equation}
  which is a defining property of independence.

  Firstly, note that if given sets $A \subset B$, we have
  \begin{equation*}
    P(A) = P(A \cap B).
  \end{equation*}
  With that,
  \begin{align}
    P(N_1 = k_1, N_2 = k_2) &= P(\overbrace{N_1 = k_1, N_2 = k_2}^{A}, \overbrace{N_1 + N_2 = k_1 + k_2}^{B}) \nonumber \\
                            &= P(A \mid B) P(B) \nonumber \\
                            &= \binom{k_1 + k_2}{k_1} p_1^{k_1} p_2^{k_2} \cdot \frac{e^{-\lambda} \lambda^{k_1 + k_2}}{(k_1 + k_2)!} \nonumber \\
                            &= \frac{(k_1 + k_2)!}{k_1! k_2!} p_1^{k_1} p_2^{k_2} \cdot \frac{e^{-\lambda (1)} \lambda^{k_1 + k_2}}{(k_1 + k_2)!} \nonumber \\
                            &= e^{-\lambda (p_1 + p_2)} \cdot \frac{(\lambda p_1)^{k_1}}{k_1!} \cdot \frac{(\lambda p_2)^{k_2}}{k_2!} \nonumber \\
                            &= \frac{e^{-\lambda p_1} (\lambda p_1)^{k_1}}{k_1!} \cdot \frac{e^{-\lambda p_2} (\lambda p_2)^{k_2}}{k_2!} \label{eq:splitting_poisson_jointdist}
  \end{align}
  Thus, the marginal distribution of $N_1$
  \begin{align*}
    P(N_1 = k_1) &= \sum_{k_2 = 0}^{\infty} P(N_1 = k_1, N_2 = k_2) \\
                 &= \frac{e^{-\lambda p_1} (\lambda p_1)^{k_1}}{k_1!} e^{-\lambda p_2} \sum_{k_2 = 0}^{\infty} \frac{(\lambda p_2)^{k_2}}{k_2!} \\
                 &= \frac{e^{-\lambda p_1} (\lambda p_1)^{k_1}}{k_1!} e^{-\lambda p_2} e^{\lambda p_2} \\
                 &= \frac{e^{-\lambda p_1} (\lambda p_1)^{k_1}}{k_1!}
  \end{align*}
  which is the pmf of $\Poi(\lambda p_1)$. The marginal distribution of $N_2$ is similar. It is clear from \cref{eq:splitting_poisson_jointdist} that we have \cref{eq:splitting_poisson_independence}. The result then follows from induction.\qed\
\end{proof}

\begin{eg}[Thinning Property of the Poisson Distribution]\label{eg:thinning_property_of_poisson}
  The number of claims of a portfolio follows $\Poi(\lambda)$. The severity of ground-up loss follows $\Unif(0, b)$. The insurer would like to impose an ordinary deductible $d$ and a policy limit $u$ such that
  \begin{equation*}
    0 < d < u < b.
  \end{equation*}
  What is the frequency distribution of \hlnotea{positive payments}?
\end{eg}

\begin{solution}
  Let Type 1 be the case where $X < d$ and Type 2 be $X > d$. Since the severity of the ground-up loss follows $\Unif(0, b)$, the probability of an occurrence of Type 2 is
  \begin{equation*}
    1 - \frac{d}{b} = \frac{b - d}{b}.
  \end{equation*}
  By \cref{propo:splitting_a_poisson_distribution}, we have that the frequency distribution of positive payments, i.e. Type 2, follows $\Poi \left( \lambda \frac{b - d}{b} \right)$.
\end{solution}

% subsubsection poisson_distribution_continued (end)

\subsubsection{Binomial Distribution}%
\label{ssub:binomial_distribution}
% subsubsection binomial_distribution

\begin{defn}[Binomial Distribution]\index{Binomial Distribution}\label{defn:binomial_distribution}
  A counting rv $N$ is said to have a \hlnoteb{binomial distribution} with parameters $q \in (0, 1)$ and $m \in \mathbb{N} \setminus \{ 0 \}$, written as $N \sim \Bin(q, m)$, if it has the pmf
  \begin{equation*}
    p_k = P(N = k) = \binom{m}{k} q^k ( 1 - q )^{m - k}, \enspace k = 0, 1, 2, \ldots .
  \end{equation*}
\end{defn}

\begin{remark}
  It is easy to verify that this is a valid probability distribution, since
  \begin{equation*}
    \sum_{k=0}^{m} \binom{m}{k} q^k (1 - q)^{m - k} = (q + 1 - q)^m = 1^m = 1
  \end{equation*}
  by the \hlnotea{binomial theorem}.
\end{remark}

\begin{note}
  \begin{itemize}
    \item When $m = 1$, the distribution is called a \hldefn{Bernoulli} rv with mean $q$. 
    \item The binomial distribution is a \hlnotea{bounded} rv, since it has a fixed number of trials.
  \end{itemize}
\end{note}

\begin{propo}[PGF of Binomial Distribution]\label{propo:pgf_of_binomial_distribution}
  Let $N \sim \Bin(q, m)$. Its pgf is given by
  \begin{equation*}
    G(t) = ( 1 - q + tq )^m.
  \end{equation*}
  Moreover, its mean and variance are
  \begin{equation*}
    E[N] = mq \text{ and } \Var(N) = mq(1 - q)
  \end{equation*}
  respectively.
\end{propo}

\begin{proof}
  We have
  \begin{equation*}
    G(t) = \sum_{k=0}^{m} \binom{m}{k} (tq)^k (1 - q)^{m - k} = ( 1 - q + tq )^m
  \end{equation*}
  The mean is, therefore,
  \begin{equation*}
    G'(1) = mq (1 - q + (1)q)^{m - 1} = mq,
  \end{equation*}
  and its variance
  \begin{align*}
    \Var(N) &= G''(1) + G'(1) - G'(1)^2 \\
            &= m(m-1)q^2 + mq - m^2 q^2 = mq ( 1 - q ).
  \end{align*}\qed\
\end{proof}

\begin{propo}[Sum of Independent Binomial RVs]\label{propo:sum_of_independent_binomial_rvs}
  If $N_1, \ldots, N_n$ are independent and $N_i \sim \Bin(q, m_i)$ for $i = 1, \ldots, n$, then
  \begin{equation*}
    N = \sum_{i=1}^{n} N_i \sim \Bin \left( q, \sum_{i=1}^{n} m_i \right).
  \end{equation*}
\end{propo}

\begin{proof}
  We shall use the pgf to prove this, instead of using the mgf (which is the common approach).
  \begin{align*}
    G_N(t) &= E \left[ t^N \right] = E \left[ t^{\sum_{i=1}^{n} N_i} \right] \overset{(*)}{=} \prod_{i=1}^{n} E \left[ t^{N_i} \right] \\
           &= \prod_{i=1}^{n} (1 - q + tq)^{m_i} = (1 - q + tq)^{\sum\limits_{i=1}^{n} m_i}
  \end{align*}
  Thus $N = \sum_{i=1}^{n} N_i \sim \Bin \left( q, \sum\limits_{i=1}^{n} m_i \right)$.\qed\
\end{proof}

\begin{note}
  As a result of \cref{propo:sum_of_independent_binomial_rvs}, if we have a sequence of Bernoulli trials, each with the same ``success'' probability $q$, call each of them $I_i$, then
  \begin{equation*}
    N = \sum_{i=1}^{m} I_i \sim \Bin ( q, m )
  \end{equation*}
  Consequently, it becomes rather silly how easy it is we can get the mean and variance of $N$:
  \begin{gather*}
    E[N] = E \left[ \sum_{i=1}^{m} I_i \right] = \sum_{i=1}^{m} E [ I_i ] = mq \\
    \Var(N) = \Var \left( \sum_{i=1}^{m} I_i \right) = \sum_{i=1}^{m} \Var(I_i) = mq(1 - q)
  \end{gather*}
\end{note}

% subsubsection binomial_distribution (end)

% subsection frequency_distributions_continued (end)

% section frequency_distribution_basic_frequency_distributions_continued (end)

% chapter lecture_14_oct_30th (end)

\chapter{Lecture 15 Nov 01st}%
\label{chp:lecture_15_nov_01st}
% chapter lecture_15_nov_01st

\section{Frequency Distribution --- Basic Frequency Distributions (Continued 2)}%
\label{sec:frequency_distribution_basic_frequency_distributions_continued_2}
% section frequency_distribution_basic_frequency_distributions_continued_2

\subsection{Frequency Distributions (Continued 2)}%
\label{sub:frequency_distributions_continued_2}
% subsection frequency_distributions_continued_2

\subsubsection{Negative Binomial Distribution}%
\label{ssub:negative_binomial_distribution}
% subsubsection negative_binomial_distribution

\begin{defn}[Negative Binomial Distribution]\index{Negative Binomial Distribution}\label{defn:negative_binomial_distribution}
  A counting rv $N$ is said to have a \hlnoteb{negatiev binomial distribution} with parameters $\beta > 0$ and $r > 0$, denoted $N \sim \NB(\beta, r)$, if it has the pmf
  \begin{equation*}
    p_k = P(N = k) = \binom{k + r - 1}{k} \left( \frac{1}{1 + \beta} \right)^r \left( \frac{\beta}{1 + \beta} \right)^k, k = 0, 1, 2, \ldots
  \end{equation*}
\end{defn}

\begin{remark}
  Note that
  \begin{equation*}
    \binom{k + r - 1}{k} = \frac{\Gamma(k + r)}{k! \Gamma(r)} = \frac{(k + r - 1)!}{k! (r - 1)!},
  \end{equation*}
  where the later equality follows if $r \in \mathbb{N} \setminus \{ 0 \}$.
\end{remark}

\begin{note}
  \begin{itemize}
    \item When $r = 1$, we can also write the pmf of $\NB(\beta, 1)$ as the pmf of the \hlnotea{geometric distribution}:
      \begin{equation*}
        p_k = \frac{1}{1 + \beta} \left( \frac{\beta}{1 + \beta} \right)^k, k = 0, 1, 2, \ldots.
      \end{equation*}

    \item To verify that the negative binomial distribution is a valid probability distribution, we need the following identity:\marginnote{
      \begin{ex}
        Verify that the negative binomial distribution is a valid probability distribution.
      \end{ex}}
      \begin{equation*}
        (1 - x)^{-r} = \sum_{k=0}^{\infty} \binom{k + r - 1}{k} x^k,
      \end{equation*}
      which is proven as follows:

      \begin{proof}
        We shall use the \hlnotea{Taylor expansion} of $(1 - x)^{-r}$.
        \begin{align*}
          (1 - x)^{-r} &= 1 + (-1)(-r)(1 - x)^{-r-1}\at{x = 0}{} x \\
                       &\quad + \frac{r}{2} (-1) (-r-2) (1 - x)^{-r-2}\at{x = 0}{} x^2 + \hdots \\
                       &= 1 + rx + \frac{r(r + 1)}{2} x^2 + \frac{r(r + 1)(r + 2)}{3!} x^3 + \hdots \\
                       &= \sum_{k=0}^{\infty} \frac{r(r + 1) \hdots (r + k - 1)}{k!} x^K \\
                       &= \sum_{k=0}^{\infty} \binom{k + r - 1}{k} x^k.
        \end{align*}\qed\
      \end{proof}

    \item The negative binomial distribution is an \hlnotea{unbounded} rv, and can take all natural numbers sans $0$.
  \end{itemize}
\end{note}

\paragraph{Interpretation} Consider an experiment with independent trails, of which each has only two possible outcomes: success with probability $\frac{1}{1 + \beta}$, and failure with probability $1 - \frac{1}{1 + \beta} = \frac{\beta}{1 + \beta}$. Let $N$ denote the number of failures until reaching the $r$\textsuperscript{th} success.

\begin{propo}[PGF of the Negative Binomial Distribution]\label{propo:pgf_of_the_negative_binomial_distribution}
  Let $N \sim \NB(\beta, r)$. Its pgf is thus
  \begin{equation*}
    G(t) = [1 - \beta(t - 1)]^{-r}.
  \end{equation*}
  Moreover, its mean and variance are
  \begin{equation*}
    E[N] = r \beta \text{ and } \Var(N) = r\beta(1 + \beta),
  \end{equation*}
  respectively.
\end{propo}

\begin{note}
  Note that the proof for getting the pgf is similar to how we can verify that $N$ is a probability (same case as in earlier counting distributions).
\end{note}

\begin{proof}
  Using the Taylor Expansion $(1 - x)^{-r} = \sum_{k=0}^{\infty} \binom{k + r - 1}{k} x^k$, we have
  \begin{align*}
    G(t) &= \sum_{k=0}^{\infty} t^k p_k = \left( \frac{1}{1 + \beta} \right)^r \sum_{k=0}^{\infty} \binom{k + r - 1}{k} \left( \frac{t \beta}{1 + \beta} \right)^k \\
         &= \left( \frac{1}{1 + \beta} \right)^r \left( 1 - \frac{t \beta}{1 + \beta} \right)^{-r} = [ 1 - \beta (t - 1) ]^{-r}.
  \end{align*}
  Consequently, the mean is
  \begin{equation*}
    E[N] = G'(1) = -r(-\beta) = r \beta
  \end{equation*}
  and variance is
  \begin{align*}
    \Var(N) &= G''(1) + G'(1) - G'(1)^2 \\
            &= -r ( -r - 1 ) \beta^2 + r \beta - r^2 \beta^2 = r \beta ( 1 + \beta )
  \end{align*}\qed\
\end{proof}

\begin{propo}[Negative Binomial from Poisson Conditioned on Gamma]\label{propo:negative_binomial_from_poisson_conditioned_on_gamma}
  Let $N \mid \Lambda = \lambda \sim \Poi(\lambda)$ and $\Lambda \sim \Gam(\alpha, \theta)$. Then
  \begin{equation*}
    N \sim \NB(\theta, \alpha).
  \end{equation*}
\end{propo}

\begin{note}
  We may also write $N \mid \Lambda = \lambda \sim \Poi(\lambda)$ as $N \mid \Lambda \sim \Poi(\Lambda)$.
\end{note}

\begin{proof}
  We shall prove this statement by finding the pgf of $N$, which identifies the distribution. Note that
  \begin{equation*}
    G_N(t) = E\left[ t^N \right] \overset{\text{Proposition }\ref{propo:total_expectation_and_total_variance}}{=} E \left[ E \left[ t^N \mid \Lambda \right] \right] \overset{(*)}{=} E \left[ e^{\Lambda (t - 1)} \right],
  \end{equation*}
  where $(*)$ \hlwarn{requires further clarification}. Now since $\Lambda \sim \Gam(\alpha, \theta)$, and $M_\Lambda(t) = E\left[e^{t \Lambda}\right] = (1 - \theta t)^{-\alpha}$, it follows that
  \begin{equation*}
    G_N(t) = [ 1 - \theta(t - 1) ]^{-\alpha}.
  \end{equation*}
  Thus $N \sim \NB(\theta, \alpha)$.\qed\
\end{proof}

\begin{propo}[Combining Negative Binomial Distributions]\label{propo:combining_negative_binomial_distributions}
  If $\{ N_i \}_{i = 1}^{n}$ is a sequence of independent rvs, and $N_i \sim \NB(\beta, r_i)$. Then
  \begin{equation*}
    N = \sum_{i=1}^{n} N_i \sim \NB \left( \beta, \sum_{i=1}^{n} r_i \right).
  \end{equation*}
\end{propo}

\begin{proof}
  We shall, again, use the pgf. We have
  \begin{align*}
    G_N(t) &= E \left[ t^N \right] \overset{(*)}{=} \prod_{i=1}^{n} E \left[ t^{N_i} \right] = \prod_{i=1}^{n} G_{N_i}(t) \\
           &= \prod_{i=1}^{n} [1 - \beta ( t - 1 )]^{r_i} = [ 1 - \beta ( t - 1 ) ]^{- \sum_{i=1}^{n} r_i},
  \end{align*}
  where $(*)$ is by independence of the rvs, and the last equality is thanks to $\beta$ being fixed for all the rvs. This completes the proof.\qed\
\end{proof}

% subsubsection negative_binomial_distribution (end)

% subsection frequency_distributions_continued_2 (end)

\subsection{$(a, b, n)$ Classes}%
\label{sub:a_b_n_classes}
% subsection a_b_n_classes

\subsubsection{$(a, b, 0)$ Class}%
\label{ssub:a_b_0_class}
% subsubsection a_b_0_class

\begin{defn}[$(a, b, 0)$ Class]\index{$(a, b, 0)$ Class}\label{defn:a_b_0_class}
  The $(a, b, 0)$ class is a set of counting rvs with pmf $p_k$ satisfying the recursive formula
  \begin{equation*}
    \frac{p_k}{p_{k - 1}} = a + \frac{b}{k}, \quad k \in \mathbb{N} \setminus \{ 0 \}.
  \end{equation*}
\end{defn}

\begin{remark}
  An $(a, b, 0)$ distribution is determiend by the parameters $a$ and $b$.
\end{remark}

\begin{note}
  Observe that
  \begin{align*}
    \frac{p_1}{p_0} &= a + \frac{b}{1} \iff p_1 = p_0 \left( a + \frac{b}{1} \right) \\
    \frac{p_2}{p_2} &= a + \frac{b}{2} \iff p_2 = p_1 \left( a + \frac{b}{2} \right) = p_0 \left( a + \frac{b}{1} \right)\left( a + \frac{b}{2} \right) \\
                    &\vdots \\
    \frac{p_k}{p_{k - 1}} &= a + \frac{b}{k} \iff p_k = p_0 \prod_{i=1}^{k} \left( a + \frac{b}{i} \right).
  \end{align*}
  Thus we see that each of the $p_k$ is completely determined by $p_0$. In other words, for the distributions of this class, if we can find $p_0$, then we can get $p_k$, even if we do not know the actual parameters of the distribution.

  In fact, we can solve for $p_0$, if we already know what $a$ and $b$ are: we need to solve for $p_0$ in $\sum_{k=0}^{\infty} p_k = 1$. In particular, we need to solve for
  \begin{equation*}
    p_0 \sum_{k=0}^{\infty} \prod_{i=1}^{k} \left( a + \frac{b}{i} \right) = 1.
  \end{equation*}
\end{note}

\paragraph{Members of the $(a, b, 0)$ class} It can be shown\sidenote{Perhaps this can be shown using \url{https://www.actuaries.org/ASTIN/Colloquia/Helsinki/Papers/S7_13_Fackler.pdf}.} that the Poisson, Binomial, and Negative Binomial distributions are \hlnotea{the only} distributions that belong to this class. We have that

\begin{table}[ht]
  \centering
  \begin{tabular}{l | c | c | c}
    \toprule
    Distribution    & $a$                       & $b$                              & $p_0$ \\
    \midrule
    $\Poi(\lambda)$ & $0$                       & $\lambda$                        & $e^{-\lambda}$ \\
    $\Bin(q, m)$    & $-\frac{q}{1 - q}$        & $( m + 1 ) \frac{q}{1 - q}$      & $(1 - q)^m$ \\
    $\NB(\beta, r)$ & $\frac{\beta}{1 + \beta}$ & $(r - 1)\frac{\beta}{1 + \beta}$ & $(1 + \beta)^{-r}$ \\
    \bottomrule
  \end{tabular}
  \caption{The $(a, b, 0)$ distributions}
  \label{table:the_a_b_0_distributions}
\end{table}

We shall prove for the case of $\Poi(\lambda)$.\marginnote{
\begin{ex}
  Find $a$, $b$ and $p_0$ for $\Bin(q, m)$ and $\NB(\beta, r)$.
\end{ex}}

\begin{proof}
  By the pmf of $\Poi(\lambda)$, it is clear that
  \begin{equation*}
    p_0 = \frac{e^{-\lambda} \lambda^0}{0!} = e^{-\lambda}.
  \end{equation*}
  Now, since
  \begin{equation*}
    p_1 = \lambda e^{-\lambda} \quad p_2 = \frac{1}{2} \lambda^2 e^{-\lambda}
  \end{equation*}
  we have the following system of equations:
  \begin{gather*}
    \lambda = \frac{p_1}{p_0} = a + b \\
    \frac{1}{2}\lambda = \frac{p_2}{p_1} = a + \frac{b}{2}
  \end{gather*}
  Thus $b = \lambda$ and $a = 0$.\qed\
\end{proof}

\begin{eg}
  Assume that the number of claims in a portfolio $N$ follows $(-0.25, 2.75, 0)$ distribution. Calculate the probability that there is at least one claim.
\end{eg}

\begin{solution}
  Using \cref{table:the_a_b_0_distributions}, we know that $N \sim \Bin(q, m)$, where
  \begin{equation*}
    - \frac{q}{1 - q} = - 0.25 \text{ and } (m + 1) \frac{q}{1 - q} = 2.75,
  \end{equation*}
  which gives $q = 0.2$ and $m = 10$. Thus the desired probability is
  \begin{equation*}
    P(N \geq 1) = 1 - P(N = 0) = 1 - (1 - 0.8)^{10} = 0.8926.
  \end{equation*}
\end{solution}

% subsubsection a_b_0_class (end)

% subsection a_b_n_classes (end)

% section frequency_distribution_basic_frequency_distributions_continued_2 (end)

% chapter lecture_15_nov_01st (end)

\chapter{Lecture 16 Nov 6th}%
\label{chp:lecture_16_nov_6th}
% chapter lecture_16_nov_6th

\section{Frequency Distribution --- Basic Frequency Distributioned (Continued 3)}%
\label{sec:frequency_distribution_basic_frequency_distributioned_continued_3}
% section frequency_distribution_basic_frequency_distributioned_continued_3

\subsection{$(a, b, n)$ Classes (Continued)}%
\label{sub:a_b_n_classes_continued}
% subsection a_b_n_classes_continued

\subsubsection{$(a, b, 1)$ Class}%
\label{ssub:_a_b_1_class}
% subsubsection _a_b_1_class

\paragraph{Motivation} There are times when the $(a, b, 0)$ class of distributions fail to give a complete characterization of certain insurance data with regards to the claim arrival process. This is especially apparent when we notice that we do not have as much freedom in fixing $P(N = 0)$. This provides us with the motivation to define the $(a, b, 1)$ class.

\begin{defn}[$(a, b, 1)$ Class]\index{$(a, b, 1)$ Class}\label{defn:a_b_1_class}
  The $(a, b, 1)$ class is defined as a set of counting rvs with pmf $p_k$ satisfying the recursion
  \begin{equation*}
    \frac{p_k}{p_{k - 1}} = a + \frac{b}{k}, \quad k = 2, 3, \ldots .
  \end{equation*}
\end{defn}

\begin{remark}
  Notice that the formula is almost exactly the same as compared to \hyperref[defn:a_b_0_class]{the definition of the $(a, b, 0)$ class}, \hlimpo{except} now we have that $k$ starts from $2$ instead of $1$. This means that this recursive definition will no longer have any control over $p_0$, which is what we want.
\end{remark}

There are two distributions from the $(a, b, 1)$ class that we shall focus on, namely
\begin{itemize}
  \item the zero-truncated distribution; and
  \item the zero-modified distribution.
\end{itemize}

\paragraph{Zero-Truncated Distribution}\label{par:zero_truncated_distribution}\index{Zero-Truncated Distribution} In this case, we set $p_0 = 0$, i.e. we always expect a claim.

Let $p_k$, where $k = 0, 1, 2, \ldots$, be the pmf of an $(a, b, 0)$ distribution, of which we label its rv as $N$. Then, let $p_k^T$, for $k = 0, 1, 2, \ldots$, be the pmf of the \hlnoteb{zero-truncated distribution}, whose rv is denoted by $N^T$, with $p_0^T = 0$.

We can obtain values for each of the $p_k^T$'s, from $k = 1, 2, \ldots$, from the $p_k$'s: \sidenote{Perhaps this was implied but it certainly was not explicitly stated: the $a, b$ in the $(a, b, 0)$ can be treated exactly as the $a, b$ from the $(a, b, 0)$ class.}notice that for $k = 2, 3, 4, \ldots$, we have
\begin{equation*}
  \frac{p_k^T}{p_{k - 1}^T} = a + \frac{b}{k} = \frac{p_k}{p_{k - 1}} \implies \frac{p_k^T}{p_k} = \frac{p_{k - 1}^T}{p_{k - 1}}.
\end{equation*}
Observe that
\begin{align*}
  \text{ When } k = 2, &\quad \frac{p_2^T}{p_2} = \frac{p_1^T}{p_1} \\
  \text{ When } k = 3, &\quad \frac{p_3^T}{p_3} = \frac{p_2^T}{p_2} \\
  \text{ When } k = 4, &\quad \frac{p_4^T}{p_4} = \frac{p_3^T}{p_3} \\
                       &\vdots
\end{align*}
Therefore, we have that
\begin{equation*}
  \frac{p_1^T}{p_1} = \frac{p_2^T}{p_2} = \frac{p_3^T}{p_3} = \hdots =: \beta^T,
\end{equation*}
where we give this value a variable. Consequently, we have
\begin{equation*}
  p_k^T = \beta^T p_k.
\end{equation*}
Of course, we'd like to know what $\beta^T$ is. Since $p_0^T = 0$, we have that
\begin{equation*}
  1 = \sum_{k=1}^{\infty} p_k^T = \beta^T \sum_{k=1}^{\infty} p_k = \beta^T ( 1 - p_0 )
\end{equation*}
and so
\begin{equation*}
  \beta^T = \frac{1}{1 - p_0}.
\end{equation*}
To store this information, we look to the pgf of $N^T$: we have
\begin{equation*}
  G_{N^T}(t) = \sum_{k=0}^{\infty} t^k p_{k}^T= \sum_{k=0}^{\infty} t^k \beta^T p_k = \frac{1}{1 - p_0} \sum_{k=1}^{\infty} t^k p_k = \frac{G_N(t) - p_0}{1 - p_0}.
\end{equation*}

\begin{eg}
  Let $N \sim \NB(\beta, r)$. Its zero-truncated version has pmf of the form
  \begin{align*}
    p_k^T &= \frac{p_k}{1 - p_0} = \frac{\binom{k + r - 1}{k} \left( \frac{1}{1 + \beta} \right)^r \left( \frac{\beta}{1 + \beta} \right)^k}{1 - \left( \frac{1}{1 + \beta} \right)^r} \\
          &= \frac{\Gamma(k + r)}{k! \Gamma(r)} \frac{\left( \frac{\beta}{1 + \beta} \right)^k}{(1 + \beta)^r - 1}
  \end{align*}
\end{eg}

\paragraph{Zero-Modified Distribution}\label{para:zero_modified_distribution}\index{Zero-Modified Distribution} If we choose $P(N = 0)$ to be some value that is not any of the $p_0$'s of the distributions from the $(a, b, 0)$ class, then this distribution is called a \hlnoteb{zero-modified distribution}.

Let $p_k$, for $k = 0, 1, 2, \ldots$, be the pmf of an $(a, b, 0)$ distribution, labelled $N$. Let $p_k^M$, for $k = 0, 1, 2, \ldots$, be the pmf of the zero-modified distribution, of which we denote by $N^M$, with $p_0^M$ chosen as described.

We can, again, express $p_k^M$ in terms of $p_k$: for $k = 1, 2, 3, \ldots$ \sidenote{Note that in this case, the probabilities after $p_0^M$ is reliant on $p_0$; of course, since the sum of the probabilities must be $1$, within the axioms of probability.},
\begin{align*}
  \frac{p_k^M}{p_{k - 1}^M} = a + \frac{b}{k} = \frac{p_k}{p_{k - 1}},
\end{align*}
and so we have, again,
\begin{equation*}
  p_k^M = \beta^M p_k.
\end{equation*}
So we can solve for $\beta^M$:
\begin{gather*}
  1 = \sum_{k=0}^{\infty} p_k^M = p_0^M + \sum_{k=1}^{\infty} \beta^M p_k \\
  \implies 1 - p_0^M = \beta^M \sum_{k=1}^{\infty} p_k \\
  \implies \beta^M = \frac{1 - p_0^M}{1 - p_0}.
\end{gather*}
The pgf of a zero-modified distribution is
\begin{align*}
  G_{N^M}(t) &= \sum_{k=0}^{\infty} t^k p_k^M = p_0^M + \sum_{k=1}^{\infty} t^k \beta^M p_k =  p_0^M + \beta^M ( G_N(t) - p_0 ) \\
             &= p_0^M + \frac{1 - p_0^M}{1 - p_0} ( G_N(t) - p_0 ) \\
             &= \frac{p_0^M - p_0^M p_0}{1 - p_0} + \frac{1 - p_0^M}{1 - p_0} G_N(t) - \frac{p_0 - p_0^M p_0}{1 - p_0} \\
             &= \frac{p_0^M - p_0}{1 - p_0} + \frac{1 - p_0^M}{1 - p_0} G_N(t)
\end{align*}

\begin{remark}
  As a consequence of the form of the pgf, if $p_0^M > p_0$, we may interpret $N_M$ as
  \begin{equation*}
    N^M = \begin{cases}
      0 & \text{ w/ prob } \frac{p_0^M - p_0}{1 - p_0} \\
      N & \text{ w/ prob } \frac{1 - p_0^M}{1 - p_0}
    \end{cases},
  \end{equation*}
  which is a mixture of a degenerated distribution at $0$, and the original $(a, b, 0)$ distribution $N$.

  Consequently, we can also solve for $G_{N^M}(t)$ using this notion: let $\Theta$ be the indicator-function-like distribution such that
  \begin{equation*}
    P(\Theta = \theta) = \begin{cases}
      \frac{p_0^M - p_0}{1 - p_0}      & \theta = 0 \\
      \frac{1 - p_0^M}{1 - p_0} G_N(t) & \theta = 1
    \end{cases}.
  \end{equation*}
  Then
  \begin{align*}
    G_{N^M}(t) &= E \left[ t^{N^M} \right] = E \left[ E \left[ t^{N^M} \mid \Theta \right] \right] \\
               &= E \left[ t^{N^M} \mid \Theta = 0 \right] P(\Theta = 0) + E \left[ t^{N^M} \mid \Theta = 1 \right] P(\Theta = 1) \\
               &= P(\Theta = 0) E \left[ t^0 \mid \Theta = 0 \right] + P(\Theta = 1) E \left[ t^N \mid \Theta = 1 \right] \\
               &= \frac{p_0^M - p_0}{1 - p_0} (1) + \frac{1 - p_0^M}{1 - p_0} G_N(t),
  \end{align*}
  as what we had.
\end{remark}

\begin{propo}[Moments of an $(a, b, 1)$ Distribution]\label{propo:moments_of_an_a_b_1_distribution}
  The moments of an $( a, b, 1 )$ distribution can be compited from the original $(a, b, 0)$ distribution as
  \begin{equation*}
    E \left[ \left( N^M \right)^k \right] = \frac{1 - p_0^M}{1 - p_0} E \left[ N^k \right], \quad k = 1, 2, 3, \ldots
  \end{equation*}
  where $N^M$ is the rv modified from $N$ from the $(a, b, 0)$ class.
\end{propo}

\begin{proof}
  The derivation is straightforward:
  \begin{align*}
    E \left[ \left( N^M \right)^k \right] &= \sum_{m = 0}^{\infty} m^k p_m^M = \sum_{m=1}^{\infty} \frac{1 - p_0^M}{1 - p_0} m^k p_m \\
                                          &= \frac{1 - p_0^M}{1 - p_0} \sum_{m=1}^{\infty} m^k p_m = \frac{1 - p_0^M}{1 - p_0} \sum_{m = 0}^{\infty} m^k p_m \\
                                          &= \frac{1 - p_0^M}{1 - p_0} E \left[ N^k \right].
  \end{align*}\qed\
\end{proof}

\begin{eg}
  Let $N \sim \Poi(2)$. Find the pmf of a zero-modified version of the Poisson distribution with $p_0^M = 0.3$.
\end{eg}

\begin{solution}
  We are given that the pmf of $N$ is
  \begin{equation*}
    p_k = \frac{e^{-2} 2^k}{k!}, \quad k = 0, 1, 2, \ldots .
  \end{equation*}
  Thus
  \begin{align*}
    p_k^M &= \frac{1 - p_0^M}{1 - p_0} p_k = \frac{1 - 0.3}{1 - e^{-2}} \cdot \frac{e^{-2} 2^k}{k!} \\
          &= \frac{0.7 \left( 2^k e^{-2} \right)}{k! (1 - e^{-2})}, \quad k = 1, 2, 3, \ldots.
  \end{align*}
\end{solution}

\begin{note}
  It should be noted that a zero-truncated distribution is a special case of the zero-modified distribution. In fact, if $p_0^M = 0$, we get that the zero-modified distribution is exactly the zero-truncated distribution.
\end{note}

In addition to the zero-modified $(a, b, 0)$ distributions, there are other members in the $(a, b, 1)$ distributions. One such example is known as the \hlnotea{logarithmic distribution}, which has the pmf
\begin{equation*}
  p_k = \frac{\beta^k}{k ( 1 + \beta )^k \ln ( 1 + \beta )} , \quad k = 1, 2, 3, \ldots,
\end{equation*}
with parameter $\beta > 0$. Notice that for $k = 2, 3, 4, \ldots$,
\begin{align*}
  \frac{p_k}{p_{k - 1}} &= \frac{\beta^k}{k ( 1 + \beta )^k \ln (1 + \beta)} \cdot \frac{(k - 1) ( 1 + \beta )^{k - 1} \ln ( 1 + \beta )}{\beta^{k - 1}} \\
                        &= \frac{\beta}{1 + \beta} \cdot \frac{k - 1}{k} = \frac{\beta}{1 + \beta} + \frac{- \frac{\beta}{1 + \beta}}{k},
\end{align*}
where we observe that
\begin{equation*}
  a = \frac{\beta}{1 + \beta} \text{ and } b = - \frac{\beta}{1 + \beta}.
\end{equation*}

% subsubsection _a_b_1_class (end)

% subsection a_b_n_classes_continued (end)

% section frequency_distribution_basic_frequency_distributioned_continued_3 (end)

% chapter lecture_16_nov_6th (end)

\chapter{Lecture 17 Nov 13th}%
\label{chp:lecture_17_nov_13th}
% chapter lecture_17_nov_13th

\section{Frequency Distributions --- Creating New Frequency Distributions and Effect on Frequency}%
\label{sec:frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency}
% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency

\subsection{Mixed Frequency Distributions}%
\label{sub:mixed_frequency_distributions}
% subsection mixed_frequency_distributions

This is a concept that we have seen before.

Suppose that $N \mid \Theta = \theta$ has conditional pmf $P(N = n \mid \Theta = \theta)$ and the mixing rv $\Theta$ is either
\begin{itemize}
  \item \hlnotea{discrete} with pmf $p_\Theta(\theta_i)$ for $i = 1, \ldots, m$; or
  \item \hlnotea{continuous} with pdf $f_\Theta(\theta)$.
\end{itemize}
Both the distributions of $N \mid \Theta = \theta$ and $\Theta$ are usually given. The unconditional pmf of $N$ is thus
\begin{equation*}
  P(N = n) = \begin{cases}
    \sum_{i=1}^{m} P(N = n \mid \Theta = \theta_i) p_\Theta(\theta_i) & \Theta \text{ is discrete } \\
    \int_{\Theta} P(N = n \mid \Theta = \theta) f_\Theta(\theta) \dif{\theta} & \Theta \text{ is continuous }
  \end{cases}.
\end{equation*}

\begin{remark}
  Due to the context of which we work in, we shall always, perversely so, assume that $N$ is a counting rv that takes on non-negative integers.
\end{remark}

\begin{note}
  Recall \cref{propo:total_expectation_and_total_variance}, which gives us two concepts that are useful to us in this section: we have
  \begin{gather*}
    E \left[ E \left[ X \mid \Theta \right] \right] = E [ X ] \\
    \Var(X) = \Var( E \left[ X \mid \Theta \right] ) + E \left[ \Var(X \mid \Theta) \right].
  \end{gather*}
\end{note}

\subsubsection{Mixed Poisson Distribution}%
\label{ssub:mixed_poisson_distribution}
% subsubsection mixed_poisson_distribution

\begin{defn}[Mixed Poisson Distribution]\label{defn:mixed_poisson_distribution}
  If $N \mid \Lambda = \lambda$ follows $\Poi(\lambda)$ for some rv $\Lambda$, we say $N$ follows a \hlnoteb{mixed Poisson Distribution}.
\end{defn}

\begin{eg}
  Recall \cref{propo:negative_binomial_from_poisson_conditioned_on_gamma}. We have that given $N \mid \Lambda = \lambda \sim \Poi(\lambda)$ and $\Lambda \sim \Gam(\alpha, \theta)$, we have that
  \begin{equation*}
    N \sim \NB(\theta, \alpha).
  \end{equation*}
\end{eg}

\begin{propo}[Mixed Poisson Distribution has a Variance Greater than its Mean]\label{propo:mixed_poisson_distribution_has_a_variance_greater_than_its_mean}
  For a mixed Poisson rv $N$, we have $\Var(N) > E[N]$.
\end{propo}

\begin{proof}
  When we say that $N$ is a mixed Poisson rv, it actually means that the rv that we have is $N \mid \Lambda = \lambda$, not $N$ alone. Now by \cref{propo:total_expectation_and_total_variance}, since $\Lambda = \lambda$ is the mean of the mixed Poisson rv, we have
  \begin{align*}
    \Var(N) &= \Var( E \left[ N \mid \Lambda \right] ) + E \left[ \Var \left( N \mid \Lambda \right) \right] \\
            &= \Var( \Lambda ) + E [ \Lambda ] = \Var(\Lambda) + E[ N ].
  \end{align*}
  Since $\Var(\Lambda) > 0$, the result follows.\qed\
\end{proof}

\begin{eg}
  Suppose that $N \mid \Lambda = \lambda$ follows $\Poi(\lambda)$ and the pdf of $\Lambda$ is given by
  \begin{equation*}
    f_\Lambda(\lambda) = \frac{\alpha^2}{1 + \alpha}( 1 + \lambda ) e^{-\alpha\lambda}, \quad \lambda > 0,
  \end{equation*}
  where $\alpha > 0$. Show that $N$ is the mixture of two negative binomial distributions.
\end{eg}

\begin{solution}
  We shall show the claim by the pgf of $N$. We have
  \begin{align*}
    G_N(t) &= E \left[ t^N \right] = E \left[ E \left[ t^N \mid \Lambda \right] \right] = E \left[ e^{\Lambda ( t - 1 )} \right] \\
           &= \int_{0}^{\infty} \frac{\alpha^2}{1 + \alpha} ( 1 + \lambda ) e^{-\lambda( \alpha + 1 - t )} \dif{\lambda}
  \end{align*}
  From here, we can only proceed iff $1 + \alpha - t > 0$, i.e. $t < 1 + \alpha$; otherwise the integral diverges. Now, using integration by parts
  \begin{align*}
    G_N(t) &= \frac{\alpha^2}{1 + \alpha} \left[ - \frac{1}{\alpha + 1 - t} e^{-\lambda (\alpha + 1 - t)} \at{0}{\infty} + \right. \\
           &\quad \left. \left[ - \frac{1}{\alpha + 1 - t} \lambda e^{-\lambda(\alpha + 1 - t)} \at{0}{\infty} - \frac{1}{(\alpha + 1 - t)^2} e^{-\lambda(\alpha + 1 - t)} \at{0}{\infty} \right] \right] \\
           &= \frac{\alpha^2}{1 + \alpha} \left[ \frac{1}{\alpha + 1 - t} + \frac{1}{( \alpha + 1 - t )^2} \right]
  \end{align*}
  Now to arrive at a mixture of two negative binomial distributions, we need to know the form of the pgf for a negative binomial distribution. Note that the pgf of $\NB(\beta, r)$ is
  \begin{equation*}
    G(t) = \left[ 1 - \beta( t - 1 ) \right]^{-r}.
  \end{equation*}
  Notice that
  \begin{equation*}
    G_N(t) = \frac{\alpha^2}{1 + \alpha} \left[ [ \alpha - ( t - 1 ) ]^{-1} + [ \alpha - ( t - 1 ) ]^{-2} \right].
  \end{equation*}
  If we expand the appropriate power of $\alpha$ into the reciprocals, we can get our desired form:
  \begin{equation*}
    G_N(t) = \frac{\alpha}{1 + \alpha} \left[ 1 - \frac{1}{\alpha} (t - 1) \right]^{-1} + \frac{1}{1 + \alpha} \left[ 1 - \frac{1}{\alpha} ( t - 1 ) \right]^{-2}.
  \end{equation*}
  It is clear that
  \begin{equation*}
    \frac{\alpha}{1 + \alpha} + \frac{1}{1 + \alpha} = 1,
  \end{equation*}
  and so we have obtained our desired result; that is $N$ is a weighted mixture of two negative binomial distributions. In particular,
  \begin{equation*}
    N = \begin{cases}
      X_1 \sim \NB \left( \frac{1}{\alpha}, 1 \right) & \text{ w/ prob } \frac{\alpha}{1 + \alpha} \\
      X_2 \sim \NB \left( \frac{1}{\alpha}, 2 \right) & \text{ w/ prob } \frac{1}{1 + \alpha}
    \end{cases}.
  \end{equation*}
\end{solution}

% subsubsection mixed_poisson_distribution (end)

We can make a similar derivation of mixed distributions for Binomial and Negative Binomial.

% subsection mixed_frequency_distributions (end)

\subsection{Compound Frequency Distributions}%
\label{sub:compound_frequency_distributions}
% subsection compound_frequency_distributions

\begin{defn}[Compound Frequency Distribution]\index{Compound Frequency Distribution}\label{defn:compound_frequency_distribution}
  For two counting rvs $N$ and $M$, let
  \begin{equation*}
    S = \sum_{i=1}^{N} M_i = \begin{cases}
      M_1 + \hdots + M_N & N \geq 1 \\
      0                  & N = 0
    \end{cases},
  \end{equation*}
  where $\{ M_i \}_{i = 1}^{\infty}$ is a sequence of iid rv's distributed as $M$, and are independent of $N$. We call $S$ a \hldefn{compound rv}, $N$ the \hldefn{primal distribution}, and $M$ the \hldefn{secondary distribution}.
\end{defn}

\begin{remark}
  \begin{itemize}
    \item Compounding two counting rvs is also an approach to create new frequency distributions.
    \item $S$ is called \hlnotea{compound Poisson, Binomial, or Negative Binomial} if $N$ is a Poisson, a Binomial, or a Negative Binomial rv, respectively.
  \end{itemize}
\end{remark}

\begin{eg}[Interpretation]
  In an insurance context, compound rvs arise rather naturally. E.g. in the auto insurance context, we could have
  \begin{itemize}
    \item $N$ represents the number of accidents;
    \item $M_i$ represents the number of claims generated by the $i$\textsuperscript{th} accident;
    \item And so in this case $S$ stands for the total number of claims for a portfolio of auto insurance policies over a given time period.
  \end{itemize}
\end{eg}

\begin{propo}[Mean and Variance of the Compound RV]\label{propo:mean_and_variance_of_the_compound_rv}
  For a compound rv $S = \sum_{i=1}^{N} M_i$, where $M_i \sim M$, we have
  \begin{gather*}
    E [ S ] = E[N] E[M] \\
    \Var(S) = \Var(N)E[M]^2 + E[N]\Var(M).
  \end{gather*}
\end{propo}

\begin{proof}
  Note that the definition of $S$ relies on $N$ first, since
  \begin{equation*}
    S = \begin{cases}
      M_1 + \hdots + M_N & N \geq 1 \\
      0                  & N = 0
    \end{cases},
  \end{equation*}
  so we shall go down of the route of conditioning $S$ by $N$. Observe that
  \begin{equation*}
    E [ S \mid N ] = E \left[ \sum_{i=1}^{N} M_i \mid N \right] \overset{(*)}{=} \sum_{i=1}^{N} E [ M_i \mid N ] \overset{(**)}{=} \sum_{i=1}^{N} E[M] = N E[M]
  \end{equation*}
  where $(*)$ is by the linearity of the expectation, and $(**)$ is by $M_i \bot N$ for each $i$ and that $M_i \sim M$. The variance of $S$ conditioned on $N$ is
  \begin{equation*}
    \Var(S \mid N) = \Var \left( \sum_{i=1}^{N} M_i \mid N \right) = \sum_{i=1}^{N} \Var(M) = N\Var(M)
  \end{equation*}
  mostly for the same reason as for the expectation, but the 2nd equality involves independence of the $M_i$'s (otherwise, we would be left with a bunch of covariances).

  Then, using \cref{propo:total_expectation_and_total_variance}, we have
  \begin{equation*}
    E[S] = E[ E [ S \mid N ] ] = E [ N E[M] ] = E[N]E[M]
  \end{equation*}
  and
  \begin{align*}
    \Var(S) &= \Var(E[S \mid N]) + E[\Var(S \mid N)] \\
            &= \Var( N E[M] ) + E[ N \Var(M) ] \\
            &= \Var(N)E[M]^2 + E[N]\Var(M)
  \end{align*}
  as required.\qed\
\end{proof}

\begin{note}[Notation]
  Hereafter, we shall use the following notations: notice that each $M$, $N$, and $S$ are counting rvs, i.e. they are discrete and are non-negative integers, so let
  \begin{itemize}
    \item $p_k$ represent the pmf of $N$, the primal distribution;
    \item $f_k$ represent the pmf of $M$, the secondary distribution; and
    \item $g_k$ represent the pmf of $S$, the compound rv.
  \end{itemize}
\end{note}

\newthought{In the next lecture}, we shall look into how to compuete $g_k$. Namely, we have the following three methods:
\begin{itemize}
  \item pgf method;
  \item pmf method; and
  \item Panjer's recursion.
\end{itemize}

% subsection compound_frequency_distributions (end)

% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency (end)

% chapter lecture_17_nov_13th (end)

\chapter{Lecture 18 Nov 15th}%
\label{chp:lecture_18_nov_15th}
% chapter lecture_18_nov_15th

\section{Frequency Distributions --- Creating New Frequency Distributions and Effect on Frequency (Continued)}%
\label{sec:frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued}
% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued

\subsection{Compound Frequency Distributions (Continued)}%
\label{sub:compound_frequency_distributions_continued}
% subsection compound_frequency_distributions_continued

\subsubsection{PGF Method}%
\label{ssub:pgf_method}
% subsubsection pgf_method

\begin{propo}[PGF Method for Compound RVs]\label{propo:pgf_method_for_compound_rvs}
  For a compound rv $S = \sum_{i=1}^{N} M_i$, we have
  \begin{equation*}
    G_S(t) = G_N( G_M(t) ),
  \end{equation*}
  where $G_S, G_N, G_M$ are pgfs of $S, N, M$, respectively.
\end{propo}

\begin{proof}
  Given the definition of $S$, we have
  \begin{align*}
    G_S(t) &= E \left[ t^S \right] = E \left[ E \left[ t^{M_1 + M_2 + \hdots + M_N} \mid N \right] \right] = E \left[ \prod_{i=1}^{N} E \left[ t^{M_i} \mid N \right] \right] \\
           &= E \left[ \left( E \left[ t^M \right] \right)^N \right] = E \left[ G_M(t)^N \right] = G_N( G_M(t) ).
  \end{align*}
\end{proof}

\begin{note}
  From \cref{propo:pgf_method_for_compound_rvs}, the pmf of $g_k$ can be computed using
    \begin{equation*}
      g_k = \frac{1}{k!}G_S^{(k)}(0).
    \end{equation*}
  E.g.
    \begin{itemize}
      \item $g_0 = G_S(0) = G_N(G_M(0)) = G_N(f_0)$
      \item $g_1 = G'_S(0) = G_M'(0) G_N'( G_M(0) ) = f_1 G_N(f_0)$
    \end{itemize}
  However, this method is inefficient.
\end{note}

% subsubsection pgf_method (end)

\subsubsection{PMF Method}%
\label{ssub:pmf_method}
% subsubsection pmf_method

We shall develop, perhaps, a more efficient way of getting $g_k$: For $k = 0, 1, 2, \ldots$, we have
\begin{align*}
  g_k &= P(S = k) = \sum_{n=0}^{\infty} P(S = k \mid N = n) P(N = n) \\
      &= \sum_{n = 0}^{\infty} P ( M_1 + \hdots + M_n = k ) p_n \\
      &= \sum_{n=0}^{\infty} f_k^{*n} p_n,
\end{align*}
where we let $f_k^{*n}$ be the \hldefn{$n$-fold convolution} of $f_k$ defined as
\begin{equation*}
  f_k^{*n} := P( M_1 + \hdots + M_n = k ).
\end{equation*}

Note that when $k = 0$, we have
\begin{equation*}
  g_0 = \sum_{n = 0}^{\infty} f_0^{*n} p_n = f_0^{*0} p_0 + \sum_{n = 1}^{\infty} f_0^{*n} p_n.
\end{equation*}
Note that
\begin{equation*}
  f_0^{*0} = P(0 = 0) = 1,
\end{equation*}
thus
\begin{align*}
  g_0 &= p_0 + \sum_{n=1}^{\infty} f_0^{*n} p_n \\
      &= p_0 + \sum_{n=1}^{\infty} P(M_1 + \hdots + M_n = 0) p_n \\
      &= p_0 + \sum_{n=1}^{\infty} p_n \prod_{i=1}^{n} P(M = 0) \\
      &= p_0 + \sum_{n=1}^{\infty} p_n \prod_{i=1}^{n} f_0 \\
      &= p_0 + \sum_{n=1}^{\infty} f_0^n p_n.
\end{align*}

Now in general,
\begin{equation*}
  g_k = \sum_{n=0}^{\infty} f_k^{*n} p_n = f_k^{*n} p_0 + \sum_{n=1}^{\infty} f_k^{*n} p_n.
\end{equation*}
Observe that 
\begin{equation*}
  f_k^{*0} = P(0 = k) = 0.
\end{equation*}
Thus
\begin{align*}
  g_k &= \sum_{n=1}^{\infty} f_k^{*n} p_n,
\end{align*}
for $k \geq 1$.

We are still short of an important information: what exactly is $f_k^{*n}$? Fix $n \in \mathbb{N}$. Observe that we only need to look for the value of $n - 1$ of the $M_i$'s, since the sum of the $M_i$'s must equal to some $k$. Then by using the methods from conditional probability
\begin{align*}
  f_k^{*n} &= P(M_1 + \hdots + M_n = k) \\
           &= \sum_{j=0}^{k} P(M_1 + \hdots + M_n = k \mid M_n = j) P(M_n = j) \\
           &= \sum_{j=0}^{k} P(M_1 + \hdots + M_{n - 1} = k - j) f_j \\
           &= \sum_{j=0}^{k} f_{k - j}^{*( n - 1 )} f_j,
\end{align*}
and we observe that we have a recursive definition. Fortunately, this recursive definition has a start where we can work with: note that $f_k^{*1} = P( M_1 = k ) = f_k$.

However, for large values of $n$, this method becomes very cumbersome.

\begin{eg}
  Suppose that $p_0 = 0.4$, $p_1 = 0.4$, and $p_2 = 0.2$. Also, we have $f_0 = 0.5$, $f_1 = 0.3$, and $f_2 = 0.2$. Find the pmf of $S$.
\end{eg}

\begin{solution}
  First, notice that since $N, M$ can take values $0, 1, 2$. Thus $S$ can take values $0, 1, 2, 3, 4$.
  Observe that
  \begin{align*}
    g_0 &= p_0 + \sum_{n=1}^{2} f_0^n p_n = 0.4 + 0.5(0.4) + 0.5^2(0.2) = 0.65 \\
    g_k &= \sum_{n=1}^{2} f_k^{*n} p_n = 0.4 f_k^{*1} + 0.2 f_k^{*2} \\
        &= 0.4 f_k + 0.2 f_k^{*2}, \quad \text{ for } k = 1, 2, 3, 4.
  \end{align*}
  It remains to solve for $f_k^{*2}$, for $k = 1, 2, 3, 4$.
  \begin{align*}
    f_1^{*2} &= P( M_1 + M_2 = 1 ) = P(M_1 = 1, M_2 = 0) + P(M_1 = 0, M_2 = 1) \\
             &= 2f_0 f_1 = 0.3 \\
    f_2^{*2} &= P(M_1 + M_2 = 2) \\
             &= f_0 f_2 + f_1 f_1 + f_2 f_0 = 0.29 \\
    f_3^{*2} &= P(M_1 + M_2 = 3) \\
             &= f_1 f_2 + f_2 f_1 = 0.12 \\
    f_4^{*2} &= P(M_1 + M_2 = 4) \\
             &= f_2 f_2 = 0.04.
  \end{align*}
  We can then obtain $g_k$ for $k = 1, 2, 3, 4$ from here.
\end{solution}

\begin{remark}
  We observe that in the previous example, the grand method of which we derived is rather cumbersome to work with, even if we just have a maximum of $4$.
\end{remark}

Now recall \cref{propo:pgf_method_for_compound_rvs}. We shall try using this. To that end, we first need to get $G_N(t)$ and $G_M(t)$, which is not difficult:
\begin{gather*}
  G_N(t) = 0.4 + 0.4 t + 0.2 t^2 \\
  G_M(t) = 0.5 + 0.3 t + 0.2 t^2.
\end{gather*}
Then


% subsubsection pmf_method (end)

\subsubsection{Panjer's Recursion}%
\label{ssub:panjer_s_recursion}
% subsubsection panjer_s_recursion

\begin{thm}[Panjer's Recursion for $(a, b, 0)$ class]\index{Panjer's Recursion}\label{thm:panjer_s_recursion_for_a_b_0_class}
  For an $(a, b, 0)$ distribution $N$, the pmf of $S$ can be calculated, recursively, by
  \begin{equation*}
    g_k = \frac{1}{1 - af_0} \sum_{i=1}^{k} \left( a + \frac{ib}{k} \right) f_i g_{k - 1}, \quad k = 1, 2, \ldots,
  \end{equation*}
  where the starting point of the recursion is
  \begin{equation*}
    g_0 = G_N(f_0).
  \end{equation*}
\end{thm}

\begin{proof}
  \hlwarn{Proof to be added later.} Reference to add \url{https://www.casact.org/library/astin/vol12no1/22.pdf}
\end{proof}

\begin{eg}
  If $N$ is a Poisson rv, or equivalently, if $S$ is a compound Poisson rv, then
  \begin{equation*}
    g_k = \sum_{i=1}^{k} \frac{ib}{k} f_i g_{k - 1}, \quad k = 1, 2, \ldots.
  \end{equation*}
\end{eg}

\begin{thm}[Panjer's Recursion for $(a, b, 1)$ Class]\index{Panjer's Recursion}\label{thm:panjer_s_recursion_for_a_b_1_class}
  For an $(a, b, 1)$ distribution $N$, the pmf of $S$ can be calculated recursively by
  \begin{equation*}
    g_k = \frac{p_1 - (a + b)p_0}{1 - af_0} f_k + \frac{1}{1 - af_0} \sum_{i=1}^{k} \left( a + \frac{ib}{k} \right) f_i g_{k - i}, \quad k = 1, 2, ...,
  \end{equation*}
  where
  \begin{equation*}
    g_0 = G_N(f_0).
  \end{equation*}
\end{thm}

% subsubsection panjer_s_recursion (end)

% subsection compound_frequency_distributions_continued (end)

% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued (end)

% chapter lecture_18_nov_15th (end)

\chapter{Lecture 19 Nov 20th}%
\label{chp:lecture_19_nov_20th}
% chapter lecture_19_nov_20th

\section{Frequency Distributions --- Creating New Frequency Distributions and Effect on Frequency (Continued 2)}%
\label{sec:frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued_2}
% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued_2

\subsection{Effect on Frequency}%
\label{sub:effect_on_frequency}
% subsection effect_on_frequency

Let $N$ denote the number of claims generated by a portfolio of insurance policies. We shall analize the effects on $N$ from two kinds of adjustments on the portfolio:
\begin{itemize}
  \item exposure adjustment;
  \item policy adjustment.
\end{itemize}

\subsubsection{Exposure Adjustment}%
\label{ssub:exposure_adjustment}
% subsubsection exposure_adjustment

Suppose that the total number of claims of an insurance portfolio consisting $k$ policies in the current year is modelled by a counting number $N$ with pgf $G_N(t)$, and suppose that we do not have the model for the number of claims of each policy.

\newthought{Question}: In the following year, if the number of policies changes to some $k^*$, and we let $N^*$ be the total number of claims in the new year, what is the relationship between the pgfs of $N$ and $N^*$?

\begin{eg}
  Let $N_i$ be the number of claims produced by the $i$\textsuperscript{th} policy. Then we have
  \begin{equation*}
    N = \sum_{i=1}^{k} N_i \text{ and } N^* = \sum_{i=1}^{k^*} N_i.
  \end{equation*}
  Also, suppose that allof the $N_i$'s are iid. Then
  \begin{align*}
    G_N(t) &= E \left[ t^{N_1 + N_2 + \hdots + N_k} \right] = \left( E \left[ t^{N_1} \right] \right)^k \\
    G_{N^*}(t) &= E \left[ t^{N_1 + N_2 + \hdots + N_{k^*}} \right] = \left( E \left[ t^{N_1} \right] \right)^{k^*} \\
               &= \left[ \left( E \left[ t^{N_1} \right] \right)^k \right]^{\frac{k^*}{k}} = \left[ G_N(t) \right]^{\frac{k^*}{k}}.
  \end{align*}
\end{eg}

However, it may not be the case where $\frac{k^*}{k}$ is a ratio such that $\left[ G_N(t) \right]^{\frac{k^*}{k}}$ is still a pgf.

\begin{defn}[Infinitely Divisible]\index{Infinitely Divisible}\label{defn:infinitely_divisible}
  A discrete distribution with pgf $G(t)$ is said to be \hlnoteb{infinitely divisible} if for all $k =1, 2, \ldots,$ the function $\left[ G(t) \right]^{\frac{1}{k}}$ is the pgf of some rv.
\end{defn}

\begin{remark}[Namesake]
  The name comes from the idea that the rv can be ``infinitely divided'' into $k$-many iid rvs, for any $k \in \mathbb{N}$, e.g.
  \begin{equation*}
    X = Y_1 + \hdots + Y_k.
  \end{equation*}
\end{remark}

\begin{remark}
  \begin{enumerate}
    \item If $G(t)^\frac{1}{k}$ is a proper pgf, then so is $G(t)^\frac{n}{k}$.
    \item If $G(t)$ is a proper pgf, then so is $G(t)^n$.
  \end{enumerate}

  The 2nd remark is true since if we let $X$ have the pgf $G(t)$, then $G(t)^n$ is the pgf of $n$-many independent copies of $X$ (i.e. we have an iid sequence $\{ X_i \}_{i = 1}^{n}$ with $X_i \sim X$).
\end{remark}

\begin{eg}
  The Poisson and negative binomial distributions are infinitely divisible, but the binomial distribution is not.
\end{eg}

\begin{solution}
  Let $N \sim \Poi(\lambda)$. Then for any $k \in \mathbb{N} \setminus \{ 0 \}$, we have
  \begin{equation*}
    G_N(t) = e^{\lambda( t - 1 )} \implies \left[ G_N(t) \right]^{\frac{1}{k}} = e^{\frac{\lambda}{k} ( t - 1 )}
  \end{equation*}
  which is a pgf of $\Poi \left( \frac{\lambda}{k} \right)$.

  If $N \sim \NB(\beta, r)$, then for any $k \in \mathbb{N} \setminus \{ 0 \}$, we have
  \begin{equation*}
    G_N(t) = (1 + \beta - \beta t)^{-r} \implies \left[ G_N(t) \right]^{\frac{1}{k}} = ( 1 + \beta - \beta t )^{-\frac{r}{k}}
  \end{equation*}
  which is the pgf of $\NB\left(\beta, \frac{r}{k} \right)$.

  For $N \sim \Bin(q, m)$, notice that for $k \in \mathbb{N} \setminus \{ 0 \}$, we have
  \begin{equation*}
    G_N(t) = (1 - q + qt)^m \implies \left[ G_N(t) \right]^{\frac{1}{k}} = ( 1 - q + qt )^{\frac{m}{k}}.
  \end{equation*}
  However, $\frac{m}{k}$ may not be an integer. For example, if $k = 2m$, then we have
  \begin{equation*}
    G(t) = \left[ G_N(t) \right]^{\frac{1}{m}} = ( 1 - q + qt )^{\frac{1}{2}}.
  \end{equation*}
  Note that $G(t)$ is a power series, since it is \hlnotea{differentiable infinitely}\sidenote{In comparison, the pgf of a binomial distribution is differentiable $m$-times, where $m$ is the counting parameter in the binomial distribution.}. Then in particular, notice that
  \begin{align*}
    \frac{d^2}{dt^2} \left[ G_N(t) \right]^{\frac{1}{2m}}\at{t = 0}{} 
      &= - \frac{1}{4} q^2 ( 1 - q + qt )^{-\frac{3}{2}} \at{t = 0}{} \\
      &= - \frac{1}{4} q^2 ( 1 - q )^{-\frac{3}{2}} < 0.
  \end{align*}
\end{solution}

\begin{eg}[Infinite Divisibility of Compound RVs]
  Recall that for a compound rv $S = \sum_{i=1}^{N} M_i$, we have that
  \begin{equation*}
    G_S(t) = G_N( G_M(t) ).
  \end{equation*}
  Then if \hlimpo{$N$ is infinitely divisible}, then so is $S$, since
  \begin{equation*}
    \left[ G_S(t) \right]^{\frac{1}{k}} = \left[ G_N( G_M(t) ) \right]^{\frac{1}{k}}.
  \end{equation*}
\end{eg}

\begin{remark}
  As a consequence of the last two examples, we have that \hlnotea{compound Poisson} and \hlnotea{compound negative binomial} distributions are infinitely divisible. 
\end{remark}

% subsubsection exposure_adjustment (end)

\subsubsection{Effect of Policy Adjustments}%
\label{ssub:effect_of_policy_adjustments}
% subsubsection effect_of_policy_adjustments

Let $X$ be the ground-up loss and $N$ be the number of losses from a portfolio of risks. The amount paid (either $Y_L$ or $Y_P$)\sidenote{See } is an amount modified from $X$ using policy adjustments.

There are two common ways to define the model reflecting the aggregate payment:
\begin{itemize}
  \item the aggregate payment on a \hlnotea{per-loss basis};
  \item the aggregate payment on a \hlnotea{per-payment basis}.
\end{itemize}

\paragraph{Aggregate Payment on a per-loss basis} Let $N_L$ be the number of payments paid on a per-loss basis. Then the payment of every loss is included, and so we have
\begin{equation*}
  N_L = N.
\end{equation*}
Thus the \hldefn{aggregate payment on a per-loss basis} is
\begin{equation*}
  S = \sum_{i=1}^{N_L} Y_{L, i},
\end{equation*}
where $Y_L, i$ denotes the amount paid for the $i$\textsuperscript{th} loss.

\paragraph{Aggregate Payment on a per-payment basis} Let $N_P$ be the number of payments paid on a per-payment basis. Then only the non-zero payments are counted, and so we have
\begin{equation*}
  N_P = \sum_{i=1}^{N_L} \mathbb{1}_{\{Y_{L, i} > 0\}}.
\end{equation*}
Thus the \hldefn{aggregate payment on a per-payment basis} is
\begin{equation*}
  S = \sum_{i=1}^{N_P} Y_{P, i},
\end{equation*}
where $Y_{P, i}$ denotes the amount paid for the $i$\textsuperscript{th} non-zero payment.

Notice that if we let $I_i = \mathbb{1}_{\{ Y_{L, i} > 0 \}}$ and let $P(Y_L > 0) = \alpha$, then the pgf of $N_P$ is given by (using \cref{propo:pgf_method_for_compound_rvs})
\begin{equation*}
  G_{N_P}(t) = G_{N_L}( G_{I_1}(t) ) = G_{N_L}(1 - \alpha + \alpha t)
\end{equation*}
since $I_i \sim \Bernoulli(\alpha)$.

% subsubsection effect_of_policy_adjustments (end)

% subsection effect_on_frequency (end)

% section frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued_2 (end)

% chapter lecture_19_nov_20th (end)

\chapter{Lecture 20 Nov 22nd}%
\label{chp:lecture_20_nov_22nd}
% chapter lecture_20_nov_22nd

\section{Frequency Distributions --- Creating New Frequency Distirbutions and Effect on Frequency Distribution (Continued 3)}%
\label{sec:frequency_distributions_creating_new_frequency_distirbutions_and_effect_on_frequency_distribution_continued_3}
% section frequency_distributions_creating_new_frequency_distirbutions_and_effect_on_frequency_distribution_continued_3

\subsection{Effect on Frequency (Continued)}%
\label{sub:effect_on_frequency_continued}
% subsection effect_on_frequency_continued

\subsubsection{Effect of Policy Adjustments (Continued)}%
\label{ssub:effect_of_policy_adjustments_continued}
% subsubsection effect_of_policy_adjustments_continued

\begin{eg}
  Assume that $P(Y_L > 0) = \alpha$. Find the corresponding distribution of $N_P$ given the following distributions of $N_L$.
  \begin{enumerate}
    \item $N_L \sim \Poi(\lambda)$;
    \item $N_L \sim \Bin(m, q)$;
    \item $N_L \sim \NB(\beta, r)$.
  \end{enumerate}
\end{eg}

\begin{solution}
  \begin{enumerate}
    \item Given $N_L \sim \Poi(\lambda)$, we have that the pgf of $N_P$ is
      \begin{align*}
        G_{N_P}(t) &= G_{N_L} ( G_{\mathbb{1}_{\{ Y_{L, i} > 0 \}}} (t) ) = G_N( 1 - \alpha + \alpha t ) \\
                   &= e^{\lambda ( 1 - \alpha + \alpha t - 1 )} = e^{ \lambda \alpha ( t - 1 ) }
      \end{align*}
      which is the pgf of $\Poi(\lambda \alpha)$. Thus $N_P \sim \Poi(\lambda \alpha)$.

    \item Given $N_L \sim \Bin(m, q)$, we have that the pgf of $N_P$ is
      \begin{align*}
        G_{N_P}(t) &= G_N( 1 - \alpha + \alpha t ) = ( 1 - q + ( 1 - \alpha + \alpha t ) q )^m \\
                   &= ( 1 - \alpha q + t \alpha q )^m
      \end{align*}
      which is the pgf of $\Bin(m, \alpha q)$. Thus $N_P \sim \Bin(m, \alpha q)$.

    \item Given $N_L \sim \NB(\beta, r)$, we have that the pgf of $N_P$ is
      \begin{equation*}
        G_{N_P}(t) = \left[ 1 - \beta ( 1 - \alpha + \alpha t  - 1) \right]^{-r} = \left[ 1 - \beta \alpha ( t - 1 ) \right]^{-r}
      \end{equation*}
      which is the pgf of $\NB(\beta \alpha, r)$. Thus $N_P \sim \NB(\beta \alpha, r)$.
  \end{enumerate}
\end{solution}

\begin{remark}
  Also, to solve for $N_P$ for when $N_L \sim \Poi(\alpha \lambda)$, recall \cref{eg:thinning_property_of_poisson}.
\end{remark}

% subsubsection effect_of_policy_adjustments_continued (end)

% subsection effect_on_frequency_continued (end)

% section frequency_distributions_creating_new_frequency_distirbutions_and_effect_on_frequency_distribution_continued_3 (end)

\section{Aggregate Risk Models}%
\label{sec:aggregate_risk_models}
% section aggregate_risk_models

\marginnote{Textbook reference: Sections 9.1 - 9.7}

We shall now look back at some of the things that we introduced throughout the course and put them together.

\subsection{Indiviual Risk Model Revisited}%
\label{sub:indiviual_risk_model_revisited}
% subsection indiviual_risk_model_revisited

Recall the \hyperref[defn:individual_risk_model]{individual risk model}. Consider a portfolio consisting of $n$ insurance policies. Let $Z_i$  denote the amount paid for the $i$\textsuperscript{th} policy. The individual risk model for the aggregate amount paid is given by
\begin{equation*}
  S = \sum_{i=1}^{n} Z_i.
\end{equation*}
While we are interested in the distribution of $S$, it is usually difficult to obtain an analytical form for the distribution. 

But if $n$ is large, and $\{ Z_i \}_{i = 1}^{n}$ is an iid sequence, we can apply the \hlnotea{normal approximation} (by the \hlnotea{central limit theorem}) to solve for $S$: in particular, as $n \to \infty$, we have
\begin{equation*}
  \frac{S - E [ S ]}{\sqrt{ \Var(S) }} \overset{d}{\to} \Nor(0, 1),
\end{equation*}
where $\overset{d}{\to}$ means a \hlnotea{convergence in distribution}\sidenote{See \href{https://tex.japorized.ink/STAT330S18/classnotes.pdf}{notes from STAT330}.}

\begin{eg}
  Consider a portfolio of $100$ iid insurance policies. Each policy is applied to a ground-up loss that follows an exponential distirbution with mean $50$. An ordinary deductible $d = 20$ is applied to each policy and the insurer charges premium as $1.1$ multiple of the expected payment amount. Find the probability that the insurer will have a negative profit with this portfolio by using the normal approximation.
\end{eg}

\begin{solution}
  The amount paid for the $i$\textsuperscript{th} policy is $Z_i = [ X_i - 20 ]_+$, where $X_i \sim \Exp(50)$. We want to find
  \begin{equation*}
    P( 1.1 E[S] - S < 0 ) = P ( S > 1.1 E[S] ).
  \end{equation*}
  Since $n$ is large enough (we suppose so), we can use normal approximation:
  \begin{equation*}
    P( S > 1.1 E[S] ) = P \left( \frac{S - E[S]}{\sqrt{ \Var(S) }} > \frac{1.1 E[S] - E[S]}{\sqrt{\Var(S)}} \right)
  \end{equation*}
  where $\frac{S - E[S]}{\sqrt{\Var(S)}} \overset{d}{\to} \Nor(0, 1)$. Thus we need to solve for
  \begin{align*}
    E[S] &= E \left[ \sum_{i=1}^{100} Z_i \right] = 100 E \left[ \left[ X - 20 \right]_+ \right] \\
    \Var(S) &= \Var \left( \sum_{i=1}^{100} Z_i \right) = 100 \Var(Z_i) = 100 \Var( [ X - 20 ]_+ ).
  \end{align*}
  We have
  \begin{align*}
    E [ [ X - 20 ]_+ ] &= \int_{20}^{\infty} e^{-\frac{1}{50}x} \dif{x} = -50e^{-\frac{x}{50}} \at{20}{\infty} = 33.516 \\
    E [ [ X - 20 ]_+^2 ] &= \int_{20}^{\infty} 2(x - 20)e^{-\frac{x}{50}} \dif{x} \\
                         &= -2000e^{-0.4} + 2 \left[ 1000e^{-0.4} + 50 [ 50e^{-0.4} ] \right] \\
                         &= 3351.600
  \end{align*}
  Thus
  \begin{equation*}
    E[S] = 3351.6 \text{ and } \Var(S) = 100 ( 3351.6 - 33.516^2 ) = 222827.7744.
  \end{equation*}
  Therefore
  \begin{align*}
    P(X > 1.1E[S]) &= P \left( \frac{S - E[S]}{\sqrt{\Var(S)}} > \frac{0.1 E[S]}{\sqrt{\Var(S)}} \right) \\
                   &= P \left( \frac{S - E[S]}{\sqrt{\Var(S)}} > \frac{335.16}{\sqrt{ 222827.7744 }} \right) \\
                   &\approx P \left( \frac{S - E[S]}{\sqrt{\Var(S)}} > 0.71 \right) \\
                   &\approx 1 - P \left( Z \leq 0.71 \right) = 0.2389
  \end{align*}
  where $Z \sim \Nor(0, 1)$.
\end{solution}

% subsection indiviual_risk_model_revisited (end)

\subsection{Collective Risk Model Revisited}%
\label{sub:collective_risk_model_revisted}
% subsection collective_risk_model_revisted

Recall the \hyperref[defn:collective_risk_model]{collective risk model}: we define the aggregate loss as a random sum of severity amounts, i.e.
\begin{equation*}
  S = \sum_{i=1}^{N} X_i,
\end{equation*}
where
\begin{itemize}
  \item $N$ is a counting rv,
  \item $\{ X_i \}_{i \geq 1}$ is a sequence of iid severity rvs,
  \item and we follow the convention that $S = 0$ if $N = 0$.
\end{itemize}

We are, again, interested in the aggregate amount paid by the insurer. The collective risk model for the aggregate amount paid can be expressed in two forms, as mentioned in \cref{sec:frequency_distributions_creating_new_frequency_distributions_and_effect_on_frequency_continued_2}.

We first study the distribution of $S$ when the severity distribution $X$ is a \hlnoteb{discrete rv} with pmf
\begin{equation*}
  f_X(x) = P(X = x), \quad x = 0, 1, 2, \ldots.
\end{equation*}
We have several ways to solve for $S$:
\begin{itemize}
  \item Notice that $S$ is also a discrete rv with pmf $f_X(x) = P(S = x)$ for $x = 0, 1, 2, \ldots$, and pgf of $S$ is
    \begin{equation*}
      G_S(t) = G_N( G_X(t) )
    \end{equation*}
    as discussed in \cref{propo:pgf_method_for_compound_rvs}.
  \item $S$ is a compound rv with primary rv $N$ and secondary rv $X$, so we can use methods from \cref{sub:compound_frequency_distributions_continued}.
\end{itemize}

For example, if $N$ is a member of the $(a, b, 0)$ or $(a, b, 1)$ class, we can use \hyperref[thm:panjer_s_recursion_for_a_b_0_class]{Panjer's Recursion} to find the pmf of $S$.

It may be the case that $N$ itself is yet another compound rv with primary rv $N_1$ and secondary rv $N_2$, and in this case the pgf of $S$ would be
\begin{equation*}
  G_S(t) = G_N( G_X(t) ) = G_{N_1}( G_{N_2}( G_X(t) ) ) = G_{N_1}(G_Z(t)),
\end{equation*}
where we let $Z$ be the compound rv with primary rv $N_2$ and secondary rv $X$.

Again, if $N_1$ and $N_2$ are \hlimpo{either members of $(a, b, 0)$ or $(a, b, 1)$}, then we can apply Panjer's Recursion in the following manner:
\begin{itemize}
  \item[Step 1] Apply Panjer's Recursion on $Z$.
  \item[Step 2] Apply Panjer's Recursion on $S$.
\end{itemize}

\paragraph{General Discrete Severity Distribution} Suppose the severity rv $X$ has a pmf 
\begin{equation*}
  f_X(x) = P(X = xh), \quad x = 0, 1, 2, \ldots,
\end{equation*}
where $h > 0$ is some constant\sidenote{Here, $h$ acts as a scale for the values that the rv can take on, allowing us to take non-integer values in a discrete manner.}. In this case, let
\begin{equation*}
  \tilde{S} = \frac{S}{h} = \sum_{i=1}^{N} \tilde{X}_i,
\end{equation*}
where $\{\tilde{X}_i\}_{i \geq 1}$ is a sequence of iid copies such that $\tilde{X}_i = \frac{X_i}{h}$.

% subsection collective_risk_model_revisted (end)

% section aggregate_risk_models (end)

% chapter lecture_20_nov_22nd (end)

\chapter{Lecture 21 Nov 27th}%
\label{chp:lecture_21_nov_27th}
% chapter lecture_21_nov_27th

\section{Aggregate Risk Models Revisited}%
\label{sec:aggregate_risk_models_revisited}
% section aggregate_risk_models_revisited

\subsection{Collective Risk Model Revisited (Continued)}%
\label{sub:collective_risk_model_revisited_continued}
% subsection collective_risk_model_revisited_continued

\paragraph{Continuous Severity Distribution} Suppose that $X$ is a continuous severity rv with pdf $f_X$. Since we usually have that $P(N = 0) > 0$, $S$ is a \hlnoteb{mixture with probability mass at $0$}.

\begin{propo}[MGF of Aggregate Loss of A Continuous Severity Distribution]\label{propo:mgf_of_aggregate_loss_of_a_continuous_severity_distribution}
  The mgf of aggregate loss $S$ is given by
  \begin{equation*}
    M_S(t) = G_N( M_X(t) ),
  \end{equation*}
  where $G_N$ is the pgf of frequency rv $N$, and $M_X$ is the mgf of severity rv $X$.
\end{propo}

\begin{proof}
  By conditioning on $N$, we have
  \begin{align*}
    M_S(t) &= E \left[ e^{tS} \right] = E \left[ E \left[ e^{t ( X_1 + \hdots + X_N )} \mid N \right] \right] \\
           &= E \left[ E \left[ e^{tX_1} \mid N \right] \hdots E \left[ e^{tX_N} \mid N \right] \right] \\
           &= E \left[ E \left[ e^{tX} \right]^N \right] = E \left[ M_X(t)^N \right] = G_N(M_X(t)).
  \end{align*}\qed\
\end{proof}

The distribution of $S$ is often closely related to the \hlnotea{Erlang distribution}, which is a special case of $\Gam(\alpha, \theta)$ with $\alpha$ as a \hlnotec{positive integer}.

\begin{defn}[Erlang Distribution]\index{Erlang Distribution}\label{defn:erlang_distribution}
  A crv $X$ is said to follow an \hlnoteb{Erlang Distribution}, dentoed as $X \sim \Erlang(n, \theta)$, if its pdf is
  \begin{equation*}
    f_X(x) = \frac{\theta^{-n} x^{n - 1} e^{-\frac{x}{\theta}}}{\Gamma(n)}.
  \end{equation*}
\end{defn}

\begin{remark}
  We usually do not have a nice closed form cdf of a Gamma distribution, except when $\alpha$ is an integer. Thus $X \sim \Erlang(n, \theta)$ has the cdf
  \begin{equation*}
    F_X(x) = 1 - \int_{x}^{\infty} \frac{\theta^{-n} y^{n - 1} e^{- \frac{y}{\theta}}}{( n - 1 )!} \dif{y} = 1 - \sum_{k=0}^{n - 1} \frac{\left( \frac{x}{\theta} \right)^k e^{-\frac{x}{\theta}}}{k!}.
  \end{equation*}

  Also, from the mgf of the Gamma distribution, we have that the mgf of the $\Erlang(n, \theta)$ is
  \begin{equation*}
    M(t) = ( 1 - \theta t )^{-n}.
  \end{equation*}
\end{remark}

\begin{eg}
  Suppose that the frequency rv $N$ has pmf $\{ p_k \}_{k = 0}^{\infty}$, and the severity rv $X \sim \Exp(\theta)$. What is the distribution of the aggregate loss $S$?
\end{eg}

\begin{solution}
  By \cref{propo:mgf_of_aggregate_loss_of_a_continuous_severity_distribution}, since $G_N(t) = \sum_{k=0}^{\infty} t^k p_k$ and $M_X(t) = ( 1 - \theta t )^{-1}$, we have
  \begin{align*}
    M_S(t) &= G_N(M_X(t)) = \sum_{k=0}^{\infty} M_X(t)^k p_k = \sum_{k=0}^{\infty} (1 - \theta t)^{-k} p_k \\
           &= p_0 + \sum_{k=1}^{\infty} (1 - \theta t)^{-k} p_k.
  \end{align*}
  Thus we observe that $S$ is a mixture of $0$ and $\Erlang(k, \theta)$, for $k = 1, 2, \ldots$.
\end{solution}

\begin{note}
  Note that since $S$ is a mixture, we can obtain its pf and cdf by taking appropriate weights on each of the distributions in its mixture.
\end{note}

\paragraph{Normal Approximation of Collective Risk Model} We can also use the normal approximation method for a collective risk model, in particular, given $S = \sum_{i=1}^{N} X_i$ for crvs $X_i$'s, we have
\begin{equation*}
  P(S \leq x) \approx P \left( Z \leq \frac{x - E [S]}{\sqrt{\Var( S )}} \right),
\end{equation*}
where $Z \sim \Nor(0, 1)$. If the primary distribution $N$ is from the $(a, b, 0)$ class, then the conditions required for applying the normal approximation is
\begin{itemize}
  \item large $\lambda$;
  \item large $m$; and
  \item large $r$
\end{itemize}
for $N \sim \Poi(\lambda)$, $N \sim \Bin(m ,q)$, and $N \sim \NB(r, q)$, respectively.

\begin{eg}
  Let $X \sim \Exp(2)$, and $N \sim \Poi(50)$. Use the normal approximation to find the $95\%$ quantile of $S$.
\end{eg}

\begin{solution}
  Let $X_i \sim X$, for $i = 1, 2, \ldots, N$. Then by \cref{propo:mean_and_variance_of_the_compound_rv}, we have
  \begin{equation*}
    E [S] = E \left[ \sum_{i=1}^{N} X_i \right] = E[N] E[X] = 100,
  \end{equation*}
  and
  \begin{align*}
    \Var(S) &= \Var \left( \sum_{i=1}^{N} X_i \right) = E \left[ \Var \left( \sum_{i=1}^{N} X_i \mid N \right) \right] + \Var \left( E \left[ \sum_{i=1}^{N} X_i \mid N \right] \right) \\
            &= E[N] \Var(X) + \Var(N) E[X]^2 = 400.
  \end{align*}
  The $95\%$ quantile, $\pi_{0.95}$, is such that
  \begin{equation*}
    P( S < \pi_{0.95} ) \leq 0.95 \leq P ( S \leq \pi_{0.95} ).
  \end{equation*}
  Then
  \begin{align*}
    0.95 &= F_S( \pi_{0.95} ) = P (S \leq \pi_{0.95}) \\
         &= P \left( \frac{S - E[S]}{\sqrt{\Var(S)}} \leq \frac{\pi_{0.95} - E[S]}{\sqrt{\Var(S)}} \right) \\
         &\approx P \left( Z \leq \frac{\pi_{0.95} - 100}{\sqrt{400}} \right) = \Phi \left( \frac{\pi_{0.95} - 100}{20} \right).
  \end{align*}
  Thus
  \begin{equation*}
    \frac{\pi_{0.95} - 100}{20} = \Phi^{-1}(0.95) = 1.645
  \end{equation*}
  and so
  \begin{equation*}
    \pi_{0.95} = 121.29.
  \end{equation*}
\end{solution}

\subsubsection{Discretization Method}%
\label{ssub:discretization_method}
% subsubsection discretization_method

The normal approximation has limitations: e.g. how large is large for each of the required parameters.

\begin{defn}[Discretization Method]\index{Discretization Method}\label{defn:discretization_method}
  The \hlnoteb{discretization method} is to approximate the distribution of a continuous (or mixed) severity rv $X$ by the distribution of a discrete severity rv $\hat{X}$, which has mass points at $\{ 0, h, 2h, \ldots \}$. The pmf of $\hat{X}$ is
  \begin{gather*}
    P \left( \hat{X} = 0 \right) = P \left( 0 \leq X \leq \frac{h}{2} \right) \\
    P \left( \hat{X} = kh \right) = P \left( kh - \frac{h}{2} < X \leq kh + \frac{h}{2} \right)
  \end{gather*}
  for $k = 1, 2, \ldots$.
\end{defn}

\begin{remark}
  Recall that the aggregate payment can be expressed as
  \begin{equation*}
    S = \sum_{i=1}^{N_L} Y_{L, i} \text{ or } S = \sum_{i=1}^{N_P} Y_{P, i}.
  \end{equation*}
  Since the amount paid per payment, $Y_P$, is usually continuous (especially so at $0$), we usually apply the discretization method to $Y_P$, and then obtain an approximation to the distribution of $S$.
\end{remark}

% subsubsection discretization_method (end)

% subsection collective_risk_model_revisited_continued (end)

% section aggregate_risk_models_revisited (end)

% chapter lecture_21_nov_27th (end)

\chapter{Lecture 22 Nov 29th}%
\label{chp:lecture_22_nov_29th}
% chapter lecture_22_nov_29th

\section{Aggregate Risk Models Revisited (Continued)}%
\label{sec:aggregate_risk_models_revisited_continued}
% section aggregate_risk_models_revisited_continued

\subsection{Collective Risk Model Revisited (Continued 2)}%
\label{sub:collective_risk_model_revisited_continued_2}
% subsection collective_risk_model_revisited_continued_2

\subsubsection{Discretization Method (Continued)}%
\label{ssub:discretization_method_continued}
% subsubsection discretization_method_continued

\begin{eg}
  Assume that the number of losses $N \sim \Poi(3)$. The ground-up loss $X \sim \Pareto(4, 10)$. An individual loss limit of $15$ and an ordinary deductible of $6$ are applied to each loss. Determine the distribution of the discretized aggregate payment with $h = 2.5$.
\end{eg}

\begin{solution}
  \marginnote{The goal is to derive the distribution of a discretized aggregate payment $\hat{S} = \sum_{i=1}^{N_P} \hat{Y}_{P, i}$. To that end, we need to figure out the secondary distribution $\hat{Y}_{P, i}$ and the primary distribution $N_P$. For each of the $\hat{Y}_{P, i}$'s, we need to use the discretized method:
    \begin{equation*}
      P \left( \hat{Y}_{P, i} = 0 \right) = P\left( 0 \leq Y_{P, i} < \frac{h}{2} \right) \\
    \end{equation*}
    \begin{align*}
      & P \left( \hat{Y}_{P, i} = k \right) \\
      &= P \left( \frac{(2k - 1)h}{2} \leq Y_{P. i} < \frac{(2k + 1)h}{2} \right)
    \end{align*}
    We know what $Y_{P, i}$ is from $Y_{L, i}$. To find out what $N_P$ is, we are given that $N_L = N \sim \Poi(3)$, and so we may use the relation
    \begin{equation*}
      G_{N_p}(t) = G_{N_L} ( 1 - \alpha + \alpha t ),
    \end{equation*}
    where $\alpha$ is the probability that a loss occurs.
}
  Let $Y_L = [ ( X \land 15 ) - 6 ]_+$, let $\{ X_i \}$ be such that $X_i \sim X$, and $\{ Y_{L, i} \}$ be such that $Y_{L, i} = [ ( X_i \land 15 ) - 6 ]_+$ for each $i$. It follows that by this construction, we have $Y_{L, i} \sim Y_L$. Now, since $X \sim \Pareto(4, 10)$, the survival function of $X$ is
  \begin{equation*}
    \bar{F}_X(x) = \left( \frac{\theta}{x + \theta} \right)^\alpha = \left( \frac{10}{x + 10} \right)^4.
  \end{equation*}
  It follows that the survival function of $Y_L$ is
  \begin{equation*}
    \bar{F}_{Y_L}(y) = \begin{cases}
      1                              & y < 0 \\
      \bar{F}_X \left( y + 6 \right) & 0 \leq y < 9 \\
      0                              & y \geq 9
    \end{cases} = \begin{cases}
      1                                  & y < 0 \\
      \left( \frac{10}{y + 16} \right)^4 & 0 \leq y < 9 \\
      0                                  & y \geq 9
    \end{cases}
  \end{equation*}
  Consequently, we have that
  \begin{equation*}
    \bar{F}_{Y_P}(y) = \begin{cases}
      1 & y < 0 \\
      \frac{\left( \frac{10}{y + 16} \right)^4}{\left( \frac{10}{16} \right)^4} = \left( \frac{16}{y + 16} \right)^4 & 0 \leq y < 9 \\
      0 & y \geq 9
    \end{cases}
  \end{equation*}
  The discretized version of each of the $Y_{P, i}$'s is
  \begin{gather*}
    f_{\bar{Y}_p / h} (0) = P \left( \hat{Y}_P = 0 \right) = F_{Y_P} \left( \frac{h}{2} \right) = 1 - \bar{F}_{Y_P} \left( \frac{h}{2} \right) \\
    f_{\hat{Y}_P / h} (k) = P \left( \hat{Y}_P = kh \right) = \bar{F}_{Y_P} \left( \frac{( 2k - 1 ) h}{2} \right) - \bar{F}_{Y_P} \left( \frac{( 2k + 1 ) h}{2} \right).
  \end{gather*}
  Now, notice that the probability of a loss occuring is
  \begin{equation*}
    \bar{F}_{Y_L} (0) = \left( \frac{10}{16} \right)^4 = \left( \frac{5}{8} \right)^4 =: \lambda.
  \end{equation*}
  Therefore, since $N_P = \sum_{i=1}^{N_L} \mathbb{1}_{\{Y_L > 0\}}$, we have
  \begin{equation*}
    G_{N_P} (t) = G_{N_L} ( 1 - \lambda + \lambda t ) = e^{3 ( 1 - \lambda + \lambda t - 1 )} = e^{3 \lambda ( t - 1 )},
  \end{equation*}
  and so $N_P \sim \Poi(3 \lambda)$. Using \hyperref[thm:panjer_s_recursion_for_a_b_0_class]{Panjer's Recursion} for the $(a, b, 0)$ class (as $N_P \sim \Poi(3 \lambda)$) on $\frac{\hat{S}}{h}$, where $\hat{S}$ is the discretized version of $S = \sum_{i=1}^{N_P} Y_{P, i}$, we have
  \begin{gather*}
    g_{\hat{S} / h} (0) = G_{N_P} \left( f_{\hat{Y}_P / h} (0) \right) = e^{3 \lambda \left( f_{\hat{Y}_P / h} (0) - 1 \right)} \\
    g_{\hat{S} / h} (k) = \sum_{i=1}^{k} \frac{3 \lambda i}{k}f_{\hat{Y}_P / h} (i) g_{\hat{S} / h} (k - i).
  \end{gather*}
\end{solution}

% subsubsection discretization_method_continued (end)

\subsubsection{Other Insurance Models based on the Collective Risk Model}%
\label{ssub:other_insurance_models_based_on_the_collective_risk_model}
% subsubsection other_insurance_models_based_on_the_collective_risk_model

\begin{defn}[Excess-of-loss Insurance]\index{Excess-of-loss Insurance}\label{defn:excess_of_loss_insurance}
  Given a collective risk model $S = \sum_{i=1}^{N} X_i$, if an ordinary deductible $d$ is applied to each risk, then the aggregate loss covered by the insurer is
  \begin{equation*}
    S^* = \sum_{i=1}^{N} [ X_i - d ]_+.
  \end{equation*}
  Such a contract is known as an \hlnoteb{excess-of-loss insurance}.
\end{defn}

\begin{note}
  By \cref{propo:mean_and_variance_of_the_compound_rv}, for an excess-of-loss insurance, we have
  \begin{gather*}
    E \left( S^* \right) = E (N) E \left[ [ X - d ]_+ \right] \\
    \Var \left( S^* \right)= E(N) \Var\left( [ X - d ]_+ \right) + \Var(N) E \left[ [ X - d ]_+ \right]^2.
  \end{gather*}
\end{note}

\begin{defn}[Stop-loss Insurance]\index{Stop-loss Insurance}\label{defn:stop_loss_insurance}
  Given a collective risk model $S = \sum_{i=1}^{N} X_i$, a \hlnoteb{stop-loss insurance} is an insurance where an ordinary deductible $d$ is applied on the aggregate loss, i.e.
  \begin{equation*}
    S^* = [ S - d ]_+ = \left[ \sum_{i=1}^{N} X_i - d \right]_+.
  \end{equation*}
\end{defn}

\begin{note}
  By \cref{propo:expected_value_of_the_policy_adjustments}, we have that
  \begin{equation*}
    E \left[ S^* \right] = E \left[ [ S - d ]_+ \right] = \int_{d}^{\infty} \bar{F}_S(x) \dif{x}.
  \end{equation*}
\end{note}

\begin{propo}[A Recursive Formula for the Expected Value of a Stop-loss Insurance]\label{propo:a_recursive_formula_for_the_expected_value_of_a_stop_loss_insurance}
  Let $S$ be discrete, taking values in $\mathbb{N}$. Then $E [ [ S - d ]_+ ]$ can be calculated recursively by
  \begin{equation*}
    E [ [ S - ( d + 1 ) ]_+ ] = E [ [ S - d ]_+ ] - \bar{F}_S(d),
  \end{equation*}
  for $d = 0, 1, \ldots$, starting from $E(S)$.
\end{propo}

\begin{proof}
  Note that for any $x \in \mathbb{R}_{\geq 0}$, we have $x \in (d, d + 1)$ for some $d \in \mathbb{N} \cup \{ 0 \}$. Thus $P ( S > x ) = P ( S > d ) = \bar{F}_S(d)$. Following this, we have that
  \begin{align*}
    E [ [ S - d ]_+ ] &= \int_{d}^{\infty} \bar{F}_S(x) \dif{x} = \int_{d}^{d + 1} \bar{F}_S(x) \dif{x} + \int_{d + 1}^{\infty} \bar{F}_S(x) \dif{x} \\
                      &= \int_{d}^{d + 1} \bar{F}_S(d) \dif{x} + E [ [ S - ( d + 1 ) ]_+ ] \\
                      &= E [ [ S - ( d + 1 ) ]_+ ] + \bar{F}_S(d),
  \end{align*}
  as is required.\qed\
\end{proof}

\begin{eg}
  Let $S = \sum_{i=1}^{N} X_i$, where $N \sim \Poi(4)$, and $X$ has pmf $p_1 = \frac{3}{4}$ and $p_2 = \frac{1}{4}$. Calculate $E [ [ S - 2 ]_+ ]$.
\end{eg}

\begin{solution}
  Note that we have $E[S] = E[N]E[X] = 4 \cdot \left[ \frac{3}{4} + \frac{1}{4} \right] = 5$, and
  \begin{equation*}
    \bar{F}_S(0) = P ( S > 0 ) = P ( N > 0 ) = 1 - e^{-4},
  \end{equation*}
  where the second equality follows from knowing that $S > 0$ if $N > 0$. Thus
  \begin{equation*}
    E [ [ S - 1 ]_+ ] = 5 - 1 + e^{-4}= 4 + e^{-4}.
  \end{equation*}
  Now
  \begin{align*}
    \bar{F}_S(1) &= P( S > 1 )  = 1 - P ( S = 0 ) - P ( S = 1 ) \\
                 &= 1 - e^{-4} - P ( N = 1, X = 1 ) \quad \footnotemark \\
                 &= 1 - e^{-4} - 4e^{-4} \cdot \frac{3}{4} \\
                 &= 1 - 4e^{-4}.
  \end{align*}
  \footnotetext{This follows for $N = 1 , X = 1$ is the only possibility for when $S = 1$.} Thus
  \begin{equation*}
    E [ [ S - 2 ]_+ ] = 4 + e^{-4} - 1 + 4 e ^{-4} = 3 + 5 e^{-4}.
  \end{equation*}
\end{solution}

% subsubsection other_insurance_models_based_on_the_collective_risk_model (end)

% subsection collective_risk_model_revisited_continued_2 (end)

% section aggregate_risk_models_revisited_continued (end)

% chapter lecture_22_nov_29th (end)

\appendix

\chapter{Problem Set 1}%
\label{chp:problem_set_1}
% chapter problem_set_1

\begin{enumerate}
  \item Suppose that the random variable $X$ has density
    \begin{equation*}
      f_X(x) = \frac{\beta ( \beta + 1 ) \alpha^\beta x}{( \alpha + x )^{\beta + 2}}, \quad x > 0,
    \end{equation*}
    where $\alpha > 0$ and $\beta > 1$.
    \begin{enumerate}
      \item Determine the cdf of $X$.
      \item Determine $E[X]$.
      \item Determine the hazard rate function, $h(x)$.
      \item Determine the mean excess loss function $e_X(x)$.
    \end{enumerate}

  \item Suppose that the loss rv $X$ is an equal mixture of an exponential distribution with mean $\frac{1}{2}$ and a gamma distribution with parameters $\alpha = 3$ and $\theta = \frac{1}{2}$.
    \begin{enumerate}
      \item Determine the density function of $X$.
      \item Determine the survival function of $X$.
      \item Determine the hazard rate of $X$.
      \item Determine the mean excess loss function.
    \end{enumerate}

  \item Suppose that $X \mid \Lambda = \lambda \sim \Exp(\lambda)$, and $\Lambda$ has the density function
    \begin{equation*}
      g_\Lambda (\lambda) = \frac{\lambda^{p - 1}}{\Gamma(p) \Gamma( 1 - p ) ( v - \lambda )^p}, \quad 0 < \lambda < v,
    \end{equation*}
    where $v > 0$ and $0 < p < 1$. Show that the unconditional distribution of $X$ is $\Gam(p, v)$.

  \item A discrete rv $N$ with probability mass function
    \begin{equation*}
      p_n = \frac{1}{n} \left( \frac{\beta}{1 + \beta} \right)^n \frac{1}{\ln ( 1 + \beta )}, \quad n \in \mathbb{N} \setminus \{ 0 \},
    \end{equation*}
    for $\beta > 0$ is said to have a logarithmic distribution.
    \begin{enumerate}
      \item Find the probability generating function (pgf) $(G(t)$ of $N$. (\textbf{Hint}: use the Taylor series $- \ln ( 1 0 x ) = \sum_{n=1}^{\infty} \frac{x^n}{n}$)
      \item Using the same pgf above to determine $E[N]$ and $\Var(N)$. (\textbf{Hint}: use \cref{defn:factorial_moments_from_PGF}.)
    \end{enumerate}

  \item In this course, we often encounter the computation of integrals such as $\int_{0}^{x} t^2 e^{-t} \dif{t} $ and $\int_{x}^{\infty} t^2 e^{-t} \dif{t}$. The following formula can be used to expedite the computation:
    \begin{equation*}
      \int_{x}^{\infty} t^n e^{-t} \dif{t} = \sum_{k=0}^{n} \frac{n!}{k!}x^k e^{-x}, \quad n = 0, 1, \ldots.
    \end{equation*}
    Also, note that the Gamma function is defined as $\Gamma(\alpha) = \int_{0}^{\infty} e^{\alpha - 1} e^{-t} \dif{t}$ for $\alpha > 0$.
    \begin{enumerate}
      \item Prove the identity above by induction.
      \item Use the identity to show $\Gamma( n + 1 ) = n!$ for $n = 0, 1, \ldots$. Then, show that
        \begin{equation*}
          \int_{0}^{x} t^n e^{-t} \dif{t} = \Gamma (n + 1) - \int_{x}^{\infty} t^n e^{-t} \dif{t} = n! - \sum_{k=0}^{n} \frac{n!}{k!}x^k e^{-x}.
        \end{equation*}
      \item Consider a loss rv $X \sim \Gam(\alpha, \theta)$ with $\alpha = 3$ and $\theta = 2$, so that it has the pdf
        \begin{equation*}
          f(x) = \frac{\left( \frac{x}{2} \right)^3 e^{-\frac{x}{2}}}{x \Gamma(3)}, \quad x > 0.
        \end{equation*}
        Compute $\Var(X)$.
    \end{enumerate}
\end{enumerate}

% chapter problem_set_1 (end)

\chapter{Problem Set 3}%
\label{chp:problem_set_3}
% chapter problem_set_3

\textit{Problem Set 2 was a bunch of questions from the recommended text.} 

\begin{enumerate}
  \item Suppose that the ground-up loss rv $X$ has the distribution function
    \begin{equation*}
      F_X(x) = 1 - \left( 1 - \frac{x}{\theta} \right)^\alpha, \quad 0 \leq x \leq \theta, \enspace \alpha > 0.
    \end{equation*}
    An ordinary deductible $d < \theta$ is applied to each loss.
    \begin{enumerate}
      \item Show that the distribution of the amount paid per payment $Y_P$ is the same as the distribution of $X$ with $\theta$ replaced by $\theta - d$.
      \item Is $X$ an IMRL or DMRL distribution? Justify.
      \item Determine the loss elimination ratio.
    \end{enumerate}

  \item The density function of the ground-up loss rv $X$ is given by
    \begin{equation*}
      f_X(x) = \frac{1}{100} \left( q + \frac{1}{100} ( 1 - q ) x \right) e^{- \frac{x}{100}}, \quad x > 0,
    \end{equation*}
    where $0 < q < 1$. With an ordinary deductible of $100$, the expected value of amount paid per payment is known to be $125$. Determine the expected value of amount paid per payment if the deductbile level is adjusted to $200$.

  \item The cdf of a ground-up loss rv $X$ is given by
    \begin{equation*}
      F_X(x) = \frac{2x + x^2}{2b + b^2}, \quad 0 \leq x < b.
    \end{equation*}
    \begin{enumerate}
      \item Does $X$ have a DFR, IFR, or neither? Justify your answer.
      \item Is $X$ a IMRL distribution, DMRL distribution, or neither? Justify your answer.
      \item Assume that all losses are subject to a Franchise deductible of $d$ with $d < b$. Find the mean of amount paid per loss.
    \end{enumerate}

  \item Suppose that the ground-up loss rv $X$ is defined as $X = \frac{1}{Y}$, where $Y$ follows an exponential distribution with the pdf
    \begin{equation*}
      f_Y(t) = 5e^{-5y}, \quad y > 0.
    \end{equation*}
    Assume that a policy limit of $100$, and an ordinary deductible of $10$ are applied to all losses. Determine the probability that the amount paid by the insurer on a per payment basis is less than or equal to $50$.

  \item Assume that the ground-up loss rv $X \sim \Unif(0, 650)$. Suppose there is an ordinary deductible of $150$.
    \begin{enumerate}
      \item Find the survival function of the amount paid per loss.
      \item Find the survival function of the amount paid per payment.
      \item Find the mean excess loss $e_X(150)$.
      \item Calculate $\Var( ( X - 150 )^2 \mid X > 150 )$.
    \end{enumerate}

  \item let $X$ be the ground-up loss for the current year for an insurer with the following pdf
    \begin{equation*}
      f_X(x) = \frac{375000}{x^4}, \quad x > 50.
    \end{equation*}
    \begin{enumerate}
      \item Suppose that a Franchise deductible of $100$ is applied to the loss. Find $\bar{F}_{Y_L}(y), \bar{F}_{Y_P}(y), E[Y_L]$ and $E[Y_P]$.
      \item Suppose that an ordinary deductible of $150$ is applied to the loss. Calculate the loss elimination ratio for the current year.
      \item Suppose that an annual inflation rate of $5\%$ will prevail. The insurer would like to model its ground-up loss by $1.05X$ for the next year. To keep the same loss elimination ratio for the next year, what should be the ordinary deductible for $d$ for the next year?
      \item Suppose that the insurer institutes an ordinary deductible of $100$, a coinsurance of $85\%$, and a \textbf{maximum payment} of $1,700$ in the current year. Calculate $\bar{F}_{Y_L}(y), \Var(Y_L)$ and the probability that the amount paid by the insurer on a per payment basis will exceed $50$.
    \end{enumerate}

  \item Total claims for a health plan have a Pareto distribution with $\alpha = 2$ and $\theta = 500$. The health plan implements an increase to physicians that will pay a bonus of $50\%$ of the amount by which total claims are less than $500$; otherwise no bonus is paid. It is anticipated that with the incentive plan, the claim distribution will change to become Pareto with $\alpha = 2$ and $\theta = K$. With the new distribution, it turns out that the expected claims plus the expected bonus is equal to the expected claims prior to the bonus system. Determine the value of $K$.

  \item Losses follow a Pareto distribution with $\alpha = 2$ and $\theta = 5000$. An insurance policy pays the following for each loss. There is no insurance payment for the first $1000$. For losses between $1000$ and $6000$, the insurance pays $80\%$. Losses above $6000$ are paid by the insured until the insured has made a total payment of $10000$. For any remaining part of the loss, the insurance pays $90\%$. Determine the expected insurance payment per loss.
\end{enumerate}

% chapter problem_set_3 (end)

\chapter{Problem Set 4}%
\label{chp:problem_set_4}
% chapter problem_set_4


\begin{enumerate}
  \item Suppose that $N \mid \Lambda = \lambda \sim \Poi(\lambda)$, and $\Lambda \sim \Unif(0, 5)$. Determine the probability that $N \geq 2$.
  \item Let $N$ be the number of claims of an insurance portfolio. Assume that $N \mid \Theta = \theta \sim \NB(\theta, 5)$, and $\Theta \sim \Unif(0, 8)$.
    \begin{enumerate}
      \item Calculate the expectation of the number of claims.
      \item Calculate the variance of the number of claims.
      \item Calculate the probability that there are at least two claims.
    \end{enumerate}
  \item Assume that $X \mid \Theta = \theta \sim \Bernoulli(\theta)$, and $\Theta \sim \BetaDist(1, 3, 1)$.
    \begin{enumerate}
      \item Show that $X$ follows a Bernoulli distribution and identify its parameter.
      \item Show that the conditional distribution of $\Theta$, given $X = 0$, is a beta distribution and identify the parameters for this beta distribution.
    \end{enumerate}
  \item Suppose that $\Lambda \sim \Gam(\alpha, \theta)$ and $N \mid \Lambda = \lambda \sim \Poi(\lambda + \mu)$. We denote the pmf of $N$ by $\{ p_k \}_{k \in \mathbb{N} \cup \{ 0 \} }$.
    \begin{enumerate}
      \item Show that $N$ has the pgf
        \begin{equation*}
          G_N(t) = e^{\mu( t - 1 )} [ 1 - \theta(t - 1) ]^{-\alpha}.
        \end{equation*}
      \item Find $p_0$.
      \item Show that $G'_N(t) = [ \mu + \alpha \theta ( 1 + \theta - \theta t )^{-1} ] G_N(t)$, and hence
        \begin{equation*}
          ( 1 + \theta ) G'_N(t) = \theta t G'_N(t) + [ \mu ( 1 + \theta ) + \alpha \theta ] G_N(t) - \mu \theta t G_N(t).
        \end{equation*}
      \item Use the above identity to show that
        \begin{gather*}
          p_i = \left( \mu + \frac{\alpha \theta}{1 + \theta} \right)p_0 \\
          p_{k + 1} = \frac{[\mu ( 1 + \theta ) + ( \alpha + k )\theta]p_k - \mu \theta p_{k - 1}}{(1 + \theta)(k  +1)}, \enspace k = 1, 2, \ldots.
        \end{gather*}
    \end{enumerate}

  \item Consider a counting rv $N$ with pmf
    \begin{equation*}
      p_0 = 0.2, \, p_1 = 0.3, \, p_2 = 0.3, \, \text{ and } p_3 = 0.2.
    \end{equation*}
    Is $N$ is member of the $(a, b, 0)$ class? Justify.

  \item Let $N$ be a member of the $(a, b, 0)$ class. Prove that
    \begin{equation*}
      E [ N ] = \frac{a + b}{1 - a}.
    \end{equation*}
    \textbf{Hint}: Show that
    \begin{equation*}
      kp_k = a(k - 1) p_{k - 1} + ( a + b ) p_{k - 1}, \quad k = 1, 2, \ldots.
    \end{equation*}
\end{enumerate}

% chapter problem_set_4 (end)

\chapter{Problem Set 5}%
\label{chp:problem_set_5}
% chapter problem_set_5

\begin{enumerate}
  \item Let $N$ be a logarithmic rv with pmf
    \begin{equation*}
      p_k = \frac{-1}{k \ln ( 1 - \beta )} \beta^k, \quad k = 1, 2, 3, \ldots,
    \end{equation*}
    for $0 < \beta < 1$.
    \begin{enumerate}
      \item Show that the distribution of $N$ is an $(a, b, 1)$ member. Identify the parameters of $a$ and $b$.
      \item Show that the pgf of $N$ is
        \begin{equation*}
          G_N(t) = \frac{\ln(1 - \beta t)}{\ln( 1 - \beta )}.
        \end{equation*}
      \item Let $M$ be an rv with pgf
        \begin{equation*}
          G_M(t) = G_N(1 - \alpha + at),
        \end{equation*}
        where $0 < \alpha < 1$. Show that the pgf of $M$ can be expressed as
        \begin{equation*}
          G_M(t) = q + ( 1 - q ) \frac{\ln( 1 - \beta^* t )}{\ln( 1 - \beta^* )},
        \end{equation*}
        where
        \begin{equation*}
          q = \frac{\ln( 1 - \beta( 1 - \alpha ) )}{\ln ( 1 - \beta )} \text{ and } \beta^* = \frac{\alpha\beta}{1 - \beta(1 - \alpha)}.
        \end{equation*}
      \item From the above, comment on the distribution of the rv $M$.
    \end{enumerate}

  \item Let $N$ be the logarithmic rv with pmf
    \begin{equation*}
      p_k = \frac{\beta^k}{k ( 1 + \beta )^k\ln(1 + \beta)},
    \end{equation*}
    and parameter $\beta = 10$. Consider its zero-modified version $N^M$ with $p_0^M = 0.1$.
    \begin{enumerate}
      \item Find $P\left(N^M > 2\right)$.
      \item Find $E \left[ N^M \right]$.
      \item Find $\Var \left( N^M \right)$.
    \end{enumerate}

  \item Assume that the number of losses $N_L$ have a zero-truncated Poisson rv with parameter $\lambda$.
    \begin{enumerate}
      \item Identify the pmf and the pgf of $N_L$.
      \item Assume that a loss results in a positive payment with probability $\alpha$ (independently of each other and of the total number of losses). Show that the distribution of the number of positive payments is a zero-modified Poisson rv with parameter $\alpha \lambda$, and identity the probability at $0$.
      \item Determine the mean and variance of the number of positive payments.
    \end{enumerate}

  \item Suppose that $N$ has the following mixed Poisson distribution with $N \mid \Theta = \theta \sim \Poi(\theta)$ and $\Theta$ has the pdf
    \begin{equation*}
      g(\theta) = \frac{1\beta}{e^{-\frac{1}{\beta}( \theta - \lambda )}}, \quad \theta > \lambda,
    \end{equation*}
    where $\lambda, \beta > 0$.
    \begin{enumerate}
      \item Prove that
        \begin{equation*}
          P(N = n) = \frac{e^{-\lambda}}{1 + \beta} \sum_{k=0}^{n} \frac{1}{( n - k )!}\lambda^{n - k} \left( \frac{\beta}{1 + \beta} \right)^k, \quad n = 0, 1, \ldots.
        \end{equation*}
      \item Find the moment generating function of $\Theta$.
      \item Comment on the distribution of $N$.
      \item Calculate $E[N]$ and $\Var(N)$.
      \item Consider a ground-up loss rv $X$ which has a lognormal distribution with paramters $\mu = 3.3$ and $\sigma = 2.5$. Suppose a Franchise deductible of $200$ is imposed. If the number of losses follows the same distribution as $N$, identify the distribution of the number of (nonzero) payments.
    \end{enumerate}

  \item Consider the compound rv $S$ with primary distribution $N$ and secondary distribution $M$. The primary distribution $N$ is a zero-truncated negative binomial rv with $\beta = 1$ and $r = 3$. The pmf of $M$ is given by $f_0 = 0.1, \, f_1 = 0.65$ and $f_2 = 0.25$. Compute the pmf of $S$, i.e. $g_k = P(S = k)$ for $k = 0, 1, 2, 3$.

  \item The cdf of a ground-up loss $X$ is given by
    \begin{equation*}
      F_X(x) = 1 - \frac{1458000(45 + 2x)}{( 90 + x )^4}, \quad x \geq 0.
    \end{equation*}
    The insurance policy calls for an ordinary deductible of $30$ to be imposed. It is assumed that the number of payments $N_P$ has a logarithmic distribution with $\beta = \frac{7}{4}$.
    \begin{enumerate}
      \item Determine the expected number of payments.
      \item Find the survival function of the amount paid per payment $Y_P$.
      \item Discretize the severity distribution of $Y_P$ using the method of rounding with a span of $h = 30$. Determine the probability that $P \left( \hat{Y}_P = 90 \right)$, where $\hat{Y}_P$ is the discretized version of $Y_P$ with $h = 30$.
      \item Use Panjer's recursion to calculate the discretized distribution of aggregate payments up to a discretized amount paid of $90$.
    \end{enumerate}

  \item The cdf of a ground-up loss is given by
    \begin{equation*}
      F_X(x) = 1 - e^{- \left( \frac{x}{100} \right)^3}, \quad x \geq 0.
    \end{equation*}
    You are given that $N_L \sim \NB(2, 1.5)$. Suppose that an ordinary deductible of $50$ is applied to each individual loss.
    \begin{enumerate}
      \item Determine the distribution of the number of payments $N_P$.
      \item Find the survival function of the amoubnt paid per payment $Y_P$.
      \item Determine the probability that $P \left( \hat{Y}_P = 120 \right)$, where $\hat{Y}_P$ is the discretized version of $Y_P$ with $h = 40$.
      \item Use Panjer's recursion to calculate the discretized distribution of aggregate payments up to a discretized amount paid of $120$.
    \end{enumerate}

  \item Consider the compound rv $S$ with secondary distribution $X \sim \Exp(100)$, and primary distribution $N$ with pmf
    \begin{center}
      \begin{tabular}{c | c | c | c | c}
        $n$   & $0$    & $1$   & $2$    & $3$ \\
        \hline
        $p_n$ & $0.35$ & $0.3$ & $0.25$ & $0.1$
      \end{tabular}
    \end{center}
    \begin{enumerate}
      \item Calculate $P(S > 250)$ using the pmf method. You are given that the distribution of $n$-convolution follows $\Exp(100)$, i.e.
        \begin{equation*}
          P ( X_1 + \hdots + X_n > x ) = e^{-\frac{x}{100}} \sum_{j=1}^{n-1} \frac{\left( \frac{x}{100} \right)^j}{j!}.
        \end{equation*}
      \item Calculate $P(X > 250)$ using the normal approximation.
    \end{enumerate}
\end{enumerate}

% chapter problem_set_5 (end)

\chapter{Additional Material}%
\label{chp:additional_material}
% chapter additional_material

\section{Individual Risk Model: An Alternate View}%
\label{sec:individual_risk_model_an_alternate_view}
% section individual_risk_model_an_alternate_view

\textit{This appendix serves to explain why our note of $Z_i = I_i X_i$ is wrong with as mush rigour as we can go for now. There may be hand-wavy parts, but those will be indicated.} 

We mentioned, as shown by Klugman, Panjer and Willmot (2012)\cite{KlugmanPanjerWillmot2012}, that for the \hyperref[defn:individual_risk_model]{Individual Risk Model}, the aggregate claim is modeled by
\begin{equation*}
  S = \sum_{i=1}^{n} Z_i
\end{equation*}
where $Z_i$ is a random variable for the potential loss of the $i$\textsuperscript{th} insurance policy, while $n$ is fixed. It is claimed that we can also express each $Z_i$ as
\begin{equation*}
  Z_i = I_i X_i
\end{equation*}
where $I_i$ is an indicator function given by
\begin{equation*}
  I_i(x) = \begin{cases}
    1 & \text{ if a claim occurs } \\
    0 & \text{ if there are no claims }
  \end{cases},
\end{equation*}
while $X_i$ is the size of the claim(s) for the $i$\textsuperscript{th} policy provided that there is a claim.

\newthought{One problem} that arises is: are $X_i$ and $I_i$ independent? They should be if we wish to define $Z_i$ in such a way. In fact, according to \\
\noindent\textcolor{be-magenta}{\underline{Klugman et al.\ in page 177}},

\begin{quotebox}{be-magenta}{light}
  Let $X_j = I_j B_j$, where $I_1, \ldots, I_n, B_1, \ldots, B_n$ are independent.
\end{quotebox}

where $X_j$ is our $Z_i$, $I_j$ is our $I_i$, and $B_j$ is our $X_i$.

\paragraph{$\S \; Z_i$ is not well-defined} Let us be explicit about the definitions of $I_i$ and $X_i$; we have
\begin{gather*}
  I_i = \mathbb{1}_{\{ Z_i > 0 \}} \\
  X_i = Z_i \mid Z_i > 0
\end{gather*}
However, we observe that such a defintion of $X_i$ is undefined on $Z_i = 0$. So the equation
\begin{equation*}
  Z_i = I_i X_i
\end{equation*}
is note well-defined.

\paragraph{$\S$ Independence of $I_i$ and $X_i$} We cannot actually tell if $I_i$ and $X_i$ are independent from each other, as it is equivalent to comparing apples with oranges\sidenote{In fact, I think this analogy fits our case perfectly so.}. Recall from our earlier courses, in particular STAT330, of the following notion:

\begin{defnnonum}[Probability Space]\label{defn:probability_space}
  Let $\Omega$ be a sample space, and $\mathcal{F}$ a $\sigma$-algebra defined on $\Omega$\sidenote{Note that $(\Omega, \mathcal{F})$ is called a \hlnotea{measurable space}.}. A \hlnoteb{probability space} is the measurable space $(\Omega, \mathcal{F})$ with a \textcolor{be-blue}{probability measure}, $f: \mathcal{F} \to [0, 1]$, defined on the space. We denote a probability space as $(\Omega, \mathcal{F}, f)$.
\end{defnnonum}

As mentioned in an earlier $\S$, $X_i$ is not defined on $Z_i = 0$, while $I_i$ is defined on $Z_i = 0$ \sidenote{\faHandPaperO\ \enspace\ This statement is hand-wavy.}. So the sample space for $X_i$ and $I_i$ are not the same, and so their probability measures are not the same as well. Therefore, \hlimpo{it is meaningless to ask if $X_i$ and $I_i$ are independent}.

Our best attempt at fixing this is probably the following: let
\begin{equation*}
  Z_i = \sum_{i=1}^{I_i} X_i,
\end{equation*}
which we can then have $X_i$ to be independent from $I_i$. However, interestingly so, this is a similar approach to a \hyperref[defn:collective_risk_model]{Collective Risk Model}.

% section individual_risk_model_an_alternate_view (end)

\section{Coherent Risk Measure}%
\label{sec:coherent_risk_measure}
% section coherent_risk_measure

An excerpt from Klugman et al.\ (2012)~\cite{KlugmanPanjerWillmot2012}:

\begin{quotebox}{be-magenta}{light}
  The study of risk measures and their properties has been carried out by authors such as Wang. Specific desirable properties of risk measures were proposed as axioms in connection with risk pricing by Wang, Young, and Panjer and more generally in risk measurement by Artzer et al.\ The Artzner paper introduced the concept of \textbf{coherence} and is considered to be the groundbreaking paper in risk measurement.
\end{quotebox}

Often, we use the function $\rho(X)$ to denote risk measures. One may think of $\rho(X)$ as \hlnotec{the amount of assets required to protect against adverse outcomes of the risk $X$}.

\begin{defn}[Coherent Risk Measure]\index{Coherent Risk Measure}\label{defn:coherent_risk_measure}
  A \hlnoteb{coherent risk measure} is a risk measure $\rho(X)$ that has the following four properties for any two loss rvs $X$ and $Y$:
  \begin{enumerate}
    \item (\hlnotea{Subadditivity}) $\rho(X + Y) \leq \rho(X) + \rho(Y)$.
    \item (\hlnotea{Monotonicity}) If $X \leq Y$ for all possible outcomes, then $\rho(X) \leq \rho(Y)$.
    \item (\hlnotea{Positive homogeneity}) $\forall c \in \mathbb{R}_{> 0}$, $\rho(cX) = c\rho(X)$.
    \item (\hlnotea{Translation invariance}) $\forall c \in \mathbb{R}_{> 0}$, $\rho(X + c) = \rho(X) + c$
  \end{enumerate}
\end{defn}

\paragraph{Interpretation of the conditions}

\begin{itemize}
  \item \textbf{Subadditivity}
    \begin{itemize}
      \item the risk measure (and in return, the capital required to cover for it) for two risks combined will not be greater than for the risks to be treated separately;
      \item reflects the fact that there shuld be some diversification benefit from combining risks;
      \item this requirement is disputed: e.g.\ the merger of several small companies into a larger one exposes each of the small companies to the \hlnotea{reputational risks} of the others.
    \end{itemize}

  \item \textbf{Monotonicity}
    \begin{itemize}
      \item if one risk always has greater losses than the other under all circumstances\sidenote{Probabilistically, this means $P(X > Y) = 0$}, then the risk measure of the greater risk should always be greater than the other.
    \end{itemize}

  \item \textbf{Positive homogeneity}
    \begin{itemize}
      \item the risk measure is independent of the currency used to measure it;
      \item doubling the exposure to a particular risk requires double the capital, which is sensible as doubling provides no diversification.
    \end{itemize}

  \item \textbf{Translation invariance}
    \begin{itemize}
      \item there is no additional risk for an additional risk which has no additional uncertainty.
    \end{itemize}
\end{itemize}

% section coherent_risk_measure (end)

% chapter additional_material (end)

\chapter{Answers to Problem Sets}%
\label{chp:answers_to_problem_sets}
% chapter answers_to_problem_sets

* \textit{Only final answer is provided for checking (if a final answer exists). For proof problems, hints are provided if a trick is involved.} 

\section{Problem Set 1}%
\label{sec:problem_set_1}
% section problem_set_1

\begin{enumerate}
  \item \begin{enumerate}
      \item $F_X(x) = 1 - \frac{\alpha^\beta ( \alpha + ( \beta + 1 ) x )}{( \alpha + x )^{\beta + 1}}$
      \item $E[X] = \frac{2 \alpha}{\beta - 1}$
      \item $h(x) = \frac{\beta ( \beta + 1 ) x}{(\alpha + x)( \alpha + ( \beta + 1 ) x )}$
      \item $e_X(x) = \frac{\alpha + x}{\alpha + ( \beta + 1 ) x} \cdot \frac{2 \alpha + ( \beta + 1 ) x}{\beta - 1}$
    \end{enumerate}
  \item \begin{enumerate}
      \item $f_X(x) = ( 1 + 2x^2 ) e^{-2x}$
      \item $\bar{F}_X(x) = ( 1 + x + x^2 ) e^{-2x}$
      \item $h(x) = \frac{1 + 2x^2}{1 + x + x^2}$
      \item $e_X(x) = \frac{1 + x + \frac{x^2}{2}}{1 + x + x^2}$
    \end{enumerate}
  \item When encountering an integral with $e$ to some awkward power, it is usually useful to use substitution to, in a sense, transform that power to something more `pleasant'.
  \item \begin{enumerate}
      \item $G(t) = 1 - \frac{\ln ( 1 - \beta ( t - 1 ) )}{\ln ( 1 + \beta )}$
      \item $E[N] = \frac{\beta}{\ln ( 1 + \beta )}$ ; $\Var(N) = \frac{\beta (1 + \beta)}{\ln ( 1 + \beta )} - \left( \frac{\beta}{\ln ( 1 + \beta )} \right)^2$
    \end{enumerate}
  \item \begin{enumerate}
      \setcounter{enumii}{2}
    \item $\Var(X) = \alpha \theta^2 = 12$
    \end{enumerate}
\end{enumerate}

% section problem_set_1 (end)

\section{Problem Set 3}%
\label{sec:problem_set_3}
% section problem_set_3

\begin{enumerate}
  \item \begin{enumerate}
      \item  $Y_L = [ X - d ]_+$ ; $\bar{F}_{Y_L}(y) = \left( 1 - \frac{d + y}{\theta} \right)^\alpha$ for $0 \leq y \leq \theta - d$
      \item $X$ is a DMRL distribution.
      \item $\LER = 1 - \left( \frac{\theta - d}{\theta} \right)^{\alpha + 1}$
    \end{enumerate}
  \item $q = \frac{2}{3}$ ; $E[Y_P] = 120$
  \item \begin{enumerate}
      \item X has an IFR
      \item X is a DMRL distribution
      \item $E[Y_L] = \frac{b^2 - d^2 + \frac{2}{3}( b^3 - d^3 )}{2b + b^2}$
    \end{enumerate}
  \item $P(Y_P \leq 50) = 0.79679$
  \item \begin{enumerate}
      \item $\bar{F}_{Y_L}(y) = \frac{500 - y}{650}$ for $0 \leq y \leq 500$
      \item $\bar{F}_{Y_P}(y) = \frac{500 - y}{500}$ for $0 \leq y \leq 500$
      \item $e_X(150) = 250$
      \item $\Var((X - 150)^2 \mid X > 150) = 5.5556 x 10^9$
    \end{enumerate}
  \item 
    \begin{enumerate}
      \item \begin{gather*}
          \bar{F}_{Y_L}(y) = \begin{cases}
            1 & y < 0 \\
            0.125 & 0 \leq y < 100 \\
            125000y^{-3} & y \geq 100
          \end{cases} \\
          \bar{F}_{Y_P} (y) = \begin{cases}
            1 & y < 100 \\
            10^6 y^{-3} & y \geq 100
          \end{cases} \\
          E [ Y_L ] = 18.75 \quad E [ Y_P ] = 150
        \end{gather*}
      \item $\LER = 0.962963$
      \item maximum payment: $0.85(u - 100) = 1700$
        \begin{gather*}
          \bar{F}_{Y_L}(y) = \begin{cases}
            1 & y < 0 \\
            125000 \left( \frac{y}{0.85} + 100 \right)^{-3} & 0 \leq y < 1700 \\
            0 & y \geq 1700
          \end{cases} \\
          \Var(Y_L) = 791.066 \\
          \bar{F}_{Y_P}(50) = 0.2496
        \end{gather*}
    \end{enumerate}
  \item bonus: $Z = \frac{1}{2}( 500 - Y ) \mathbb{1}_{\{ Y < 500\}}$ where $Y \sim \Pareto(2, K)$ ; $K = 353.5534$
  \item $Y_L = 0.8 [ X - 1000 ]_+ - 0.8 [ X - 6000 ]_+ + 0.9[ X - u ]_+$, where $u$ is the size of the claim such that the insured makes a total payment of $10000$, i.e.
    \begin{equation*}
      1000 + 0.2 ( 6000 - 1000 ) + ( u - 6000 ) = 10000;
    \end{equation*}
    $E[Y_L] = 2699.362$
\end{enumerate}

% section problem_set_3 (end)

\section{Problem Set 4}%
\label{sec:problem_set_4}
% section problem_set_4

\begin{enumerate}
  \item $P(N \geq 2) = 0.609433$
  \item \begin{enumerate}
      \item $E[N] = 20$
      \item $\Var(N) = 260$
      \item $P(N \geq 2) = 0.9375$
    \end{enumerate}
  \item \begin{enumerate}
      \item $X \sim \Bernoulli \left( \frac{1}{4} \right)$
      \item $\Theta \mid X = 0 \sim \BetaDist(1, 4, 1)$
    \end{enumerate}
  \item \begin{enumerate}
      \setcounter{enumii}{1}
      \item $p_0 = \frac{e^{-\mu}}{( 1 + \theta )^\alpha}$
      \setcounter{enumii}{3}
      \item Substitute the power series $G_N(t)$ and $G'_N(t)$ into the given identity, and compare by coefficients.
    \end{enumerate}
  \item No.
\end{enumerate}

% section problem_set_4 (end)

\section{Problem Set 5}%
\label{sec:problem_set_5}
% section problem_set_5

\begin{enumerate}
  \item \begin{enumerate}
      \item $a = \beta$ ; $b = - \beta$
      \item Use the Taylor expansion $\ln(1 - x) = - \sum_{k=1}^{\infty} \frac{x^k}{k}$
      \setcounter{enumii}{3}
      \item $M$ is a mixture of a degenerate rv at $0$ with weight $q$, and a logarithmic rv with parameter $\beta^*$ and weight $1 - q$
    \end{enumerate}
  \item \begin{enumerate}
      \item $P \left( N^M > 2 \right) = 0.4037$
      \item $E \left[ N^M \right] = 3.7533$
      \item $\Var \left( N^M \right) = 27.199$
    \end{enumerate}
  \item \begin{enumerate}
      \item $p_k^T = \frac{1}{1 - e^{-\lambda}}\frac{\lambda^k e^{-\lambda}}{k!}$ ; $G_{N_L}(t) = \frac{e^{\lambda(t - 1) - e^{-\lambda}}}{1 - e^{-\lambda}}$
      \item $G_{N_P}(t) = \frac{e^{\alpha \lambda(t - 1) - e^{-\lambda}}}{1 - e^{-\lambda}}$ ; $p_0^M = \frac{e^{-\alpha\lambda} - e^{-\lambda}}{1 - e^{-\lambda}}$
      \item $\Var(N_P) = \frac{\alpha \lambda (1 - ( 1 + \alpha \lambda ) e^{-\lambda})}{( 1 - e^{-\lambda} )^2}$
    \end{enumerate}
  \item \begin{enumerate}
      \item The binomial theorem is useful here.
      \item $M_\Theta(t) = \frac{e^{\lambda t}}{1 - \beta t}$
      \item $G_N(t) = e^{\lambda(t - 1)} \cdot \frac{1}{1 - \beta( t - 1 )}$ ; $N$ is a product of $\Poi(\lambda)$ and $\Geo(\beta)$.
      \item $E[N] = \lambda + \beta$ ; $\Var(N) = \lambda + \beta( 1 + \beta )$
      \item $N_P$ can be viewed as an independent sum of a Poisson rv with parameter $\alpha \lambda$ and a geometric rv with parameter $\alpha \beta$
    \end{enumerate}
  \item $N$ is an $\left(\frac{1}{2}, 1, 1\right)$ member ; $g_0 = 0.023761$ ;
    \begin{equation*}
      g_k = \frac{\frac{3}{14}f_k + \sum_{j=1}^{k} \left( \frac{1}{2} + \frac{j}{k} \right) f_j g_{k - i}}{0.95}
    \end{equation*}
    so $g_1 = 0.171006$ ; $g_2 = 0.182776$ ; $g_3 = 0.156716$
  \item \begin{enumerate}
      \item $E[N_P] = 1.7299$
      \item $\bar{F}_{Y_P} = \frac{13824000(105 + 2y)}{7(120 + y)}$ for $y \geq 0$
      \item $f_0 = 0.197335$ ; $f_1 = 0.28311$ ; $f_2 = 0.17127$ ; $f_3 = 0.10556$
      \item $P(\hat{S} = 90) = 0.10421$
    \end{enumerate}
  \item \begin{enumerate}
      \item $N_P \sim \NB(2, 1.32375)$
      \item $\bar{F}_{Y_P}(y) = \exp \left\{ - \left( \frac{y + 50}{100}^3 \right) + \frac{1}{8} \right\}$ for $y \geq 0$
      \item $P( \hat{Y}_P = 120 ) = 0.0037584$
      \item $P( \hat{S} = 120 ) = 0.11926$
    \end{enumerate}
  \item \begin{enumerate}
      \item $P(S > 250) = 0.15083$
      \item $P(S > 250) = 0.4874$
    \end{enumerate}
\end{enumerate}

% section problem_set_5 (end)

% chapter answers_to_problem_sets (end)

\backmatter\

\pagestyle{plain}

\nobibliography*
\bibliography{references}

\input{listofsymbols.tex}

\printindex

\end{document}
